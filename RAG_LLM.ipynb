{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_chromadb(query, n_results=2, path=\"./chroma_storage\", collection_name=\"magic_document\"):\n",
    "    \"\"\"Запрос в chromadb\"\"\"\n",
    "    client = chromadb.PersistentClient(path=path)\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results  # Number of results\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_elements_by_ids(ids, path=\"./chroma_storage\", collection_name=\"magic_document\"):\n",
    "    \"\"\"Получение элементов из chromadb по их идентификаторам (IDs)\"\"\"\n",
    "    client = chromadb.PersistentClient(path=path)\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    # Получение данных по IDs\n",
    "    results = collection.get(ids=ids)\n",
    "\n",
    "        # Если результатов нет, возвращаем -1\n",
    "    if not results or not results.get(\"documents\"):\n",
    "        return -1\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_server_health(base_url='http://localhost:8080'):\n",
    "    \"\"\"{'status': 'ok'}\"\"\"\n",
    "    response = requests.get(f'{base_url}/health')\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def query_to_llm(user_input, context, base_url='http://localhost:8080'):\n",
    "\n",
    "    health_status = get_server_health(base_url=base_url)\n",
    "    if not health_status['status'] == 'ok':\n",
    "        return \"Error processing your request. Please try again.\\n{health_status}\"\n",
    "\n",
    "    # prompt = f\"{context}\\nUser: {user_input}\\nAssistant:\"\n",
    "    prompt = user_input\n",
    "    data = {\n",
    "        'prompt': prompt,\n",
    "        'temperature': 0.1,\n",
    "        'top_k': 35,\n",
    "        'top_p': 0.95,\n",
    "        'n_predict': 100,\n",
    "        'stop': [\"</s>\", \"Assistant:\", \"User:\"]\n",
    "    }\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(f'{base_url}/completion', json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        # return response.json()['content'].strip()\n",
    "        return response.json()\n",
    "    else:\n",
    "        return \"Error processing your request. Please try again.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_to_llm(\"Who are created you?\", {}, base_url='http://localhost:8080')['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': ' Fig. illustrates the attribute selection process. The decision tree algorithm starts at the root with the set of all target outcomes in the training data set. Each of the column attributes is evaluated to determine how it partitions the target outcomes. An impurity measure is used to determine which attribute best partitions the target outcomes. Ideally, for each attribute value, the corresponding outcome subset contains identical target values. B ? A ? a a b b C ? c c t, t B ? b b',\n",
       " 'id_slot': 0,\n",
       " 'stop': True,\n",
       " 'model': 'C:\\\\Users\\\\Igorexy\\\\.ai-navigator\\\\models\\\\openchat\\\\openchat-3.5-0106\\\\OpenChat-3.5-0106_Q8_0.gguf',\n",
       " 'tokens_predicted': 100,\n",
       " 'tokens_evaluated': 1254,\n",
       " 'generation_settings': {'n_ctx': 8192,\n",
       "  'n_predict': -1,\n",
       "  'model': 'C:\\\\Users\\\\Igorexy\\\\.ai-navigator\\\\models\\\\openchat\\\\openchat-3.5-0106\\\\OpenChat-3.5-0106_Q8_0.gguf',\n",
       "  'seed': 4294967295,\n",
       "  'seed_cur': 1535966156,\n",
       "  'temperature': 0.10000000149011612,\n",
       "  'dynatemp_range': 0.0,\n",
       "  'dynatemp_exponent': 1.0,\n",
       "  'top_k': 35,\n",
       "  'top_p': 0.949999988079071,\n",
       "  'min_p': 0.05000000074505806,\n",
       "  'tfs_z': 1.0,\n",
       "  'typical_p': 1.0,\n",
       "  'repeat_last_n': 64,\n",
       "  'repeat_penalty': 1.0,\n",
       "  'presence_penalty': 0.0,\n",
       "  'frequency_penalty': 0.0,\n",
       "  'mirostat': 0,\n",
       "  'mirostat_tau': 5.0,\n",
       "  'mirostat_eta': 0.10000000149011612,\n",
       "  'penalize_nl': False,\n",
       "  'stop': ['</s>', 'Assistant:', 'User:'],\n",
       "  'max_tokens': 100,\n",
       "  'n_keep': 0,\n",
       "  'n_discard': 0,\n",
       "  'ignore_eos': False,\n",
       "  'stream': False,\n",
       "  'n_probs': 0,\n",
       "  'min_keep': 0,\n",
       "  'grammar': '',\n",
       "  'samplers': ['top_k', 'tfs_z', 'typ_p', 'top_p', 'min_p', 'temperature']},\n",
       " 'prompt': 'For what tasks are decision trees most suitable?\\nUse the information from the hint: Collapsing the Decision Tree the Concurrent Data Predictor Cristian Alb ca.publicusgmail.com Abstract A family of concurrent data predictors is derived from the decision tree classifier by removing the limitation of sequentially evaluating attributes. By evaluating attributes concurrently, the decision tree collapses into a flat structure. Experiments indicate improvements of the prediction accuracy. Keywords Machine Learning Supervised Classification Decision Trees Introduction Techniques in machine learning are used to discover correlations, patterns, and trends in data. They also classify, or predict, data outcomes based on knowledge of a training data set. A great variety of algorithms exist and many combinations thereof are possible , , . Restricting the area of interest to supervised learning, the most widely used families of algorithms are . Decision trees . Random forests . Neural networks . Naive Bayes . Knearest neighbor . Supportvector machines . Linear regression . Logistic regression . Linear discriminant analysis By design, algorithms are meant to be used with data sets that have numerical attributes. Algorithms do operate with categorical attribute data sets. A Concurrent Data Predictor family of algorithms is proposed here. These algorithms are designed to operate with categorical attributes. Unlike the aforementioned algorithms, the Concurrent Data Predictors do not require a training phase. Cristian Alb May , Decision trees and random forests proved to be quite successful in a variety of classification and prediction tasks , . Random Forest classifiers extend the operation of decision trees by means of ensemble methods. However, decision trees might not fully exploit the informative content of the training data they evaluate attributes sequentially and ignore possible synergies that arise when evaluated concurrently. Decision Tree Operation Decision tree operation is based on the assumption that attribute values better predict the target outcomes when evaluated in a specific order determined during the training phase . The training data set is equivalent to a table of values. Each row can be thought of as a training instance, or entry. Columns correspond to attributes, or features, associated to the training data set. The target column is a particular column representing the outcome of interest. It classifies the row instance according to the outcome value. The other attribute columns contain values that characterize each instance from the perspective of the known features. In Fig. , a simple example illustrates a training data set and its associated decision tree. The trapezoids represent the leaves and they contain the associated target outcomes. The goal of the classifier is to predict the most likely target outcome for a given query entry. The query entry consists in a list of values that correspond to the column attributes of the training data set. The decision tree algorithm starts at the root with the set of all target outcomes in the training data set. Each of the column attributes is evaluated to determine how it partitions the target outcomes. An impurity measure is used to determine which attribute best partitions the target outcomes. Ideally, for each attribute value, the corresponding outcome subset contains identical target values. B ? A ? a a b b C ? c c t, t B ? b b C ? c c t t t t, t t Target Attr A Attr B Attr C t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c Figure Decision tree example. Fig. illustrates an example for the evaluation of the most suitable attribute for splitting the decision tree. Entropy is used as the impurity measure attribute A is the best choice while attribute C is the worst. Each subset of target outcomes becomes a new node branching out from the parent node. The process is recursively repeated on each of the branched outcome subsets. The process stops when no more attributes are left, or the current target outcome set is perfectly homogeneous. Such terminal node, or leaf, is used to produce the outcome prediction corresponding to the attribute values of the branch. A majority voting is performed on the nodes subset of outcomes in order to elect the predicted outcome. The impurity measure, used as a splitting criteria, is an average of the homogeneity of the split subsets of target outcomes. The homogeneity of the outcomes is usually expressed by the entropy of the ensemble. Other measures can be used, as is the Gini index, or information energy . Splitting Outcomes with Ensembles of Attributes Decision trees split the current set of outcomes using the values of one selected attribute. The selected attribute is the one that achieves the best score using the splitting criteria. What happens if this attribute is chosen at random? The resulting algorithm still retains a predictive power as experimental results indicate. Such a decision tree will be referred to as a Random Tree. Why use only one attribute instead of multiple ones? It is conceivable that the split subsets that result from evaluating more than one attribute generate a better impurity measure. Training data sorted by attribute A Training data sorted by attribute B Training data sorted by attribute C Idx Target Attr A Attr B Attr C Idx Target Attr A Attr B Attr C Idx Target Attr A Attr B Attr C t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a b c t a t b t c t t t t t t t t t t a t b t c t t t t t t t t t averageentropy . . . . averageentropy . . . . averageentropy . . . . Figure Decision tree attribute selection.',\n",
       " 'truncated': False,\n",
       " 'stopped_eos': False,\n",
       " 'stopped_word': False,\n",
       " 'stopped_limit': True,\n",
       " 'stopping_word': '',\n",
       " 'tokens_cached': 1353,\n",
       " 'timings': {'prompt_n': 1254,\n",
       "  'prompt_ms': 8831.861,\n",
       "  'prompt_per_token_ms': 7.042951355661883,\n",
       "  'prompt_per_second': 141.98593025863968,\n",
       "  'predicted_n': 100,\n",
       "  'predicted_ms': 9563.636,\n",
       "  'predicted_per_token_ms': 95.63636000000001,\n",
       "  'predicted_per_second': 10.456274161835518},\n",
       " 'index': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_db = \"\"\"For what tasks are decision trees most suitable?\"\"\"\n",
    "\n",
    "query_result = query_to_chromadb(query_db, n_results=5)    # query_result['documents'][0][:1]\n",
    "\n",
    "page = int(query_result['ids'][0][0].split()[1][:-1])\n",
    "doc = query_result['ids'][0][0].split()[0]\n",
    "\n",
    "id = doc + f\" {page-1})\"\n",
    "dop_doc_1 = get_elements_by_ids(id)\n",
    "id = doc + f\" {page+1})\"\n",
    "dop_doc_2 = get_elements_by_ids(id)\n",
    "\n",
    "docs = []\n",
    "if dop_doc_1 != -1:\n",
    "    docs = [dop_doc_1['documents'][0]]\n",
    "    docs.append(query_result['documents'][0][0])\n",
    "if dop_doc_2 != -1:\n",
    "    dop_doc_2 = dop_doc_2['documents'][0]\n",
    "    docs.append(dop_doc_2)\n",
    "\n",
    "# context = ' '.join(query_result['documents'][0][:1])\n",
    "context = ' '.join(docs)\n",
    "\n",
    "# query_llm = f\"\"\"Your task is to answer the question according to the context. If the context does not contain information about the question, then report it.\n",
    "#             question: {query_db}\n",
    "#             context: {context}\"\"\"\n",
    "\n",
    "query_llm = f\"\"\"{query_db}\\nUse the information from the hint: {context}\"\"\"\n",
    "\n",
    "\n",
    "result = query_to_llm(query_llm, {}, base_url='http://localhost:8080')#['content']\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': ['1806.05886v2 7)'],\n",
       " 'embeddings': None,\n",
       " 'documents': ['. A Deeper Look into the Operation of the Framework In order to visualize how the reinforcement learning framework preprocesses distorted images, we run another experiment on MNIST with coarser distortions, in particular rotations are performed with large angles degrees and ipping operations as in the previous experiment. For each distorted image, we trace the operation of the framework and obtain the transformation chain that the framework automatically generates for the image. An illustration for a few images is shown in Figure . It is interesting that most images are either classied directly or transformed to their original version before being classied. The exact recovery is possible thanks to the symmetry property of transformation actions. Although the framework is able to recover distorted images, it is not guaranteed to nd the optimal chain of transformations in term of the shortest recovery path. In addition, there is a small number of images which are confused by the framework as shown in the bottom row of Figure . These are the main source of misclassication errors of the reinforcement learning classier. Figure Illustration of how the reinforcement learning framework preprocesses distorted images. Discussion The key contributions of this paper are threefold. Firstly, we developed the idea of automated data preprocessing using a reinforcement learning framework. While we demonstrated and evaluated it for image data, it is applicable to other types of structured and unstructured data as well. Secondly, the proposed system is iterative and therefore it provides explainable data preprocessing, i.e. one can inspect which transformations were applied to each data instance during the preprocessing. Thirdly, compared with traditional data augmentation approaches, our system follows a more efcient approach to produce a clean training data set that can be used effectively for training highly accurate and robust machine learning models. Despite being of high practical value, the automation of data preprocessing has only drawn little interest by the machine learning research community so far. Although we suggest in this paper a novel approach for this problem, there is still a lot of room to extend this work. Firstly, the set of transformations may contain more advanced preprocessing techniques such as rotations with learnable angles, croppingscaling with learnable ratios, image segmentation, object detection, etc. While it is easy to integrate continuous actions with learnable parameters into the framework as described in Section .., complicated actions like image segmentation and object detection may require more efforts. For example, one could select only a small number of segments or objects as the simplied representation of an image for the next iteration after applying those actions. Secondly, one could boost the performance of the reinforcement learning framework by replacing the current simple DQN policy network. In addition, CNNs derived from the policy network as described in Figure c may be a way to obtain better performance in terms of accuracy. Conclusions We have presented in this paper a novel approach to the problem of automating data preprocessing, which is of high potential value for realworld data science and machine learning projects. The approach is based on a reinforcement learning framework to nd sequences of preprocessing transfor mations for each data instance individually. We showed in our experiments that even with simple preprocessing actions such as rotation and ipping, image classiers can benet signicantly with respect to their accuracy and particularly their robustness. Thanks to the iterative nature of the framework, our solution also provides a certain level of explanability, i.e. we can trace exactly how an image is preprocessed via a chain of transformations. In summary, we believe that this is a promising'],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [{'filename': '1806.05886v2.pdf',\n",
       "   'keyword': 'data preprocessing',\n",
       "   'page_count': 9,\n",
       "   'topic': 'Data Analysis'}],\n",
       " 'included': [<IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# page = int(query_result['ids'][0][0].split()[1][:-1])\n",
    "# doc = query_result['ids'][0][0].split()[0]\n",
    "\n",
    "# id = doc + f\" {page-1})\"\n",
    "# get_elements_by_ids(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fig. illustrates the attribute selection process. The decision tree algorithm starts at the root with the set of all target outcomes in the training data set. Each of the column attributes is evaluated to determine how it partitions the target outcomes. An impurity measure is used to determine which attribute best partitions the target outcomes. Ideally, for each attribute value, the corresponding outcome subset contains identical target values. B ? A ? a a b b C ? c c t, t B ? b b\n"
     ]
    }
   ],
   "source": [
    "print(result['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp.txt', 'a') as f:\n",
    "    f.write(result['content'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the decision tree T with k leaf nodes. The split trees, T, k, s, ks, bks (T, k, l, h, s, Xk, Ys, ks, bks), are generated from T with the split information. The Grow algorithm can be represented as follows. Grow algorithm is a tree growing algorithm where decision trees are generated from other trees in an online manner. The input is a decision tree T and the output is the search set of decision trees, denoted by S. The Grow algorithm is defined in Algorithm . The heuristic function h is used for partitioning the training data in an optimal way, so that the most informative partitioning of the data is found. The function GetSplitLeavesT, l, h takes an input decision tree T and computes l leaf nodes of T with the largest heuristic hs from LT . The function GenerateSplitRuless takes an input decision tree T and a number of decision trees B and generates the split information for each decision tree. The split information is a set of ks, bks. The Xk s denotes the subset of the decision tree T with k leaf nodes. The split trees, T, k, s, ks, bks (T, k, l, h, s, Xk, Ys, ks, bks), are generated from T with the split information. The Grow algorithm can be represented as follows. Grow algorithm is a tree growing algorithm where decision trees are generated from other trees in an online manner. The input is a decision tree T and the output is the search set of decision trees, denoted by S. The Grow algorithm is defined in Algorithm . The heuristic function h is used for partitioning the training data in an optimal way, so that the most informative partitioning of the data is found. The function GetSplitLeavesT, l, h takes an input'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_llm = f\"\"\"Translate the text from English into Russian.\n",
    "Text:\n",
    "{result['content']}\"\"\"\n",
    "\n",
    "# query_llm = f\"\"\"Переведи текст с английского языка на русский.\n",
    "# текст:\n",
    "# {result['content']}\"\"\"\n",
    "\n",
    "res_translate = query_to_llm(query_llm, {}, base_url='http://localhost:8080')#['content']\n",
    "\n",
    "res_translate['content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
