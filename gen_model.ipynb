{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылки на документацию  \n",
    "https://docs.anaconda.com/ai-navigator/user-guide/tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API в ламу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "base_url = 'http://localhost:8080'\n",
    "\n",
    "def get_server_health():\n",
    "    response = requests.get(f'{base_url}/health')\n",
    "    return response.json()\n",
    "\n",
    "get_server_health()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was created by the Assistant of the Department of Mathematics at the University of Cambridge.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def post_completion(context, user_input):\n",
    "    prompt = f\"{context}\\nUser: {user_input}\\nAssistant:\"\n",
    "    data = {\n",
    "        'prompt': prompt,\n",
    "        'temperature': 0.8,\n",
    "        'top_k': 35,\n",
    "        'top_p': 0.95,\n",
    "        'n_predict': 400,\n",
    "        'stop': [\"</s>\", \"Assistant:\", \"User:\"]\n",
    "    }\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    response = requests.post(f'{base_url}/completion', json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()['content'].strip()\n",
    "    else:\n",
    "        return \"Error processing your request. Please try again.\"\n",
    "    \n",
    "post_completion({}, \"Who are created you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "def pdf_to_query(file_path: str, extract_images=True):\n",
    "    \"\"\"На входе путь к PDF файлу, на выходе \n",
    "    document_text - текствая версия файла\n",
    "    pages - страницы с текстом и метаданными\"\"\"\n",
    "    loader = PyPDFLoader(file_path, extract_images=extract_images)\n",
    "    pages = loader.load()\n",
    "    document_text = [page.page_content for page in pages]\n",
    "    document_text = ' '.join(document_text)\n",
    "    return document_text, pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, _ = pdf_to_query(r\"E:\\ImportantFiles\\Документы\\University\\Magic Science Assistant\\dataset\\Machine Learning\\decision trees\\1302.4207v1.pdf\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A composition theorem for decision tree complexity\n",
      "Ashley Montanaro∗\n",
      "Centre for Quantum Information and Foundations, DAMTP, University of Cambridge, UK.\n",
      "July 27, 2021\n",
      "Abstract\n",
      "We completely characterise the complexity in the decision tree model of computing composite\n",
      "relations of the form h = g ◦(f1, . . . , fn), where each relation fi is boolean-valued. Immediate\n",
      "corollaries include a direct sum theorem for decision tree complexity and a tight characterisation\n",
      "of the decision tree complexity of iterated boolean functions.\n",
      "1 Introduction\n",
      "The decision tree model captures the complexity of computing functions f : Xm →Y in a setting\n",
      "where the quantity of interest is the number of queries to the input (see [1] for a good review of\n",
      "the model). We are allowed to query individual elements xi ∈X at unit cost, and seek to compute\n",
      "f(x1, . . . , xm) using the minimal number of queries.\n",
      "Here we will be interested in the decision tree complexity of composite functions, i.e. functions\n",
      "of the form\n",
      "h(x) = g(f1(x1), . . . , fn(xn)),\n",
      "where xi ∈Xmi . One strategy to compute h is ﬁrst to replace g with the function ¯g given by\n",
      "substituting the values taken by any constant functions fi into g, and then to compute ¯ g using\n",
      "eﬃcient algorithms for f1, . . . , fn as black boxes. In other words, we use an eﬃcient algorithm for\n",
      "computing ¯g(y1, . . . , yn), and replace each query to a variable yi with the evaluation of fi(xi) using\n",
      "an optimal algorithm. As the xi inputs are independent, it is natural to conjecture that this is in fact\n",
      "always the most eﬃcient way to compute g. However, a moment’s thought shows that this cannot\n",
      "be the case in general: indeed, consider the function h(x) = g(f(x)), where f : {0, 1}n →{0, 1}n\n",
      "is the identity function f(x) = x, and g : {0, 1}n →{0, 1}is the function extracting the ﬁrst bit\n",
      "of the input, g(y) = y1. Then f requires n queries to be computed, whereas h can be computed\n",
      "with 1 query. Another simple, but arguably less trivial, example of this phenomenon is illustrated\n",
      "in Figure 1 below; the key point in this example is that the worst-case input to the functions fi\n",
      "may not produce an output which is a worst-case input to g.\n",
      "We will show that, nevertheless, the simple algorithm above is optimal if the functions fi are\n",
      "boolean-valued, i.e. when Y = {0, 1}. In fact, we prove such a result for the generalisation of the\n",
      "decision tree model to computing relations. A decision tree computes a relation f ⊆Xm ×Y if,\n",
      "on input x ∈Xm, it outputs y such that ( x, y) ∈f (see Section 1.2 for a formal deﬁnition of the\n",
      "model). We call this result a composition theorem for decision tree complexity.\n",
      "∗am994@cam.ac.uk\n",
      "1\n",
      "arXiv:1302.4207v1  [cs.CC]  18 Feb 2013 x1\n",
      "0 x2\n",
      "1 2\n",
      "(a) Optimal tree for f\n",
      "y1\n",
      "y2 1 2\n",
      "0 1 2\n",
      "(b) Optimal tree for g\n",
      "x1\n",
      "x3 x2\n",
      "0 x4 1 2\n",
      "1 2\n",
      "(c) Optimal tree for h\n",
      "Figure 1: Let f : {0, 1}2 →{0, 1, 2}and g : {0, 1, 2}2 →{0, 1, 2}be deﬁned by the decision trees\n",
      "above (where edges correspond to elements of {0, 1}or {0, 1, 2}in ascending order from left to\n",
      "right). Set h(x1, x2, x3, x4) = g(f(x1, x2), f(x3, x4)). Then f and g require 2 queries each to be\n",
      "computed, but h can be computed using only 3.\n",
      "1.1 Related work\n",
      "This work can be seen as ﬁtting into the framework of “direct sum” results in query complexity,\n",
      "in which there has been signiﬁcant recent interest (see the thesis [2] for a good review). The basic\n",
      "aim of such work is to show that, given n independent computational problems, the most eﬃcient\n",
      "way to solve them all is simply to use an optimal algorithm for each problem separately. While\n",
      "this claim is intuitively obvious, it is not necessarily easy to prove, and in some cases is even false;\n",
      "see [2] for some examples.\n",
      "In particular, Jain, Klauck and Santha have shown [5] such a direct sum theorem for determin-\n",
      "istic and randomised decision tree complexity. As discussed in [5], previous work had dismissed\n",
      "this question as uninteresting, but the proof is not immediate. In the deterministic case, the result\n",
      "of [5] is proven by showing that, given a decision tree T computing n independent outputs, one\n",
      "can produce n independent trees Ti, one for each output, and lower bounding the depth of T in\n",
      "terms of the depth of the trees Ti. This approach does not seem suitable for proving a composition\n",
      "theorem, as given a tree computing h, it is not necessarily possible to produce individual trees for\n",
      "f1, . . . , fn. On the other hand, the composition theorem given here in fact implies a direct sum\n",
      "theorem for decision tree complexity (see Section 2.1 for this and other corollaries).\n",
      "Similar, but more complicated, composition theorems to the present work have been proven for\n",
      "quantities which are important in the study ofquantum query complexity. Høyer, Lee and ˇSpalek [3]\n",
      "proved that the so-called non-negative weight adversary bound (which we will not deﬁne here;\n",
      "see [3] for details) obeys a composition theorem. Interestingly, their proof is based on generalising\n",
      "the query model to weighted queries, similarly to the approach taken here. A composition theorem\n",
      "was later proven for the general adversary bound (in two parts; a lower bound by Høyer, Lee and\n",
      "ˇSpalek [4], and an upper bound by Reichardt [7]), which was then used by Reichardt to infer a\n",
      "composition theorem for quantum query complexity [8].\n",
      "1.2 Relations and decision trees\n",
      "We use essentially the standard model of decision trees (see e.g. [1] for deﬁnitions, and [5] for the\n",
      "extension to relations). Fix ﬁnite sets X and Y , and integer m. A decision tree T is a rooted |X|-\n",
      "ary tree whose internal vertices are labelled with indices between 1 and n, whose edges are labelled\n",
      "with elements of X, and whose leaves are labelled with elements of Y . T computes a function\n",
      "fT : Xn →Y as follows: starting with the root, it queries the input variable indexed by the label\n",
      "2 of the current vertex and, depending on the outcome, evaluates the corresponding subtree. When\n",
      "a leaf is reached, the tree outputs the label of that leaf. The depth of T is the maximal length of\n",
      "a path from the root to a leaf (i.e. the worst-case number of queries used on any input).\n",
      "We will consider the complexity of computing relations in the decision tree model. Let f ⊆\n",
      "Xn ×Y . We write dom f = {x : ∃y, (x, y) ∈f}and range f = {y : ∃x, (x, y) ∈f}, and also\n",
      "f(x) = {y : (x, y) ∈f}, f−1(y) = {x : (x, y) ∈f}. f is said to be a partial function if |f(x)|= 1\n",
      "for all x ∈dom f; in this case, we abuse notation and write f(x) for the single element within the\n",
      "set f(x). f is a total function if in addition dom f = Xn. We say that f is constant if there exists\n",
      "y ∈Y such that f−1(y) = dom f, and trivial if, for all y ∈Y , f−1(y) = dom f. Note that trivial\n",
      "relations are constant, and that the empty relation f = ∅is trivial.\n",
      "We say that a decision tree T computes a relation f if, for all x ∈dom f, on input x, T outputs\n",
      "y such that (x, y) ∈f, i.e. fT (x) ∈f(x) for all x ∈dom f. If this holds, we say that fT is consistent\n",
      "with f. The decision tree complexity D(f) is the minimal depth over all decision trees computing\n",
      "f. This quantity is also known as the exact (classical) query complexity of f.\n",
      "This can be generalised to deﬁne a weighted variant of decision tree complexityD(f, [w1, . . . , wn]),\n",
      "which is equal to the minimal worst-case number of queries required to computef, given that query-\n",
      "ing the i’th element of the input costs wi queries. This can again be expressed as the minimal depth\n",
      "of a decision tree computing f with weights w1, . . . , wn on the inputs, where the depth of a de-\n",
      "cision tree T with weights on the inputs is simply the maximal sum of the weights of the inputs\n",
      "speciﬁed by the labels of vertices of T, on any path from the root to a leaf. The usual decision tree\n",
      "complexity is recovered by setting D(f) = D(f, [1, . . . ,1]).\n",
      "For f ⊆Xn ×Y , let fi←b ⊆Xn ×Y be the modiﬁcation of f we obtain by substituting b as the\n",
      "i’th input to f. That is,\n",
      "fi←b = {(x, y) ∈f : xi = b}.\n",
      "We can use this deﬁnition to write down the following characterisation of weighted decision tree\n",
      "complexity (indeed, this could even be seen as its deﬁnition).\n",
      "Fact 1. Let f ⊆Xn ×Y , and w1, . . . , wn ∈N. Then D(f, [w1, . . . , wn]) = 0 if and only if f is\n",
      "constant. If f is not constant,\n",
      "D(f, [w1, . . . , wn]) = min\n",
      "i∈[n]\n",
      "max\n",
      "b∈X\n",
      "D(fi←b, [w1, . . . , wn]) + wi.\n",
      "Proof. If f is constant, the tree can simply output y such that f−1(y) = dom f using no queries.\n",
      "On the other hand, if there exists a tree that computes f and makes no queries, then there is only\n",
      "one ﬁxed value y that is output by the tree; so for all x ∈dom f, (x, y) ∈f; so f−1(y) = dom f.\n",
      "If f is not constant, then for any i, we can produce a tree computing f with weights w1, . . . , wn\n",
      "by ﬁrst querying the i’th variable, obtaining outcome b, then using an optimal tree for fi←b with\n",
      "weights w1, . . . , wn. This proves the “ ≤” part. For the “ ≥” part, given a tree computing f which\n",
      "makes its ﬁrst query to the i’th variable, where i achieves the minimum above, we can obtain a\n",
      "tree for computing fi←b, for any b ∈X, by simulating this tree, replacing the ﬁrst query with the\n",
      "constant b and hence saving weight wi.\n",
      "In the above fact, and the rest of the paper, we write [ n] for {1, . . . , n}.\n",
      "3 1.3 Composition of relations\n",
      "For i ∈[n], let fi ⊆Xmi ×Y , and g ⊆Y n ×Z. Set m = ∑\n",
      "i mi. The composition g ◦(f1, . . . , fn)\n",
      "is deﬁned as\n",
      "{(x, z) ∈Xm ×Z : ∃y ∈Y n, ∀i (xi, yi) ∈fi ∧(y, z) ∈g},\n",
      "where we write x = (x1, . . . , xn), with xi ∈Xmi . If f and g are total functions, this simply says\n",
      "that for all x ∈Xm,\n",
      "h(x) = g(f1(x1), . . . , fn(xn)).\n",
      "In words, an algorithm to compute the relation h on input x has to output an arbitrary z such that\n",
      "z ∈g(y), for some y such that for all i, yi ∈fi(xi).\n",
      "Assume that fi is non-trivial for all i. If some of the relations fi are constant (i.e. (fi)−1(bi) =\n",
      "dom fi, for some bi ∈Y ), this expression for h can be simpliﬁed by removing these inputs and\n",
      "modifying g appropriately. Write ¯g for the relation obtained by ﬁxing constants to the inputs to g,\n",
      "where possible. In other words, if S ⊆[n] is the set of indices i such that fi is not constant, then\n",
      "¯g = {(y, z) ∈g : yi = bi, i /∈S},\n",
      "h = ¯g ◦(f1, . . . , fn) = {(x, z) ∈Xm ×Z : ∃y ∈Y n, ∀i ∈S, (xi, yi) ∈fi ∧(y, z) ∈¯g}.\n",
      "2 A composition theorem\n",
      "We are ﬁnally ready to state and prove our main result.\n",
      "Theorem 2. Fix n ∈N and ﬁnite sets X, Z. Let g ⊆ {0, 1}n ×Z, and for 1 ≤i ≤n, let\n",
      "fi ⊆Xmi ×{0, 1}be non-trivial. Set m = ∑\n",
      "i mi. Deﬁne h ⊆Xm ×Z by\n",
      "h = g ◦(f1, . . . , fn).\n",
      "Also let ¯g be the relation given by substituting constant inputs into g. That is, for each i, if\n",
      "(fi)−1(bi) = dom fi, then g is replaced with gi←bi (as fi is non-trivial, there is exactly one such\n",
      "bi). Then\n",
      "D(h) = D(¯g, [D(f1), . . . , D(fn)]).\n",
      "Proof. To see that D(h) ≤D(¯g, [D(f1), . . . , D(fn)]), observe that the algorithm discussed in the\n",
      "introduction achieves this complexity. Formally, we compute h as follows: ﬁrst form a new relation\n",
      "¯g by replacing constant relationsfi with corresponding constants as inputs tog; then use an optimal\n",
      "tree for computing ¯g with weights D(f1), . . . , D(fn), replacing each query to the i’th variable with\n",
      "the use of an optimal tree for fi. The more interesting part is the corresponding lower bound, the\n",
      "proof of which proceeds by induction on D(h).\n",
      "For the base case, take D(h) = 0. Then h is constant. By deﬁnition, this implies that there\n",
      "exists z such that the following holds: For all x = (x1, . . . , xn) such that there exist y′ ∈{0, 1}n\n",
      "and z′∈Z with (xi, y′\n",
      "i) ∈fi for all i and (y′, z′) ∈g, there exists y ∈{0, 1}n such that (xi, yi) ∈fi\n",
      "for all i and (y, z) ∈g. Let S = {i : fi is not constant}. For all i ∈S, let ai, bi ∈Xmi be picked\n",
      "such that (ai, 0) ∈f, (ai, 1) /∈f, (bi, 0) /∈f and (bi, 1) ∈f. As fi is not constant, such elements ai,\n",
      "bi exist. Further, for all i /∈S, let ci satisfy (fi)−1(ci) = dom fi. As fi is not trivial, ci is unique\n",
      "and there exists di ∈Xmi such that (di, ci) ∈fi, but (di, c′\n",
      "i) /∈fi for c′\n",
      "i ̸= ci.\n",
      "For s : S →{0, 1}, let the string y(s) ∈{0, 1}n be deﬁned as follows: y(s)i = s(i) for i ∈S, and\n",
      "y(s)i = ci for i /∈S. For each distinct input x ∈{0, 1}m whose i’th block is equal to ai or bi (for\n",
      "4 i ∈S) or di (for i /∈S), a diﬀerent string y(s) is obtained, and if ( x, y) ∈(f1, . . . , fn), y = y(s). In\n",
      "particular, by choosing s appropriately one can produce any y ∈dom ¯g. As there is exactly one y\n",
      "such that ( x, y) ∈(f1, . . . , fn), and h is constant, ( y, z) ∈g. Thus, for all y ∈dom ¯g, (y, z) ∈¯g.\n",
      "Hence ¯g is constant and D(¯g, [D(f1), . . . , D(fn)]) = 0 as required.\n",
      "For the inductive step, assume that D(h) = D(¯g, [D(f1), . . . , D(fn)]) for all relations h =\n",
      "g ◦(f1, . . . , fn) such that fi is non-trivial for all i, and such that D(h) ≤k, for some k. Now let\n",
      "h = g ◦(f1, . . . , fn) satisfy D(h) = k + 1. As h is not constant, by Fact 1\n",
      "D(h) = min\n",
      "i∈[m]\n",
      "max\n",
      "b∈X\n",
      "D(hi←b) + 1\n",
      "= min\n",
      "i∈[n]\n",
      "min\n",
      "j∈[mi]\n",
      "max\n",
      "b∈X\n",
      "D(g ◦(f1, . . . , fi\n",
      "j←b, . . . , fn)) + 1.\n",
      "As fi is non-trivial for all i, for any pair i, jthere exists b′ such that fi\n",
      "j←b′ is non-trivial. Thus,\n",
      "for b achieving the above maximum, fi\n",
      "j←b is non-trivial. Let i, jbe the indices achieving the above\n",
      "minimum. Then, by the inductive hypothesis,\n",
      "max\n",
      "b∈X\n",
      "D(g ◦(f1, . . . , fi\n",
      "j←b, . . . , fn)) = max\n",
      "b∈X\n",
      "D(¯¯g, [D(f1), . . . , D(fi\n",
      "j←b), . . . , D(fn)]),\n",
      "where ¯¯g is the relation obtained by substituting bk as the k’th input to g, for all k ̸= i such that\n",
      "(fk)−1(bk) = dom fk, and also substituting bi as the i’th input to g if (fi\n",
      "j←b)−1(bi) = dom fi\n",
      "j←b.\n",
      "We thus have\n",
      "D(h) = max\n",
      "b∈X\n",
      "D(¯¯g, [D(f1), . . . , D(fi\n",
      "j←b), . . . , D(fn)]) + 1\n",
      "= D(¯¯g, [D(f1), . . . ,max\n",
      "b∈X\n",
      "D(fi\n",
      "j←b), . . . , D(fn)]) + 1,\n",
      "where the second equality holds because decreasing any of the weights cannot increase the decision\n",
      "tree complexity. First assume that, for b achieving the maximum, fi\n",
      "j←b is not constant. Then ¯¯g = ¯g\n",
      "and\n",
      "D(h) = D(¯g, [D(f1), . . . ,max\n",
      "b∈X\n",
      "D(fi\n",
      "j←b), . . . , D(fn)]) + 1\n",
      "≥ D(¯g, [D(f1), . . . , D(fi) −1, . . . , D(fn)]) + 1\n",
      "≥ D(¯g, [D(f1), . . . , D(fn)])\n",
      "as required. The ﬁrst inequality follows from D(fi) ≤maxb∈X D(fi\n",
      "j←b) + 1, while the second holds\n",
      "because, to ﬁnd a decision tree T which computes ¯g with weight D(fi) on the i’th variable, we\n",
      "can just use an optimal decision tree T′for ¯g with weight D(fi) −1 on the i’th variable. As T′is\n",
      "optimal, we only encounter the i’th variable at most once on any path from the root to a leaf, so\n",
      "the depth of T is upper bounded by the depth of T′, plus 1.\n",
      "On the other hand, now assume that, for b achieving the maximum, fi\n",
      "j←b is constant. Then\n",
      "it must hold that D(fi) = 1 (if fi were already constant, querying any of the variables on which\n",
      "it depends would not achieve the overall minimum; if fi is not constant, and there exists b′ ∈X\n",
      "such that fi\n",
      "j←b′ is not constant, such a b′ would achieve the maximum over b; hence D(fi) =\n",
      "1). So we can produce a decision tree for ¯ g with weights D(f1), . . . , D(fn) by computing fi\n",
      "using one query to the j’th variable and then using an optimal decision tree for ¯¯g with weights\n",
      "D(f1), . . . , D(fi−1), 0, D(fi+1), . . . , D(fn). Thus\n",
      "D(¯g, [D(f1), . . . , D(fn)]) ≤max\n",
      "b∈X\n",
      "D(¯¯g, [D(f1), . . . , D(fi\n",
      "j←b), . . . , D(fn)]) + 1 = D(h)\n",
      "as required, completing the proof.\n",
      "5 2.1 Corollaries\n",
      "We ﬁnish by noting a few simple corollaries of Theorem 2 that can be useful in amplifying sepa-\n",
      "rations between decision tree complexity and other complexity measures. We begin by considering\n",
      "the case where all the relations fi are the same.\n",
      "Corollary 3.Fix f ⊆Xk ×{0, 1}and g ⊆{0, 1}n ×Z. Deﬁne h ⊆Xnk ×Z as h = g◦(f, f, . . . , f).\n",
      "Then D(h) = D(f)D(g).\n",
      "Proof. If f is constant, then D(h) = 0. Otherwise, take f1 = ··· = fn = f in Theorem 2. Then\n",
      "D(h) = D(g, [D(f), . . . , D(f)]) = D(f)D(g).\n",
      "Second, we observe that the composition theorem can be used to characterise the decision tree\n",
      "complexity of an iterated boolean function, via the following corollary.\n",
      "Corollary 4. For any f : {0, 1}n →{0, 1}, let the k-fold iteration of f, f(k) : {0, 1}nk\n",
      "→{0, 1}, be\n",
      "deﬁned as follows. Set f(1) = f, and for k ≥2, split the input x ∈{0, 1}nk\n",
      "into blocks x1, . . . , xn\n",
      "of nk−1 bits each, and set f(k)(x) = f(f(k−1)(x1), . . . , f(k−1)(xn)). Then D(f(k)) = D(f)k.\n",
      "Proof. If f is constant, then D(f(k)) = 0 = D(f)k. Otherwise, by Theorem 2, for k ≥2, D(f(k)) =\n",
      "D(f, [D(f(k−1)), . . . , D(f(k−1))]) = D(f)D(f(k−1)). The claim follows by induction.\n",
      "Nisan and Szegedy used an iterated version of the “not all equal” function on 3 bits to prove an\n",
      "asymptotic separation between decision tree complexity and polynomial degree [6], based on lower\n",
      "bounding decision tree complexity by sensitivity. Corollary 4 gives a direct proof of this result.\n",
      "Finally, we obtain the following special case of a direct sum result of Jain, Klauck and Santha [5].\n",
      "Corollary 5. For any non-trivial f1, . . . , fn ⊆Xk ×{0, 1}, let h ⊆Xkn ×{0, 1}n be deﬁned by\n",
      "h = (f1, f2, . . . , fn). Then D(h) = ∑\n",
      "i∈[n] D(fi).\n",
      "Proof. Take g(y1, . . . , yn) = (y1, . . . , yn) in Theorem 2. Then D(h) = D(¯g, [D(f1), . . . , D(fn)]). Let\n",
      "S = {i : fi is not constant}. Then D(¯g, [D(f1), . . . , D(fn)]) = ∑\n",
      "i∈S D(fi) = ∑\n",
      "i∈[n] D(fi).\n",
      "The proof technique of [5] allows the range of each relation fi to be arbitrary, rather than being\n",
      "restricted to {0, 1}as here.\n",
      "Acknowledgements\n",
      "I would like to thank Graeme Mitchison and Toby Cubitt for helpful discussions and comments on\n",
      "this work, and an anonymous referee for inspiring it.\n",
      "References\n",
      "[1] H. Buhrman and R. de Wolf. Complexity measures and decision tree complexity: a survey.\n",
      "Theoretical Computer Science, 288:21–43, 2002.\n",
      "[2] A. Drucker. The Complexity of Joint Computation . PhD thesis, MIT, 2012.\n",
      "6 [3] P. Høyer, T. Lee, and R. ˇSpalek. Tight adversary bounds for composite functions, 2005.\n",
      "quant-ph/0509067.\n",
      "[4] P. Høyer, T. Lee, and R. ˇSpalek. Negative weights make adversaries stronger. In Proc. 39th\n",
      "Annual ACM Symp. Theory of Computing , pages 526–535, 2007. quant-ph/0611054.\n",
      "[5] R. Jain, H. Klauck, and M. Santha. Optimal direct sum results for deterministic and randomized\n",
      "decision tree complexity. Inf. Proc. Lett., 110(20):893–897, 2010. arXiv:1004.0105.\n",
      "[6] N. Nisan and M. Szegedy. On the degree of Boolean functions as real polynomials. Computa-\n",
      "tional Complexity, 4(4):301–313, 1994.\n",
      "[7] B. Reichardt. Span programs and quantum query complexity: The general adversary bound is\n",
      "nearly tight for every boolean function. In Proc. 50th Annual Symp. Foundations of Computer\n",
      "Science, pages 544–551, 2009. arXiv:0904.2759.\n",
      "[8] B. Reichardt. Reﬂections for quantum query algorithms. In Proc. 22nd ACM-SIAM Symp.\n",
      "Discrete Algorithms, pages 560–569, 2011. arXiv:1005.1601.\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Updated Chroma client initialization\n",
    "client = chromadb.PersistentClient(\n",
    "    path=\"./chroma_storage\"  # Directory for storing data\n",
    ")\n",
    "\n",
    "# Creating or loading a collection\n",
    "collection = client.get_or_create_collection(name=\"magic_document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results: {'ids': [['1004.0694v1', '0012536v1']], 'embeddings': None, 'documents': [['arXiv:1004.0694v1  [astro-ph.CO]  5 Apr 2010\\nAccepted for publication in ApJ\\nPreprint typeset using LATEX style emulateapj v. 03/07/07\\nOBSERVED SCALING RELATIONS FOR STRONG LENSING CLUSTERS: CO NSEQUENCES FOR\\nCOSMOLOGY AND CLUSTER ASSEMBLY\\nJulia M. Comerford1, Leonidas A. Moustakas2, and Priyamvada Natarajan3,4\\n1Astronomy Department, 601 Campbell Hall, University of Cal ifornia, Berkeley, CA 94720\\n2Jet Propulsion Laboratory, California Institute of Techno logy, MS 169-327, 4800 Oak Grove Drive, Pasadena, CA 91109\\n3Department of Astronomy, Yale University, P. O. Box 208101, New Haven, CT 06511 and\\n4Radcliﬀe Institute for Advanced Study, Byerly Hall, 8 Garde n Street, Cambridge, MA 02138\\nAccepted for publication in ApJ\\nABSTRACT\\nScaling relations of observed galaxy cluster properties are useful tools for constraining cosmological\\nparameters as well as cluster formation histories. One of the key c osmological parameters, σ8, is\\nconstrained using observed clusters of galaxies, although curren t estimates of σ8 from the scaling\\nrelations of dynamically relaxed galaxy clusters are limited by the large scatter in the observed cluster\\nmass-temperature ( M − T ) relation. With a sample of eight strong lensing clusters at 0 . 3 < z < 0. 8,\\nwe ﬁnd that the observed cluster concentration-mass relation ca n be used to reduce the M − T scatter\\nby a factor of 6. Typically only relaxed clusters are used to estimate σ8, but combining the cluster\\nconcentration-mass relation with the M −T relation enables the inclusion of unrelaxed clusters as well.\\nThus, the resultant gains in the accuracy of σ8 measurements from clusters are twofold: the errors on\\nσ8 are reduced and the cluster sample size is increased. Therefore, t he statistics on σ8 determination\\nfrom clusters are greatly improved by the inclusion of unrelaxed clus ters. Exploring cluster scaling\\nrelations further, we ﬁnd that the correlation between brightest cluster galaxy (BCG) luminosity and\\ncluster mass oﬀers insight into the assembly histories of clusters. W e ﬁnd preliminary evidence for\\na steeper BCG luminosity - cluster mass relation for strong lensing clu sters than the general cluster\\npopulation, hinting that strong lensing clusters may have had more a ctive merging histories.\\nSubject headings: cosmological parameters – clusters: individual (3C 220, A 370, Cl 0 024, Cl 0939,\\nCl 2244, MS 0451, MS 1137, MS 2137) – dark matter – galaxies: evolu tion –\\ngalaxies: formation – gravitational lensing\\n1. INTRODUCTION\\nAs the most massive bound systems known, galaxy\\nclusters provide an important link in understanding the\\ncomposition and growth of structure in the Universe.\\nClusters follow a variety of observational scalings of mass\\nwith temperature, luminosity, or cluster counts, and\\nthese scalings are sensitive to cosmological parameters\\nincluding the matter density parameter Ωm, the cosmo-\\nlogical constant density parameter Ω Λ , the dark energy\\nequation-of-state parameter w, and the normalization\\nof the matter power spectrum σ8 (e.g., Haiman et al.\\n2001; Bahcall & Comerford 2002; Levine et al. 2002;\\nSchuecker et al. 2003; Allen et al. 2004; Vikhlinin et al.\\n2009). Such constraints from galaxy clusters comple-\\nment the constraints on cosmological parameters from\\nType Ia supernovae and cosmic microwave background\\nobservations.\\nHowever, useful galaxy cluster constraints on cosmo-\\nlogical parameters depend primarily on accurate deter-\\nminations of cluster masses. Observationally, cluster\\nmasses are typically measured in one of three ways.\\nA long-established method for determining cluster\\nmasses employs the virial theorem and the measure-\\nment of velocities of the galaxies that constitute the clus-\\nter. Based on the three assumptions that the cluster is\\nin virial equilibrium, the galaxy distribution eﬃciently\\ntraces the cluster mass distribution, and the velocity dis-\\npersionsσ of the galaxies are isotropic, the cluster mass\\ncontained within a radius r is estimated M ∼ σ2r/G .\\nHowever, these mass estimates may be biased as a re-\\nsult of galaxy velocity anisotropies or if the galaxy dis-\\ntribution does not follow the total mass distribution (e.g.,\\nBailey 1982).\\nA second method uses cluster X-ray emission as a\\ntracer of cluster masses. The hot intracluster gas, which\\nis the dominant baryonic component of a cluster and is\\ntypically twice the mass of the total mass of the galaxies\\nin a cluster, emits X-rays via bremsstrahlung radiation\\nand atomic line emission. With the temperatureT and\\nradial density ρ(r) proﬁles determined from X-ray spec-\\ntra and surface brightness distributions, the cluster mass\\nis given byM ∼ r2/ρ (r) d(−ρT )/dr . This method as-\\nsumes that the intracluster gas is spherically distributed\\nand is in hydrostatic equilibrium (Evrard et al. 1996).\\nHowever, these assumptions may be incorrect. If the gas\\ndistribution is not spherical, X-ray mass estimates will\\nbe biased by projection eﬀects. Many galaxy clusters are\\nalso not in hydrostatic equilibrium, in particular dynam-\\nically unrelaxed clusters that are undergoing mergers.\\nThere is evidence that the bias of hydrostatic equilib-\\nrium mass is linked to the dynamical state of the galaxy\\ncluster (e.g., Andersson et al. 2009; Zhang et al. 2009).\\nIn addition, the hot gas of galaxy clusters with buoyant\\nbubbles near their cores might indicate a departure from\\nhydrostatic equilibrium (e.g., Churazov et al. 2001).\\nThe most direct estimates of cluster masses employ\\ngravitational lensing distortions of background galaxies.\\nThis technique is free of assumptions about the dynam-\\nical state of the cluster, which enables it in principle to\\nyield more consistent mass estimates, though it is also\\nsensitive to projection eﬀects. More accurate cluster 2 Comerford, Moustakas, & Natarajan\\nmass estimates can in turn provide tighter constraints\\non cosmological parameters, and therefore it is of key im-\\nportance to reduce the errors in cluster mass estimates.\\nFor example, the primary source of error in cluster-\\nbased determinations of σ8 is the error in the\\nmass-temperature relation for relaxed clusters (e.g.,\\nPierpaoli et al. 2003; Henry 2004; Voit 2005). Re-\\ncent studies show that an X-ray independent mass ap-\\nproach such as gravitational lensing provides a unique\\ntool to calibrate the mass - temperature relation (e.g.,\\nSmith et al. 2005; Mahdavi et al. 2008; Zhang et al.\\n2008). Here, we use strong gravitational lensing mass\\nmeasurements of a sample of eight strong lensing clus-\\nters at 0. 3 < z < 0. 8 to accurately measure the galaxy\\ncluster mass-temperature relation. We also include the\\neﬀects of cluster concentrations in an eﬀort to further\\nreduce the scatter in the cluster mass-temperature rela-\\ntion, which would ultimately enable tighter constraints\\nonσ8.\\nIn addition to the correlations that exist between clus-\\nter properties, some observational properties of brightest\\ncluster galaxies (BCGs) also scale with properties of the\\nhost clusters. Whereas scalings between cluster proper-\\nties are sensitive to cosmological parameters, scalings be-\\ntween BCGs and their host clusters provide constraints\\non BCG formation and the evolution of clusters.\\nBCGs are a unique population: they are the most mas-\\nsive and luminous galaxies in the Universe. They are\\ntypically located near the centers of clusters, which sug-\\ngests that a BCG’s formation history is intricately linked\\nto the formation of the cluster itself. However, the for-\\nmation of BCGs is still poorly understood.\\nBCGs may form after their host clusters assemble in\\none of two ways. First, a BCG may be the ﬁrst galaxy to\\nbe dragged in by dynamical friction to the center of the\\ndark matter halo destined to become a cluster, where\\nit then grows through galactic cannibalism by merging\\nwith subsequent galaxies that fall to the center (e.g.,\\nOstriker & Tremaine 1975; Hausman & Ostriker 1978).\\nHowever, this scenario typically requires more than a\\nHubble time to form a BCG because much of the mass\\nof the infalling galaxy is tidally stripped, which reduces\\nthe dynamical friction eﬀect and slows the infall (Merritt\\n1985).\\nBCG formation may also occur after cluster forma-\\ntion if the host cluster’s central cooling ﬂow forms\\nstars at the cluster center and those stars build the\\nBCG (Cowie & Binney 1977). There are several in-\\nstances of ongoing or recent star formation in BCGs\\nthat occupy cooling-ﬂow clusters (e.g., Cardiel et al.\\n1998; Crawford et al. 1999; Hicks & Mushotzky 2005;\\nMcNamara et al. 2006), but it is unclear whether the star\\nformation is fueled by the cooling ﬂows or by cold gas\\nbrought in through recent galaxy mergers (Bildfell et al.\\n2008).\\nIn another scenario, BCGs might form in concert with\\ntheir host clusters. A BCG may begin with several galax-\\nies merging together in a group to form a large galaxy,\\nand then when groups merge as hierarchical structure\\nformation continues, this large galaxy eventually be-\\ncomes a BCG in a massive cluster (e.g., Merritt 1985;\\nDubinski 1998; Boylan-Kolchin et al. 2006).\\nHere, we examine the correlation between BCG lumi-\\nnosity and cluster mass in eight strong lensing clusters at\\n0. 3 < z < 0. 8. This will enable constraints not only on\\nBCG and cluster formation in general, but also on how\\nthe BCGs in strong lensing clusters may have formed\\nand evolved diﬀerently than BCGs in the general cluster\\npopulation.\\nThe rest of this paper is organized as follows. In Sec-\\ntion 2 we describe the selection of our cluster sample,\\nand Section 3 gives the masses, dynamical states, and X-\\nray temperatures for these clusters. In Section 4 we ﬁnd\\ntheM − T relation for the relaxed clusters in our sam-\\nple and show how the inclusion of cluster concentrations\\nboth signiﬁcantly reduces the scatter in theM − T rela-\\ntion and lifts the restriction on cluster dynamical state.\\nIn Section 5 we identify the BCGs in our sample and\\nmeasure their luminosities. We use these luminosities\\nin Section 6 to measure the correlation between BCG\\nluminosity and cluster mass, and we ﬁnd preliminary ev-\\nidence that strong lensing clusters may have more ac-\\ntive merging histories than the general cluster popula-\\ntion. Section 7 presents our conclusions. Throughout\\nthis paper, we adopt a spatially ﬂat cosmological model\\ndominated by cold dark matter and a cosmological con-\\nstant (Ωm = 0 . 3, Ω Λ = 0 . 7, h = 0 . 7).\\n2. SAMPLE SELECTION\\nWe base our sample on 10 well-known strong lens-\\ning clusters analyzed in Comerford et al. (2006). All\\n10 clusters haveHubble Space Telescope (HST) imag-\\ning, which make possible the mass determinations and\\nphotometry measurements central to this paper. How-\\never, there are no published arc redshifts for two of the\\nclusters, Cl 0016+1609 and Cl 0054−27, which limits\\nthe strong lensing determination of their cluster masses\\nto the unknown factorDs/D ls, the ratio of the angular\\ndiameter distances to the source and between the lens\\nand source. Consequently we remove these two clusters,\\nand our sample consists of the remaining eight clusters\\nat 0. 3 < z < 0. 8: ClG 2244 −02, Abell 370, 3C 220.1,\\nMS 2137.3 −2353, MS 0451.6 −0305, MS 1137.5+6625,\\nCl 0939+4713, and ZwCl 0024+1652.\\n3. CLUSTER PROPERTIES\\nStrong correlations are found between cluster observ-\\nables, and the resultant scaling relations clearly encapsu-\\nlate key information about cosmological parameters and\\nthe assembly history of clusters. Cluster masses are a\\ncomponent of many cluster scaling relations, and we mea-\\nsure strong lensing masses for our sample of clusters and\\ncompare these to mass estimates from the distributions\\nof cluster X-ray gas. Based on these comparisons and\\nother observable properties of the cluster, we determine\\nthe dynamical state of each cluster as relaxed or unre-\\nlaxed. We also present cluster X-ray temperatures, which\\nare another component of cluster scaling relations.\\n3.1. Cluster Strong Lensing Mass Determination\\nWe model each cluster mass distribution with an ellip-\\ntical Navarro-Frenk-White (NFW; Navarro et al. 1996,\\n1997) dark matter halo centered on the BCG, using\\nthe best-ﬁt NFW parameters found by Comerford et al.\\n(2006). Strong lensing arcs with measured redshifts ob-\\nserved in a cluster constrain its mass distribution, and\\nComerford et al. (2006) use the arcs to characterize best-\\nﬁt NFW ellipsoids to each cluster. With the NFW dark Formation Histories of Strong Lensing Clusters 3\\nmatter halos completely deﬁned in this way, we can de-\\ntermine any cluster radiusr∆ as the radius at which the\\ndensity of the halo is ∆ times the critical density at the\\ncluster redshift.\\nLack of information about the clusters’ three-\\ndimensional shapes prevents us from calculating their\\nelliptical masses, but instead we determine the equiv-\\nalent mass of a spherical NFW halo. With the\\nComerford et al. (2006) best-ﬁt scale convergenceκs and\\nscale radius rs, we estimate the cluster mass within ra-\\ndius r∆ as\\nM∆ = 4 πΣ critκsr2\\ns\\n[\\nln(1 + x) − x\\n1 + x\\n]\\n, (1)\\nwhere x ≡ r∆ /r s and Σ crit is the critical surface mass\\ndensity, deﬁned as\\nΣ crit ≡ c2\\n4πG\\nDs\\nDlDls\\n, (2)\\nwhich depends on the angular diameter distances Dl, s, ls\\nfrom the observer to the lens, to the source, and from\\nthe lens to the source, respectively.\\nWe estimate the errors in mass by propagating the\\nerrors in the best-ﬁt NFW parameters. As detailed in\\nComerford et al. (2006) these errors are quite small but\\nare realistic, because the reproduced lensed image is sen-\\nsitive to slight variations in a parameter’s value. How-\\never, we note that these errors are relevant only to the\\nchoice of lens model and data and do not represent a\\nglobal systematic uncertainty.\\nWe use the method described here to measure the lens-\\ning cluster masses in Table 1, as well as the cluster masses\\nM200 and M2500 in Table 2.\\n3.2. Dynamical State of Clusters:\\nRelaxed vs. Unrelaxed\\nSince one of our aims is to measure the mass-\\ntemperature relation for relaxed lensing clusters, we must\\ndetermine which of the eight clusters in our sample are\\ndynamically relaxed. X-ray cluster mass estimates are\\nbased on the assumption that the cluster is in hydro-\\nstatic equilibrium, and if a cluster is relaxed it is also\\nin hydrostatic equilibrium. Therefore, X-ray mass mea-\\nsurements for relaxed clusters should be accurate and\\nconsistent with lensing mass measurements.\\nWe use X-ray mass estimates from the literature, where\\nthe X-ray masses are measured for each cluster at two or\\nthree diﬀerent radii. For each cluster, Table 1 gives the\\nlensing mass and X-ray mass measured within the two\\nor three diﬀerent cluster radii. Table 1 also shows the\\nlensing mass to X-ray mass ratio and the reducedχ 2\\nof the comparison of lensing and X-ray masses. For six\\nclusters, at all radii at which masses were measured, the\\nratio of lensing mass to X-ray mass is consistent with\\nunity and the reducedχ 2 is ≲ 1, suggesting that these\\nsix clusters could be relaxed. Additional observational\\nevidence in§ 3.2.1 and § 3.2.2 shows that four of these\\nsix clusters are relaxed, while the remaining two clusters\\nare unrelaxed.\\nFor at least one of the radii considered, the two clusters\\nMS 2137 −23 and Cl 0939+4713 each exhibit lensing to\\nX-ray mass ratios that are inconsistent with unity and\\nreducedχ 2 that are greater than unity, which is evidence\\nthat the clusters are unrelaxed. We measure masses for\\nMS 2137−23 within three diﬀerent radii, and within one\\nof these radii the mass ratio is inconsistent with unity\\nand the reducedχ 2 is greater than unity. However there\\nis opposing evidence, given in § 3.2.1, that characterizes\\nMS 2137 −23 as a relaxed cluster. For Cl 0939+4713,\\nthe mass ratios measured at both radii considered are\\ninconsistent with unity and both reducedχ 2 are much\\ngreater than unity, suggesting Cl 0939+4713 may be an\\nunrelaxed cluster. In§ 3.2.2 we present more evidence in\\nsupport of this conclusion.\\nAdditional information about the dynamical state of\\na cluster can be found in its X-ray emission map. For\\nexample, the position of the BCG relative to the peak in\\nthe cluster’s X-ray proﬁle may be evidence of a cluster’s\\ndynamical state: if the two are coincident the cluster\\nis likely relaxed, otherwise it is likely unrelaxed. The\\ncentroid shift is one means of quantifying this positional\\ndiﬀerence (e.g., Mohr et al. 1993; Jeltema et al. 2008).\\nAdditionally, a smooth distribution of X-ray gas indi-\\ncates the cluster is likely in a relaxed state. However, if\\nthe X-ray gas is distributed irregularly or shows evidence\\nof shocks or substructure, the cluster is likely unrelaxed\\nand undergoing a merger. Below we examine evidence\\nfor the dynamical state of each cluster individually and\\nlabel each cluster as relaxed or unrelaxed (these labels\\nare also given in Table 1). We ﬁrst discuss the four re-\\nlaxed clusters, then the four unrelaxed clusters.\\n3.2.1. Relaxed Clusters\\n• Cl 2244 −02: We ﬁnd that X-ray and lensing\\nmasses for Cl 2244 −02 are consistent (Table 1)\\nand Ota et al. (1998) also ﬁnd consistent X-ray and\\nlensing masses, suggesting that hydrostatic equilib-\\nrium is a valid assumption for Cl 2244−02 and that\\nit is a relaxed cluster.\\n• 3C 220.1: The radial proﬁle of X-ray emission\\nfrom 3C 220.1 shows no sign of irregularity and\\nthe proﬁle is well-ﬁt by a model assuming hydro-\\nstatic equilibrium, which suggest that 3C 220.1 is\\na relaxed cluster (Worrall et al. 2001).\\n• MS 2137 −23: The X-ray and strong lensing\\nmasses of MS 2137 −23 are in good agreement\\n(Allen 1998), indicating that it is in a relaxed state.\\nMany relaxed clusters also have cooling ﬂows, such\\nas the massive cooling ﬂow in MS 2137−23 (Allen\\n1998; Wu 2000).\\n• MS 1137 +66: The cluster MS 1137+66 not\\nonly has consistent X-ray and weak lensing masses\\n(Table 1), but also has a small centroid shift\\n(Maughan et al. 2008) and may host a moder-\\nate cooling ﬂow (Donahue et al. 1999). In addi-\\ntion, Sunyaev Zel’dovich observations of the cluster\\nshow no obvious substructure (Cotter et al. 2002).\\nThese properites connote that MS 1137+66 is a re-\\nlaxed cluster.\\n3.2.2. Unrelaxed Clusters\\n• Abell 370 : Abell 370 hosts two cD galaxies,\\nand there are X-ray peaks centered on each cD 4 Comerford, Moustakas, & Natarajan\\nTABLE 1\\nComparisons between strong lensing and X-ray cluster mass estimates.\\nCluster ∆ r M lens(≤ r) MX−ray(≤ r) Mlens(≤ r)/ Reduced Dynamical Reference\\n(h−1\\n70 Mpc) (10 14 h−1\\n70 M⊙) (10 14 h−1\\n70 M⊙) MX−ray(≤ r) χ 2 State\\nClG 2244 − 02 500 Ω 0.427 0. 83+0.26\\n−0.20 2. 85+1.25\\n−0.99 1. 50+1.07\\n−0.63 1. 90+2.81\\n−1.18 0.91 Relaxed 1\\n18π 2 Ω 0.427 1. 31+0.42\\n−0.31 4. 22+1.63\\n−1.27 2. 37+1.73\\n−0.99 1. 78+2.46\\n−1.06 0.86 1\\nAbell 370 500 Ω 0.427 1. 15+0.28\\n−0.20 6. 45+2.04\\n−1.52 4. 19+2.06\\n−1.30 1. 54+1.40\\n−0.75 0.85 Unrelaxed 1\\n18π 2 Ω 0.427 1. 81+0.44\\n−0.32 9. 25+2.50\\n−1.92 6. 73+3.57\\n−2.16 1. 37+1.20\\n−0.66 0.49 1\\n3C 220.1 500 Ω 0.427 1. 17+0.45\\n−0.25 3. 22+1.37\\n−0.80 5. 80+5.25\\n−2.19 0. 56+0.72\\n−0.34 0.44 Relaxed 1\\n18π 2 Ω 0.427 1. 74+0.67\\n−0.37 4. 25+1.56\\n−0.92 8. 64+7.85\\n−3.27 0. 49+0.59\\n−0.29 0.59 1\\nMS 2137.3 − 2353 2500 0 . 46+0.02\\n−0.03 1. 62+0.18\\n−0.19 1. 89+0.25\\n−0.31 0. 86+0.28\\n−0.19 0.65 Relaxed 2\\n500 Ω 0.427 1. 07+0.10\\n−0.06 2. 73+0.34\\n−0.30 3. 16+0.60\\n−0.36 0. 86+0.23\\n−0.22 0.57 1\\n18π 2 Ω 0.427 1. 69+0.15\\n−0.10 3. 40+0.40\\n−0.37 4. 99+0.95\\n−0.57 0. 68+0.18\\n−0.17 3.5 1\\nMS 0451.6 − 0305 500 Ω 0.427 1. 38+0.25\\n−0.20 13. 4+3.1\\n−2.6 8. 90+3.44\\n−2.31 1. 50+1.00\\n−0.63 1.2 Unrelaxed 1\\n18π 2 Ω 0.427 2. 09+0.38\\n−0.30 18. 3+3.7\\n−3.2 13. 6+5.4\\n−3.6 1. 34+0.86\\n−0.55 0.68 1\\nMS 1137.5+6625 500 Ω 0.427 1. 41+1.26\\n−0.45 6. 80+7.22\\n−2.64 12. 5+32.0\\n−6.7 0. 54+1.87\\n−0.45 0.082 Relaxed 1\\n18π 2 Ω 0.427 2. 06+1.84\\n−0.66 9. 10+8.34\\n−3.08 18. 2+46.9\\n−9.8 0. 50+1.58\\n−0.41 0.099 1\\nCl 0939+4713 0 . 36 0 . 38 ± 0. 05 0 . 72 ± 0. 21 0 . 53+0.31\\n−0.17 2.5 Unrelaxed 3\\n0. 71 0 . 69 ± 0. 08 2 . 13 ± 0. 50 0 . 32+0.15\\n−0.09 8.1 3\\nZwCl 0024+1652 500 Ω 0.427 0. 94+0.39\\n−0.21 2. 02+0.97\\n−0.54 2. 31+2.34\\n−0.91 0. 87+1.26\\n−0.56 0.027 Unrelaxed 1\\n18π 2 Ω 0.427 1. 45+0.61\\n−0.32 2. 77+1.15\\n−0.64 3. 59+3.63\\n−1.41 0. 77+1.03\\n−0.48 0.094 1\\nReferences. — (1) Ota & Mitsuda (2004); (2) Allen et al. (2001); (3) De Fil ippis et al. (2003).\\nTABLE 2\\nCluster lensing masses and X-ray temperatures.\\nCluster z M 200 M2500 kT Reference\\n(1014 h−1\\n70 M⊙) (10 14 h−1\\n70 M⊙) (keV)\\nClG 2244 − 02 0.33 4 . 5 ± 0. 9 1 . 3 ± 0. 2 4 . 85+1.25\\n−0.96 1\\nAbell 370 0.375 9 . 0 ± 1. 0 2 . 9 ± 0. 3 7 . 20+0.75\\n−0.77 1\\n3C 220.1 0.62 3 . 1 ± 0. 3 0 . 91 ± 0. 10 5 . 6+1.5\\n−1.1 2\\nMS 2137.3 − 2353 0.313 2 . 9 ± 0. 4 1 . 5 ± 0. 2 4 . 57+0.41\\n−0.35 1\\nMS 0451.6 − 0305 0.55 18 ± 2 6 . 3 ± 0. 7 8 . 62+1.54\\n−1.21 1\\nMS 1137.5+6625 0.783 6 . 5 ± 0. 7 1 . 5 ± 0. 2 6 . 70+1.84\\n−1.46 1\\nCl 0939+4713 0.41 0 . 71 ± 0. 11 0 . 21 ± 0. 03 7 . 6+2.8\\n−1.6 3\\nZwCl 0024+1652 0.395 2 . 3 ± 0. 2 0 . 69 ± 0. 07 5 . 17+1.95\\n−1.34 1\\nReferences. — (1) Horner (2001); (2) Ota et al. (2000); (3) Schindler et a l. (1998).\\n(Mellier et al. 1994). The two cD galaxies are mov-\\ning relative to each other at 1000 km s−1, signaling\\nthat Abell 370 is an unrelaxed cluster undergoing\\na merger (Kneib et al. 1993).\\n• Cl 0939 +4713: X-ray observations of\\nCl 0939+4713 show evidence for substructure\\n(Schindler & Wambsganss 1996), and the disagree-\\nment between lensing and X-ray masses shown\\nin Table 1 further suggests that Cl 0939+4713 is\\nnot in hydrostatic equilibrium. These observations\\nindicate Cl 0939+4713 is an unrelaxed cluster.\\n• Cl 0024 +17: The two dark matter clumps\\nnear the center of Cl 0024+17 are separated\\nin redshift, implying that it is a merging clus-\\nter (Natarajan et al. 2009). There is additional\\nevidence for substructure in Cl 0024+17 in its\\nmass models, which require substructure to pro-\\nduce a good ﬁt to the cluster’s lensing arcs\\n(Broadhurst et al. 2000). The redshifts of the\\nmember galaxies are distributed bimodally, fortify-\\ning the evidence that Cl 0024+17 may have under-\\ngone a merger with another cluster (Czoske et al.\\n2002). The evidence implies that Cl 0024+17 is an\\nunrelaxed cluster.\\n• MS 0451.6 −0305: The distribution of mass\\nwithin the central 1 ′ of MS 0451.6 −0305 is not\\nsmooth, and the centroid shift indicates the BCG\\nis not located at the X-ray peak (Borys et al. 2004;\\nMaughan et al. 2008). These observations suggest\\nthat MS 0451.6−0305 is unrelaxed.\\n3.3. Cluster X-ray Temperatures\\nThe temperature of the intracluster medium is com-\\nmonly measured using its X-ray emission in one of\\nseveral ways: through ﬁts to the cluster’s observed\\nX-ray spectrum (yielding the spectroscopic tempera-\\ntureTs), through weighting by the mass of the gas\\nelement (yielding the mass-weighted temperature Tm),\\nor through weighting by the emissivity of the gas Formation Histories of Strong Lensing Clusters 5\\nelement (yielding the emission-weighted temperature\\nTem). However, the spectroscopic temperature Ts is\\nsystematically lower than the mass-weighted temper-\\natureTm and the emission-weighted temperature Tem\\n(Mathiesen & Evrard 2001; Mazzotta et al. 2004), so an\\naccurate temperature comparison across diﬀerent clus-\\nters requires consistent temperature measurements.\\nTo ensure that the cluster temperatures we use for our\\nsample are as consistent as possible, we use the mean\\ncluster temperatures derived from the single-temperature\\nmodel ﬁts ofASCA data in Horner (2001). This large,\\nhomogeneous catalog of spectroscopic cluster tempera-\\ntures includes six of our clusters, and for the remain-\\ning two clusters, 3C 220.1 and Cl 0939+4713, we remain\\nas consistent as possible by using spectroscopic temper-\\natures from single-temperature ﬁts. Table 2 gives the\\ncluster temperatures and the corresponding references.\\nWe note that none of the temperatures we use apply cor-\\nrections for cool cores at the cluster centers.\\nSome clusters in our sample also have temperature\\nmeasurements from Chandra and XMM-Newton data.\\nSpeciﬁcally, 3C 220.1 has a Chandra temperature of\\n8. 5+3. 7\\n−2. 3 keV within 10 – 45 ′′ (Worrall et al. 2001);\\nMS 0451.6 −0305 has a Chandra temperature of 6 . 7+0. 6\\n−0. 5\\nkeV within r500 (Maughan et al. 2008); MS 1137.5+6625\\nhas a Chandra temperature of 5 . 8+0. 7\\n−0. 6 keV within r500\\n(Maughan et al. 2008); and Cl 0024+17 has an average\\nChandratemperature of 4 . 47+0. 83\\n−0. 54 keV (Ota et al. 2004)\\nand an XMM-Newton temperature of 3 . 52 ± 0. 17 keV\\nwithin 3 ′ (Zhang et al. 2005). Use of Chandra or XMM-\\nNewton temperatures could change the results of the\\nmass-temperature relation. However, because Chandra\\nand XMM-Newton temperatures have been measured for\\nonly a subset of our sample, and because these temper-\\natures are measured within inconsistent cluster radii, we\\ndo not useChandra and XMM-Newton measurements in\\nour determination of the cluster mass-temperature rela-\\ntion below.\\n4. THE MASS-TEMPERATURE RELATION\\nTheoretical arguments suggest a correlation between\\ncluster mass and X-ray temperature for relaxed clus-\\nters, which provides the link between the gas in a clus-\\nter and its mass. Here we determine the cluster mass-\\ntemperature relation for relaxedstrong lensing clusters,\\nand we also explore the correlation between the scatter\\nin cluster temperature and the scatter in cluster concen-\\ntration to establish a general mass-temperature relation\\nthat is independent of the dynamical state of the clusters.\\n4.1. The M − T Relation for Relaxed Strong Lensing\\nClusters\\nA correlation between cluster mass and cluster X-ray\\ngas temperature in relaxed clusters is expected as a direct\\nconsequence of theoretical arguments. If a cluster’s X-\\nray gas is in virial and hydrostatic equilibrium, then the\\ntheoretical expectation is that cluster mass scales with X-\\nray temperature asE(z)M∆ = A(∆) T 1. 5, where E(z) =\\nH(z)/H 0 =\\n√\\nΩ m(1 + z)3 + Ω Λ for a ﬂat Universe, M∆\\nis the cluster mass within the radius where the mean\\nmass density is ∆ times the critical density, andA(∆) is\\nthe ∆-dependent normalization.\\nMS2137\\nCl2244\\nCl0024\\n3C220\\nMS1137\\nA370\\nCl0939\\nMS0451\\nFig. 1.— The mass-temperature relation for observed strong\\nlensing clusters. Unrelaxed clusters (open circles) are no t included\\nin the ﬁt, and the relaxed clusters (black points) are ﬁt by a p ower\\nlaw with slope α = 1 . 43 (black solid line). The 1 σ scatter for\\nall eight clusters is large, ∆(log[ E(z)M2500]) = 0 . 2 (black dashed\\nlines). Also shown are the other M − T relations for observational\\nsamples that use spectroscopic temperatures as we do: 17 wea k\\nlensing clusters with 3 . 6 < T s (keV) < 9. 8 (Hoekstra 2007; red\\ndotted line), 13 relaxed X-ray clusters with 0 . 7 < T s (keV) <\\n8. 9 (Vikhlinin et al. 2006; blue dash-dotted line), 10 relaxed X-ray\\nclusters with 2 . 2 < T s (keV) < 8. 3 (Arnaud et al. 2005; orange\\ndashed line), and six relaxed X-ray clusters with 3 . 7 < T s (keV) <\\n8. 3 (Arnaud et al. 2005; green long dashed line). We ﬁnd that\\nour slope is in agreement with both the theoretical expectat ion\\nof α = 1 . 5 and measurements of α by other observations. For a\\ndetailed comparison to these and other estimates of the M − T\\nrelation, see Table 3.\\nThe critical overdensity ∆ = 2500 is commonly used in\\ncluster analyses because in the central regions enclosed\\nbyr2500, Chandra cluster temperature proﬁles can be\\nmeasured even at high redshifts (e.g., up to z = 0 . 9 in\\nAllen et al. 2004). The overdensity ∆ = 2500 is therefore\\nappropriate for our cluster sample, which extends toz =\\n0. 8. Using the overdensity ∆ = 2500, we can write the\\ncluster mass-temperature relation in power law form as\\nE(z)\\n( M2500\\n1014 h−1\\n70 M⊙\\n)\\n= A\\n( kT\\n5 keV\\n) α\\n. (3)\\nUsing our sample of four dynamically relaxed lensing\\nclusters given in § 3.2.1, a best ﬁt to the power law M −T\\nrelation yields A = 1 . 60 ± 3. 42 and α = 1 . 43 ± 1. 28,\\nconsistent with the theoretical expectation of α = 1 . 5.\\nFigure 1 shows this best-ﬁt relation, for which the RMS\\nscatter is 360% for all eight clusters and 500% for the\\nfour unrelaxed clusters.\\nWe compare with other observations and simulations of\\nthe M − T relation in Table 3, including those that used\\nspectroscopic temperatures Ts and those that used mass-\\nweighted temperatures Tm. For cases where the temper-\\nature normalization is not 5 keV and/or the mass scaling\\nis not 1014 h−1\\n70 M⊙, we recalculate A using the published 6 Comerford, Moustakas, & Natarajan\\nTABLE 3\\nPower Law Fits to theM − T Relation.\\nA α Methoda kT (keV)b Sample Reference\\n1. 60 ± 3. 42 1 . 43 ± 1. 28 SL 4 . 6 < T s < 6. 7 4 relaxed SL clusters 1\\n2. 0 ± 0. 29 1 . 34+0.30\\n−0.28 WL 3 . 6 < T s < 9. 8 17 WL clusters 2\\n1. 79 ± 0. 07 1 . 64 ± 0. 06 X-ray 0 . 7 < T s < 8. 9 13 relaxed clusters 3\\n2. 06 ± 0. 10 1 . 58 ± 0. 07 X-ray 0 . 6 < T m < 9. 3 13 relaxed clusters 3\\n1. 69 ± 0. 05 1 . 70 ± 0. 07 X-ray 2 . 2 < T s < 8. 3 10 relaxed clusters 4\\n1. 79 ± 0. 06 1 . 51 ± 0. 11 X-ray 3 . 7 < T s < 8. 3 6 relaxed clusters 4\\n1. 88 ± 0. 26 1 . 52 ± 0. 36 X-ray 5 . 6 < T m < 15. 3 5 relaxed WL or SL clusters 5\\n1. 97 ± 0. 07 1 . 54 ± 0. 02 Simulation Tm M2500 > 4 × 1014 h−1\\n70 M⊙ clusters 6\\nin hydrodynamics simulation\\nReferences. — (1) This paper; (2) Hoekstra (2007); (3) Vikhlinin et al. ( 2006); (4) Arnaud et al. (2005);\\n(5) Allen et al. (2001); (6) Kay et al. (2005).\\naMethod used to determine the cluster mass, where SL is strong lensing and WL is weak lensing.\\nb Temperature range of the cluster sample, where Ts is the spectroscopic temperature and Tm is the mass-\\nweighted temperature.\\nCl0939\\nCl0024\\n3C220\\nMS1137\\nCL2244\\nA370\\nMS2137\\nMS0451\\nFig. 2.— The correlation between the diﬀerence ∆ T between\\nthe observed X-ray temperatures and the predicted temperat ures\\nfrom the M − T relation and the diﬀerence ∆ c between the mea-\\nsured concentrations and the predicted concentrations fro m the\\nc − M relation. The eight strong lensing clusters in our sample ar e\\nrepresented, and the solid line shows the best-ﬁt line to the data\\n∆ T = ( − 2. 75 keV)∆ c − (1. 56 keV). The dashed lines show the 1 σ\\nscatter ∆(∆ T ) = 0 . 9 keV.\\nslope α , a temperature normalization of 5 keV, and a\\nmass normalization of 10 14 h−1\\n70 M⊙. To be conservative,\\nwe assume the fractional error in A is unchanged.\\nThe observations we compare span varying temper-\\nature ranges, and there is some evidence that the\\nM− T relation steepens for cooler clusters (e.g.,\\nNevalainen et al. 2000; Finoguenov et al. 2001); for ex-\\nample, Arnaud et al. (2005) ﬁnd a slope ofα = 1 . 51 for\\nclusters with 3 . 7 < T s < 8. 3 keV, which increases to\\nα = 1 . 70 for clusters with 2 . 2 keV < T s < 8. 3 keV. The\\ntemperature range we probe (4 . 6 keV < T s < 6. 7 keV) is\\nlikely too small to exhibit a signiﬁcant change in slope,\\nbut we lack a large enough statistical sample to test this\\nCl2244\\n3C220\\nMS1137\\nMS2137\\nCl0939\\nCl0024\\nA370\\nMS0451\\nFig. 3.— The mass-temperature relation, after correcting for\\nthe scatter in temperature, for observed strong lensing clu sters.\\nAs in Figure 1, open circles represent unrelaxed clusters an d black\\npoints represent relaxed clusters. We adjust the temperatu re of\\neach cluster according to its concentration and the ∆ T − ∆ c rela-\\ntion. The best-ﬁt M − T relation for relaxed clusters, derived in\\n§ 4.1, is shown as the solid line. The 1 σ scatter for all eight clus-\\nters is ∆(log[ E(z)M2500]) = 0 . 1 (black dashed lines), signiﬁcantly\\nsmaller than the scatter in the uncorrected M − T relation (see\\nFigure 1).\\nproperly.\\nWe ﬁnd that our best-ﬁt slope α is consistent with\\nboth the theoretical expectation and the slopes derived\\nby other observations and simulations of clusters. Our\\nbest-ﬁt normalizationA is somewhat lower than, but\\nstill consistent with, the normalizations found by the\\nother observations and simulations. We ﬁnd that relaxed\\nstrong lensing clusters follow the sameM − T relation as\\nrelaxed clusters in general. Formation Histories of Strong Lensing Clusters 7\\n4.2. Correlation between the Temperature Scatter and\\nConcentration Scatter\\nWe have derived an M − T power-law relation for re-\\nlaxed lensing clusters, but a more general M −T relation\\nincluding both relaxed and unrelaxed clusters may be\\npossible if we account for the diﬀerences in cluster con-\\ncentrations. First, we deﬁne the virial radius of a cluster\\nas the radiusrvir at which the average cluster density\\nequals ∆ vir(z) times the mean density at the cluster red-\\nshift z, where ∆ vir(z) ≃ (18π2 + 82x − 39x2)/ (1 + x) and\\nx ≡ Ω m(z) − 1 (Bryan & Norman 1998). Using the scale\\nradius rs of the best-ﬁt NFW proﬁle to each cluster, the\\ncluster concentration is deﬁned as cvir ≡ rvir/r s.\\nSince more concentrated clusters are expected to\\nform at higher redshifts (e.g., Navarro et al. 1997;\\nWechsler et al. 2002), if the cluster X-ray gas cools with\\ntime there might be a correlation between high cluster\\nconcentrations and low cluster temperatures. In addi-\\ntion, mergers with other clusters or groups may deplete\\nthe central mass densities in clusters while shock-heating\\nthe cluster gas, producing high cluster temperatures for\\nlow cluster concentrations. Here, we analyze whether\\nthere is any such correlation between the scatter in tem-\\nperature and the scatter in concentration for our sample\\nof eight strong lensing clusters.\\nCluster concentrations cvir and cluster virial masses\\nMvir ≡ M(≤ rvir) are determined by strong lensing\\nmeasurements for each of the clusters in our sample\\nin Comerford & Natarajan (2007). The concentration\\ncvir = 16 determined by strong lensing measurements of\\nMS 2137.3 −2353 is known to be overestimated because\\nthe cluster’s dark matter halo is likely elongated along\\nor near the line of sight (Gavazzi 2005), so we instead\\nuse the concentrationcvir = 8 . 75 derived from the X-\\nray mass proﬁle for MS 2137.3 −2353 (Schmidt & Allen\\n2007). We note that if the lensing concentration were\\nused for MS 2137.3−2353, Equation 5 would be ∆ T =\\n(−0. 07 keV)∆ c − (0. 49 keV).\\nFrom a sample of 62 galaxy clusters,\\nComerford & Natarajan (2007) ﬁnd a power-law\\nrelation between cluster concentrationcvir and cluster\\nvirial mass Mvir of\\ncvir = 14. 5 ± 6. 4\\n(1 + z)\\n( Mvir\\n1. 3 × 1013 h−1 M⊙\\n) −0. 15±0. 13\\n, (4)\\nwhere z is the cluster redshift. For each of the eight clus-\\nters in our sample, we calculate the diﬀerence ∆ c between\\nthe measured concentration and the concentration pre-\\ndicted by the abovec−M relation. We also calculate the\\ndiﬀerence ∆ T between the measured X-ray temperature\\nand the temperature predicted by the M − T relation we\\ndetermined in § 4.1 for the four relaxed clusters.\\nFigure 2 shows the results of these ∆ T and ∆ c calcu-\\nlations. The best-ﬁt line to the data is\\n∆T = ( −2. 75 keV ±0. 07 keV)∆ c−(1. 56 keV ±0. 49 keV) ,\\n(5)\\nsuggesting that indeed higher (lower) temperature clus-\\nters tend to have lower (higher) concentrations.\\n4.3. The M − T Relation for All Strong Lensing\\nClusters\\nUsing the relation between the scatter in cluster tem-\\nperature and the scatter in cluster concentration for the\\neight strong lensing clusters ( § 4.2), we adjust for the\\napparent dependence of cluster temperatures on clus-\\nter concentrations. We use ∆c for each cluster to cal-\\nculate its corresponding ∆ T from the best-ﬁt relation\\ngiven in Equation 5. We then subtract this ∆ T from\\nthe measured temperature to obtain a corrected temper-\\natureTcorr, and we illustrate the resultant temperature-\\ncorrected M − T relation in Figure 3. The ﬁgure also\\nshows the relation we derived in § 4.1 for the four re-\\nlaxed clusters, where A = 1 . 60 and α = 1 . 43.\\nWe ﬁnd that cluster concentration, mass, and X-ray\\ntemperature are tightly correlated, and as a result in-\\ncorporating the ∆T − ∆ c relation signiﬁcantly reduces\\nthe scatter in the M − T relation. Comparing Figure 3\\nto Figure 1 underscores the impact of our temperature\\ncorrection in reducing the scatter in theM − T relation.\\nThe temperature correction reduces the RMS scatter for\\nall eight clusters by a factor of 6, from 360% to 60%, and\\nmore signiﬁcantly, reduces the RMS scatter for the four\\nunrelaxed clusters by a factor of 30, from 500% to 15%.\\n(The RMS scatter for the four relaxed clusters increases\\nfrom 26% to 83%, possibly because the temperatures we\\nuse do not correct for cool cores at the cluster centers.)\\nWith the temperature correction, even unrelaxed clusters\\nfollow theM −T relation we originally derived using only\\nthe relaxed clusters ( § 4.1). Therefore, we suggest this\\ntemperature correction as a tool for establishing a uni-\\nversalM − T relation that applies to all galaxy clusters\\nregardless of their dynamical state.\\nThe error in the measurement of σ8 from cluster counts\\ndepends directly on the error in the cluster M − T re-\\nlation; for example, a 25% 1 σ uncertainty in the zero\\npoint of the M − T relation corresponds to a 10% 1 σ\\nuncertainty in σ8 (Evrard et al. 2002). Consequently, we\\nﬁnd that the temperature correction not only reduces\\nthe scatter in theM − T relation, but also signiﬁcantly\\nreduces the error in the corresponding measurement of\\nσ8.\\nAn alternate cluster scaling relation that also has\\nlower scatter than the traditional M − T relation is\\nthe YX − M500 relation (Kravtsov et al. 2006). Here,\\nM500 is the cluster mass within the radius r500 enclos-\\ning an overdensity of 500 relative to the critical density,\\nYX = MgTX, Mg is the cluster gas mass within r500, and\\nTX is the mean spectral X-ray temperature of the cluster.\\nHowever, this scaling relation is limited by the assump-\\ntions that the gas is both spherically distributed and in\\nhydrostatic equilibrium. Our scaling relation oﬀers the\\nadvantage that it is based on lensing mass estimates that\\nare free of these assumptions.\\n5. BCG PROPERTIES\\nIn addition to the interdependencies of many cluster\\nproperties, properties of the BCG have also been shown\\nto correlate with the host cluster. Here we identify the\\nBCG in each of our clusters, measure the luminosity of\\neach BCG, and examine the correlation between BCG\\nluminosity and host cluster mass for our strong lensing\\nsample.\\n5.1. BCG Determination\\nWe select each cluster’s BCG as the brightest member\\ngalaxy. Each BCG corresponds to the lens galaxy or one\\nof the lens galaxies used to determine the cluster mass 8 Comerford, Moustakas, & Natarajan\\nTABLE 4\\nBCG luminosities.\\nCluster BCG a LK,BCG LK,passive,BCG Reference\\n(1011 h−2\\n70 L⊙) (10 11 h−2\\n70 L⊙)\\nClG 2244 − 02 1 . 03 ± 0. 09 0 . 96 ± 0. 09 1\\nAbell 370 G1 1 . 5 ± 0. 1 1 . 3 ± 0. 1\\n3C 220.1 6 . 4 ± 0. 4 5 . 3 ± 0. 3\\nMS 2137.3 − 2353 8 . 98 ± 0. 09 0 . 84 ± 0. 08\\nMS 0451.6 − 0305 4 . 3 ± 0. 3 3 . 7 ± 0. 3 2\\nMS 1137.5+6625 15 ± 2 11 ± 1\\nCl 0939+4713 G1 1 . 9 ± 0. 2 1 . 7 ± 0. 2 3\\nZwCl 0024+1652 #362 1 . 69 ± 0. 07 1 . 51 ± 0. 06 4, 5\\nReferences. — (1) Bautz et al. (1982); (2) Ellingson et al. (1998); (3)\\nDe Filippis et al. (2003); (4) Kneib et al. (2003); (5) Moran e t al. (2005).\\na See Comerford et al. (2006) for identiﬁcation of the galaxie s by name.\\ndistribution in Comerford et al. (2006). When multiple\\nlens galaxies were used to model a single cluster, we iden-\\ntify which of the lens galaxies is the BCG in Table 4, and\\nwe also note references that conﬁrm the BCG selection.\\n5.2. BCG Luminosity Determination\\nFor each cluster we have HST imaging taken in\\nsome combination of the ﬁlters F450W, F555W,\\nF675W, F702W, and F814W. Using Source EXtrac-\\ntor (Bertin & Arnouts 1996), we measure MAG\\nAUTO\\nmagnitudes for the BCG galaxies. We estimate the mag-\\nnitude uncertainties by adding in quadrature the error\\nin the measured ﬂux and the estimated background sub-\\ntraction error, which is the product of the area of the\\nextraction aperture and the RMS variation of the sub-\\ntracted background ﬂux. We calculate the BCG lumi-\\nnosities using the available photometry in an observed\\nband as the normalization factor on two types of spec-\\ntral energy distribution templates, and then compute the\\nrest-frame magnitudes and luminosities in several bands\\nincludingK-band. The templates we use are calculated\\nfrom the Bruzual & Charlot (2003) stellar population\\nsynthesis models with a Salpeter initial mass function.\\nThe ﬁrst we use is a ﬁxed-age 10 Gyr old simple stellar\\npopulation, and the second is for a simple stellar popu-\\nlation with an age given by an assumed formation red-\\nshift ofz = 3 . 0. The latter enables an estimate of the\\npassively-evolved BCG luminosity.\\n6. THE BCG LUMINOSITY - CLUSTER MASS\\nRELATION\\nAlthough it is still unclear how BCGs form, conven-\\ntional formation scenarios include galactic cannibalism,\\ncooling ﬂows, and mergers during cluster formation (§ 1).\\nThe evolution of the luminosity of the BCG with the\\nmass of the cluster may distinguish between these mod-\\nels and oﬀer insight into the formation of BCGs. Semi-\\nanalytic and numerical simulations of structure forma-\\ntion suggest a tight correlation between BCG luminosi-\\nties and cluster masses (e.g., Somerville & Primack 1999;\\nCole et al. 2000), and we can parameterize such a cor-\\nrelation betweenK-band BCG luminosities and cluster\\nmasses M200 by the power law\\nLK, BCG\\n1011 h−2\\n70 L⊙\\n= B\\n( M200\\n1014 h−1\\n70 M⊙\\n) β\\n. (6)\\nCl0939\\nCl0024\\nMS2137\\n3C220\\nCl2244\\nMS1137\\nA370\\nMS0451\\nFig. 4.— The correlation between K-band BCG luminosity and\\ncluster mass for our sample of strong lensing clusters. Unco rrected\\nluminosities (black points) are ﬁt by the solid line, while l umi-\\nnosities corrected for passive evolution (open circles) ar e ﬁt by the\\ndashed line. For comparison, the Lin & Mohr (2004) L − M re-\\nlation for the general cluster population is shown as the dot ted\\nline. Our best-ﬁt power laws are signiﬁcantly steeper than t hat\\nof Lin & Mohr (2004), hinting that BCGs in lensing clusters ma y\\nhave diﬀerent formation histories than BCGs in typical clus ters.\\nHere, we examine the relation between BCG luminos-\\nity and cluster mass for clues about the formation histo-\\nries of BCGs in strong lensing clusters and how their\\nformations may diﬀer from the general BCG popula-\\ntion. We represent the general BCG population with\\nthe Lin & Mohr (2004) study of 93 BCGs atz ≤ 0. 09 in\\nthe Two Micron All Sky Survey (2MASS).\\nFor an accurate comparison to the L − M relation\\nLin & Mohr (2004) ﬁnd from 2MASS, we follow their def-\\ninition of BCG luminosity. Lin & Mohr (2004) measure\\nBCG luminosities in theK-band using 20 mag arcsec −2\\nisophotal elliptical aperture magnitudes for 2MASS,\\ncalled K20 magnitudes. Similarly, we convert to theK-\\nband (see § 5.2) and measure BCG magnitudes using Formation Histories of Strong Lensing Clusters 9\\nSExtractor’s MAG AUTO function (Bertin & Arnouts\\n1996), which has good agreement with 2MASS K20 total\\nmagnitudes for sources such as BCGs that are bright and\\nextended (Elston et al. 2006). We then convert the mag-\\nnitudes intoK-band luminosities as described in § 5.2.\\nThe resultant K-band BCG luminosities, along with the\\nluminosities corrected for passive evolution, are given in\\nTable 4.\\nFigure 4 illustrates the correlation between BCG lu-\\nminosities and cluster masses M200. We ﬁnd the best-ﬁt\\npower law to the data is given by B = 0 . 97 ± 0. 17 and\\nβ = 0 . 48 ± 0. 09 for all strong lensing clusters (solid line\\nin Figure 4) and B = 0 . 93 ± 0. 18 and β = 0 . 39 ± 0. 10 for\\nall strong lensing clusters when the BCG luminosities are\\ncorrected for passive evolution (dashed line in Figure 4).\\nThe similarity of these two results implies that the pas-\\nsive evolution of BCG luminosities with redshift has little\\neﬀect on theL − M relation, and more generally there\\nis no evidence for evolution in the L − M relation from\\nz ∼ 1 to z ∼ 0 (Brough et al. 2008)\\nFor comparison, Lin & Mohr (2004) ﬁnd a best-ﬁt\\npower law of B = 4 . 9 ± 0. 2 and β = 0 . 26 ± 0. 04 (dot-\\nted line in Figure 4), which is consistent with the slopes\\nfound by analytic estimates and cosmological simulations\\nof the growth of central galaxies. Using the galaxy-dark\\nmatter correlation function to determine host dark mat-\\nter halo masses for observational catalogs of galaxies,\\nCooray & Milosavljevi´ c (2005) ﬁndL ∝ M<0. 3\\n200 for halo\\nmasses ≳ 4 × 1013 h−1 M⊙. Similarly, Vale & Ostriker\\n(2006) determine a correlation of L ∝ M0. 28\\n100 when they\\ncombine the subhalo mass distribution derived from sim-\\nulations with an empirical galaxy luminosity function.\\nThey also ﬁnd little dependence of theL − M relation\\non waveband.\\nFrom their slope of β = 0 . 26, Lin & Mohr (2004) con-\\nclude that while other cluster members may merge with\\nBCGs and increase BCG luminosities, such eﬀects are\\nnot suﬃcient to fully account for the growth inLK, BCG\\nwith cluster mass. Instead, Lin & Mohr (2004) suggest\\nthat BCGs must grow mainly through mergers with other\\nBCGs brought in when the host galaxy cluster merges\\nwith other groups or clusters. In addition to the many\\nhierarchical structure formation simulations and models\\nthat support this scenario (e.g., Merritt 1985; Dubinski\\n1998; Boylan-Kolchin et al. 2006), there are also observa-\\ntions of a pair of∼ L∗ elliptical galaxies merging to build\\nup the BCG in a rich cluster at z = 1 . 26 (Yamada et al.\\n2002).\\nOur slope β is 50% (when luminosities are corrected\\nfor passive evolution) to 85% (when luminosities are\\nnot corrected for passive evolution) steeper than that of\\nLin & Mohr (2004), hinting that strong lensing clusters\\nmay undergo more mergers with groups and clusters, or\\nmerge with more massive groups and clusters, than the\\naverage cluster. Both more mergers and mergers with\\nmore massive systems could account for the initial ev-\\nidence for an increase inLK, BCG with cluster mass we\\nﬁnd in strong lensing clusters, and would also be con-\\nsistent with simulations that suggest strong lensing clus-\\nters are dynamically more active than the general cluster\\npopulation (Bartelmann & Steinmetz 1996). However,\\nthe scatter in ourL − M relation is signiﬁcant, and a\\nlarger sample of strong lensing clusters is necessary to\\ndraw deﬁnitive conclusions about the formation histories\\nof strong lensing clusters.\\n7. CONCLUSIONS\\nWe have determined the scaling of cluster mass with\\ncluster temperature and the scaling of BCG luminosity\\nwith cluster mass for eight observed strong lensing galaxy\\nclusters imaged withHST and at redshifts 0 . 3 < z < 0. 8.\\nWe explored cluster concentrations as a means of reduc-\\ning the scatter in theM − T relation and enabling more\\nprecise constraints on cosmological parameters, and we\\nused theL − M relation as an indicator of the formation\\nhistories of strong lensing BCGs and clusters. Our main\\nresults are:\\n1. The best-ﬁt cluster mass-temperature relation for\\nour four dynamically relaxed strong lensing clusters\\nis\\nE(z)\\n( M2500\\n1014 h−1\\n70 M⊙\\n)\\n= 1 . 60 ± 3. 42\\n( kT\\n5 keV\\n) 1. 43±1. 28\\n,\\n(7)\\nwhich is consistent with the theoretical expectation\\nof theM −T relation for relaxed clusters, as well as\\nthe M − T relations determined by other observa-\\ntions and simulations. We ﬁnd that relaxed strong\\nlensing clusters do not deviate from theM −T rela-\\ntion for the general population of relaxed clusters.\\nSigniﬁcantly, we ﬁnd an inverse correlation between\\ncluster temperature and cluster concentration that,\\nwhen incorporated into theM −T relation, reduces\\nthe M − T scatter by a factor of 6, from 360% to\\n60%. By correcting cluster temperatures accord-\\ning to the temperature-concentration relation, we\\nﬁnd that theM − T relation given in Equation 7\\ndescribes not only the relaxed strong lensing clus-\\nters, but the entire cluster population regardless\\nof dynamical state. Speciﬁcally, the scatter in un-\\nrelaxed clusters decreases by a factor of 30, from\\n500% in the uncorrectedM − T relation to 15%\\nin the temperature-corrected M − T relation. In-\\ncorporating concentration eﬀects into the M − T\\nrelation tightens the M − T relation for all clus-\\nters, which in turn reduces the error in the deter-\\nmination ofσ8 from cluster counts. Whereas ac-\\ncurate cluster determinations of σ8 were previously\\nmade only with relaxed clusters, concentrations en-\\nable the inclusion of unrelaxed clusters. The larger\\ncluster samples possible with the inclusion of un-\\nrelaxed clusters oﬀer yet more preciseσ8 estimates\\nfrom cluster observations.\\n2. The best-ﬁt relation between BCG luminosity and\\ncluster mass for our sample of strong lensing clus-\\nters is\\nLK, BCG\\n1011 h−2\\n70 L⊙\\n= 0 . 97 ± 0. 17\\n( M200\\n1014 h−1\\n70 M⊙\\n) 0. 48±0. 09\\n,\\n(8)\\nwhich is ∼ 85% steeper than the correlations pre-\\ndicted for non-strong-lensing clusters by other ob-\\nservations, theory, and cosmological simulations.\\nThis result supports the current evidence that\\nBCGs are built up through mergers with massive 10 Comerford, Moustakas, & Natarajan\\ngalaxies in other groups and clusters, and also hints\\nthat strong lensing clusters may have more active\\nmerging histories than typical clusters. A larger\\nsample of strong lensing clusters might enable more\\ndeﬁnite conclusions about the formation histories\\nof strong lensing clusters.\\nAccurate cluster mass measurements and full use of\\nthe range of cluster property interdependencies are key\\ncomponents in the calibration of clusters as tracers of cos-\\nmological parameters. As we have shown, gravitational\\nlensing enables the most direct measurements of cluster\\nmass, without assumptions about the cluster’s dynamical\\nstate that are inherent in other methods. We have also\\nshown that the correlation between cluster temperature\\nand concentration can signiﬁcantly reduce the scatter in\\nthe cluster M − T relation, enabling more precise esti-\\nmates of σ8. It may be that other cluster scalings can\\nbe eﬀectively combined to reduce the error on additional\\ncosmological parameter estimates.\\nJ.M.C. acknowledges support of this work by a Na-\\ntional Science Foundation Graduate Research Fellow-\\nship. The work of L.A.M. was carried out at the Jet\\nPropulsion Laboratory, California Institute of Technol-\\nogy, with the support of NASA ATFP08-0169. P.N.\\nwould like to thank the Radcliﬀe Institute for Advanced\\nStudy and the Center for Astrophysics (CfA) for provid-\\ning an intellectually stimulating atmosphere that enabled\\nthis work.\\nREFERENCES\\nAllen, S. 1998, MNRAS, 296, 392\\nAllen, S. W., Schmidt, R. W., Ebeling, H., Fabian, A. C., & van\\nSpeybroeck, L. 2004, MNRAS, 353, 457\\nAllen, S. W., Schmidt, R. W., & Fabian, A. C. 2001, MNRAS, 328,\\nL37\\nAndersson, K., Peterson, J. R., Madejski, G., & Goobar, A. 20 09,\\nApJ, 696, 1029\\nArnaud, M., Pointecouteau, E., & Pratt, G. W. 2005, A&A, 441,\\n893\\nBahcall, N. A., & Comerford, J. M. 2002, ApJ, 565, L5\\nBailey, M. E. 1982, MNRAS, 201, 271\\nBartelmann, M., & Steinmetz, M. 1996, MNRAS, 283, 431\\nBautz, M., Loh, E., & Wilkinson, D. T. 1982, ApJ, 255, 57\\nBertin, E., & Arnouts, S. 1996, A&AS, 117, 393\\nBildfell, C., Hoekstra, H., Babul, A., & Mahdavi, A. 2008, MNRAS,\\n389, 1637\\nBorys, C., Chapman, S., Donahue, M., Fahlman, G., Halpern,\\nM., Kneib, J.-P., Newbury, P., Scott, D., & Smith, G. P. 2004,\\nMNRAS, 352, 759\\nBoylan-Kolchin, M., Ma, C.-P., & Quataert, E. 2006, MNRAS, 3 69,\\n1081\\nBroadhurst, T., Huang, X., Frye, B., & Ellis, R. S. 2000, ApJ, 534,\\nL15\\nBrough, S., Couch, W. J., Collins, C. A., Jarrett, T., Burke, D. J.,\\n& Mann, R. G. 2008, MNRAS, 385, L103\\nBruzual, G., & Charlot, S. 2003, MNRAS, 344, 1000\\nBryan, G. L., & Norman, M. L. 1998, ApJ, 495, 80\\nCardiel, N., Gorgas, J., & Aragon-Salamanca, A. 1998, MNRAS,\\n298, 977\\nChurazov, E., Br¨ uggen, M., Kaiser, C. R., B¨ ohringer, H., &\\nForman, W. 2001, ApJ, 554, 261\\nCole, S., Lacey, C. G., Baugh, C. M., & Frenk, C. S. 2000, MNRAS ,\\n319, 168\\nComerford, J. M., Meneghetti, M., Bartelmann, M., & Schirme r,\\nM. 2006, ApJ, 642, 39\\nComerford, J. M., & Natarajan, P. 2007, MNRAS, 379, 190\\nCooray, A., & Milosavljevi´ c, M. 2005, ApJ, 627, L85\\nCotter, G., Buttery, H. J., Das, R., Jones, M. E., Grainge, K.,\\nPooley, G. G., & Saunders, R. 2002, MNRAS, 334, 323\\nCowie, L. L., & Binney, J. 1977, ApJ, 215, 723\\nCrawford, C. S., Allen, S. W., Ebeling, H., Edge, A. C., & Fabian,\\nA. C. 1999, MNRAS, 306, 857\\nCzoske, O., Moore, B., Kneib, J.-P., & Soucail, G. 2002, A&A, 386,\\n31\\nDe Filippis, E., Schindler, S., & Castillo-Morales, A. 2003 , A&A,\\n404, 63\\nDonahue, M., Voit, G. M., Scharf, C. A., Gioia, I. M., Mullis, C. R.,\\nHughes, J. P., & Stocke, J. T. 1999, ApJ, 527, 525\\nDubinski, J. 1998, ApJ, 502, 141\\nEllingson, E., Yee, H. K. C., Abraham, R. G., Morris, S. L., &\\nCarlberg, R. G. 1998, ApJS, 116, 247\\nElston, R. J., Gonzalez, A. H., McKenzie, E., Brodwin, M., Br own,\\nM. J. I., Cardona, G., Dey, A., Dickinson, M., Eisenhardt, P. R.,\\nJannuzi, B. T., Lin, Y.-T., Mohr, J. J., Raines, S. N., Stanfo rd,\\nS. A., & Stern, D. 2006, ApJ, 639, 816\\nEvrard, A. E., MacFarland, T. J., Couchman, H. M. P., Colberg ,\\nJ. M., Yoshida, N., White, S. D. M., Jenkins, A., Frenk, C. S.,\\nPearce, F. R., Peacock, J. A., & Thomas, P. A. 2002, ApJ, 573,\\n7\\nEvrard, A. E., Metzler, C. A., & Navarro, J. F. 1996, ApJ, 469,\\n494\\nFinoguenov, A., Reiprich, T. H., & B¨ ohringer, H. 2001, A&A, 368,\\n749\\nGavazzi, R. 2005, in IAU Symposium, 179–184\\nHaiman, Z., Mohr, J. J., & Holder, G. P. 2001, ApJ, 553, 545\\nHausman, M. A., & Ostriker, J. P. 1978, ApJ, 224, 320\\nHenry, J. P. 2004, ApJ, 609, 603\\nHicks, A. K., & Mushotzky, R. 2005, ApJ, 635, L9\\nHoekstra, H. 2007, MNRAS, 379, 317\\nHorner, D. J. 2001, PhD thesis, University of Maryland College\\nPark\\nJeltema, T. E., Hallman, E. J., Burns, J. O., & Motl, P. M. 2008 ,\\nApJ, 681, 167\\nKay, S. T., da Silva, A. C., Aghanim, N., Blanchard, A., Liddl e,\\nA. R., Puget, J.-L., Sadat, R., & Thomas, P. A. 2005, Advances\\nin Space Research, 36, 694\\nKneib, J., Hudelot, P., Ellis, R. S., Treu, T., Smith, G. P., M arshall,\\nP., Czoske, O., Smail, I., & Natarajan, P. 2003, ApJ, 598, 804\\nKneib, J., Mellier, Y., Fort, B., & Mathez, G. 1993, A&A, 273, 367\\nKravtsov, A. V., Vikhlinin, A., & Nagai, D. 2006, ApJ, 650, 12 8\\nLevine, E. S., Schulz, A. E., & White, M. 2002, ApJ, 577, 569\\nLin, Y.-T., & Mohr, J. J. 2004, ApJ, 617, 879\\nMahdavi, A., Hoekstra, H., Babul, A., & Henry, J. P. 2008,\\nMNRAS, 384, 1567\\nMathiesen, B. F., & Evrard, A. E. 2001, ApJ, 546, 100\\nMaughan, B. J., Jones, C., Forman, W., & Van Speybroeck, L.\\n2008, ApJS, 174, 117\\nMazzotta, P., Rasia, E., Moscardini, L., & Tormen, G. 2004,\\nMNRAS, 354, 10\\nMcNamara, B. R., Raﬀerty, D. A., B ˆ ırzan, L., Steiner, J., Wi se,\\nM. W., Nulsen, P. E. J., Carilli, C. L., Ryan, R., & Sharma, M.\\n2006, ApJ, 648, 164\\nMellier, Y., Fort, B., Bonnet, H., & J.P., K. 1994, in ”Cosmol ogical\\nAspects of X-ray Clusters of Galaxies”, W.C. Seitter ed., NA TO\\nASI Series, 441, 219\\nMerritt, D. 1985, ApJ, 289, 18\\nMohr, J. J., Fabricant, D. G., & Geller, M. J. 1993, ApJ, 413, 492\\nMoran, S. M., Ellis, R. S., Treu, T., Smail, I., Dressler, A., Coil,\\nA. L., & Smith, G. P. 2005, ApJ, 634, 977\\nNatarajan, P., Kneib, J.-P., Smail, I., Treu, T., Ellis, R., Moran,\\nS., Limousin, M., & Czoske, O. 2009, ApJ, 693, 970\\nNavarro, J., Frenk, C., & White, S. 1996, ApJ, 462, 563\\n—. 1997, ApJ, 490, 493\\nNevalainen, J., Markevitch, M., & Forman, W. 2000, ApJ, 532,694\\nOstriker, J. P., & Tremaine, S. D. 1975, ApJ, 202, L113\\nOta, N., & Mitsuda, K. 2004, A&A, 428, 757\\nOta, N., Mitsuda, K., & Fukazawa, Y. 1998, ApJ, 495, 170\\nOta, N., Mitsuda, K., Hattori, M., & Mihara, T. 2000, ApJ, 530,\\n172\\nOta, N., Pointecouteau, E., Hattori, M., & Mitsuda, K. 2004, ApJ,\\n601, 120\\nPierpaoli, E., Borgani, S., Scott, D., & White, M. 2003, MNRA S,\\n342, 163\\nSchindler, S., Belloni, P., Ikebe, Y., Hattori, M., Wambsga nss, J.,\\n& Tanaka, Y. 1998, A&A, 338, 843\\nSchindler, S., & Wambsganss, J. 1996, A&A, 313, 113\\nSchmidt, R. W., & Allen, S. W. 2007, MNRAS, 379, 209\\nSchuecker, P., B¨ ohringer, H., Collins, C. A., & Guzzo, L. 2003,\\nA&A, 398, 867\\nSmith, G. P., Kneib, J., Smail, I., Mazzotta, P., Ebeling, H. , &\\nCzoske, O. 2005, MNRAS, 359, 417\\nSomerville, R. S., & Primack, J. R. 1999, MNRAS, 310, 1087\\nVale, A., & Ostriker, J. P. 2006, MNRAS, 371, 1173\\nVikhlinin, A., Kravtsov, A., Forman, W., Jones, C., Markevitch,\\nM., Murray, S. S., & Van Speybroeck, L. 2006, ApJ, 640, 691 Formation Histories of Strong Lensing Clusters 11\\nVikhlinin, A., Kravtsov, A. V., Burenin, R. A., Ebeling, H.,\\nForman, W. R., Hornstrup, A., Jones, C., Murray, S. S., Nagai ,\\nD., Quintana, H., & Voevodkin, A. 2009, ApJ, 692, 1060\\nVoit, G. M. 2005, Reviews of Modern Physics, 77, 207\\nWechsler, R., Bullock, J., Primack, J., Kravtsov, A., & Dekel, A.\\n2002, ApJ, 568, 52\\nWorrall, D. M., Birkinshaw, M., Hardcastle, M. J., & Lawrenc e,\\nC. R. 2001, MNRAS, 326, 1127\\nWu, X.-P. 2000, MNRAS, 316, 299\\nYamada, T., Koyama, Y., Nakata, F., Kajisawa, M., Tanaka, I.,\\nKodama, T., Okamura, S., & De Propris, R. 2002, ApJ, 577,\\nL89\\nZhang, Y., B¨ ohringer, H., Mellier, Y., Soucail, G., & Forma n, W.\\n2005, A&A, 429, 85\\nZhang, Y., Finoguenov, A., B¨ ohringer, H., Kneib, J., Smith , G. P.,\\nKneissl, R., Okabe, N., & Dahle, H. 2008, A&A, 482, 451\\nZhang, Y., Reiprich, T. H., Finoguenov, A., Hudson, D. S., &\\nSarazin, C. L. 2009, ApJ, 699, 1178', 'arXiv:astro-ph/0012536v1  29 Dec 2000\\nDraft version April 26, 2024\\nPreprint typeset using LATEX style emulateapj\\nOPTICAL AND X-RA Y CLUSTERS AS TRACERS OF THE SUPERCLUSTER-V OID NETWORK.\\nI SUPERCLUSTERS OF ABELL AND X-RA Y CLUSTERS\\nM. Einasto1, J. Einasto 1, E. T ago1 , V. M¨uller 2 & H. Andernach3\\nDraft version April 26, 2024\\nABSTRACT\\nW e study the distribution of X-ray selected clusters of gala xies with respect to superclusters determined\\nby Abell clusters of galaxies and show that the distribution of X-ray clusters follows the supercluster-void\\nnetwork determined by Abell clusters. W e ﬁnd that in this net work X-ray clusters are more strongly\\nclustered than other clusters: the fraction of X-ray cluste rs is higher in rich superclusters, and the fraction\\nof isolated X-ray clusters is lower than the fraction of isol ated Abell clusters. There is no clear correlation\\nbetween X-ray luminosity of clusters and their host supercl uster richness. Poor, non-Abell X-ray clusters\\nfollow the supercluster-void network as well: these cluste rs are embedded in superclusters determined by\\nrich clusters and populate ﬁlaments between them. W e presen t a new catalog of superclusters of Abell\\nclusters out to a redshift of zlim = 0 . 13, a catalog of X-ray clusters located in superclusters det ermined\\nby Abell clusters, and a list of additional superclusters of X-ray clusters.\\nSubject headings: cosmology: large-scale structure of the universe – cosmolo gy: observations – galaxies:\\nX-ray clusters – galaxies: clusters\\n1. INTRODUCTION\\nThe formation of a ﬁlamentary web of galaxies and sys-\\ntems of galaxies is predicted in any physically motivated\\nmodel of structure formation in the Universe (Bond, Kof-\\nman and Pogosyan 1996, Katz et al. 1996). The largest\\nrelatively isolated density enhancements in the Universe\\nare superclusters of galaxies. Observationally the presence\\nof superclusters and voids between them has been known\\nsince long ago (de V aucouleurs 1953, Abell 1958, Einasto,\\nJ˜ oeveer, & Saar 1980, Zeldovich, Einasto & Shandarin 1982,\\nOort 1983, Bahcall 1988). Superclusters of galaxies and\\nlarge voids between them form a supercluster-void network\\nof scale 100− 120 h−1 Mpc ( h is the Hubble constant in\\nunits of 100 km s −1 Mpc−1). The supercluster-void network\\nevolves from density perturbations of similar wavelength\\n(F risch et al. 1995). Superclusters correspond to the den-\\nsity maxima, and the largest voids to the density minima of\\nperturbations of this scale, in a density ﬁeld smoothed with\\na Gaussian window of dispersion∼ 8 h−1 Mpc(F risch et\\nal. 1995). The fact that superclusters are the largest phys-\\nically well-deﬁned systems in the Universe is equivalent to\\nthe fact that they correspond to the density perturbations\\nof the largest relative amplitude. On these large scales the\\nevolution of density perturbations is slow; thus superclus-\\nters and their ﬁne details grow from density perturbations\\nformed in the very early Universe. In this way the geometry\\nof the supercluster-void network, as well as its ﬁne struc-\\nture gives us information on the physical processes in the\\nearly Universe.\\nThe ﬁne structure of superclusters with their galaxy\\nand cluster chains and ﬁlaments, and voids in-between,\\nis presently quite well studied. The structure of the\\nsupercluster-void network itself is known with much less\\naccuracy . Recently Einasto et al. (1994, 1997a, 1997c and\\n1997d, hereafter EETDA, E97a, E97c and E97d, respec-\\ntively) demonstrated the presence of a preferred scale of\\n120h−1 Mpc in the distribution of rich clusters and super-\\nclusters of galaxies. Although several studies have found a\\nmaximum in the power spectra of galaxies and clusters of\\ngalaxies at the same scale (Einasto et al. 1999a and refer-\\nences therein), the shape of the power spectrum of clusters\\non very large scales is not clear yet (V ogeley 1998, Miller\\nand Batuski 2000). The reason for this is simple: on scales\\nlarger than∼ 100 h−1 Mpc the observational data are less\\ncomplete. On the other hand, diﬀerences between cosmo-\\nlogical models become signiﬁcant only on these larger scales,\\nthus a better understanding of the real situation is of great\\nimportance.\\nAn independent line of evidence for the structure of the\\nUniverse on large scales comes from the analysis of the CMB\\nangular spectrum (de Bernardis et al. 2000 and Hanany et\\nal. 2000). Fine structure of temperature ﬂuctuations on a\\ndegree scale has been detected; this scale corresponds to\\na linear scale about 100h−1 Mpc; thus large scale distri-\\nbution of matter can be studied using combined CMB and\\noptical data. These studies have caused increasing interest\\nin the studies of the clustering properties of matter on larg e\\nscales.\\nSo far superclusters have been determined using rich clus-\\nters of galaxies from the catalogs by Abell (1958) and Abell,\\nCorwin & Olowin (1989, hereafter ACO). Abell samples of\\nclusters of galaxies have been used mainly for the reason\\nthat they form presently the largest and deepest surveys of\\ngalaxy clusters available, containing more than 4000 clus-\\nters. However, Abell clusters were found by visual inspec-\\ntion of Palomar Observatory Sky Survey plates and the\\nsample may be inﬂuenced by various selection eﬀects. Se-\\nlection eﬀects change the number of galaxies observed in\\nclusters, and we can consider observed catalogs of clusters\\nas random selections from the underlying true cluster sam-\\n1T artu Observatory , EE-61602 T˜ oravere, Estonia\\n2Astrophysical Institute Potsdam, An der Sternwarte 16, D-1 4482 Potsdam, Germany\\n3Depto. de Astronom ´ ıa, Univ. Guanajuato, Apdo. Postal 144, Guanajuato, C.P . 36000, GTO, Mexico\\n1 2\\nple using certain probabilities which represent various se -\\nlection eﬀects. The inﬂuence of these selection eﬀects can\\nbe studied by comparison of samples of clusters of galaxies\\nselected independently . One of these optically selected in-\\ndependent cluster samples is the catalog of clusters derive d\\nfrom scans with the Automated Plate Measuring (APM)\\nF acility (Dalton et al. 1997). The other possibility is to\\nuse samples of clusters selected by their hot intracluster\\ngas. Hot gas accumulates in high-density regions; this gas\\nemits X-rays and can be detected by X-ray sensitive de-\\ntectors installed on satellites. Resulting samples of X-ray\\nselected clusters of galaxies form independent samples se-\\nlected from the same underlying true cluster sample using\\ndiﬀerent selection criteria. In recent years several catalogs\\nof X-ray clusters have been published based on ROSA T X-\\nray observations comprising data on several hundreds of\\nthese objects. These new catalogs have been used to inves-\\ntigate the clustering properties of X-ray clusters recently .\\nUsually these studies analyze the correlation function on\\nscales up to about 100h−1 Mpc (Romer et al. 1994, Abadi\\net al. 1999, Lee and Park 1999, Moscardini et al. 1999a,\\nCollins et al. 2000). The clustering of the X-ray clusters\\nup to the same scales has been predicted theoretically by\\nMoscardini et al. (1999, 2000).\\nAnother approach is to compile catalogs of superclusters\\nof galaxies and to study the distribution of clusters in supe r-\\nclusters. Supercluster catalogues have been used for many\\npurposes – to investigate the distribution of high-density\\nregions in the Universe, the large-scale motions in the Uni-\\nverse, the analysis of the Sunyaev-Zeldovich eﬀect (the scat-\\ntering of the cosmic microwave background radiation by hot\\ngas in clusters and superclusters of galaxies) in cosmic mi-\\ncrowave background maps. Examples of the last type of\\nanalyses are Birkinshaw (1998), Refregier, Spergel & Her-\\nbig (2000), Kashlinsky & Atrio-Barandela (2000). Diaferio,\\nSunyaev & Nusser (2000) propose that the presence of close\\nlarge CMB decrements may help to identify superclusters\\nat cosmological distances.\\nThe main goal of this series of papers is to compare the\\ndistribution of Abell, X-ray selected and APM clusters of\\ngalaxies and to check how well these cluster samples trace\\nthe properties of the underlying true cluster distribution\\nand the supercluster-void network. W e present an updated\\nversion of the supercluster catalog based on Abell clusters,\\nsupercluster catalogs of X-ray and APM clusters, and a list\\nof X-ray clusters in superclusters determined by Abell clus-\\nters. W e compare the distribution of Abell, X-ray and APM\\nclusters in diﬀerent environments. The aim of this analysis\\nis twofold: it gives us information about the clustering prop-\\nerties of Abell, X-ray and APM clusters; and independent\\nevidence about how well diﬀerent cluster samples trace the\\ndistribution of high-density regions of the Universe. In the\\nﬁrst paper of the series (this Paper) we compare clustering\\nproperties of Abell and X-ray selected clusters in superclus-\\nters. In paper II we shall analyze the correlation function\\nof X-ray clusters and provide evidence for a characteristic\\nscale of 120h−1 Mpc in the distribution of X-ray clusters\\n(T ago et al. 2001, Paper II). A similar comparison of Abell\\nclusters and clusters found from the Automatic Plate Mea-\\nsuring Machine (APM) catalog of galaxies will be made by\\nEinasto et al. (2001, Paper III).\\nThe paper is organized as follows. In Section 2 we shall\\ndescribe cluster samples used and present an updated ver-\\nsion of the catalog of superclusters of Abell clusters. In\\nSection 3 we compile a list of X-ray clusters in superclus-\\nters, analyze the distribution of Abell and non-Abell clus-\\nters, calculate the fraction of X-ray clusters in superclusters\\nof diﬀerent richness, and look for a relation between X-ray\\nluminosities of clusters with the richness of their parent su-\\nperclusters. In Section 4 we draw our conclusions. In the\\nAppendix we present an updated version of the supercluster\\ncatalog based on Abell clusters, and a list of X-ray clusters\\nin superclusters and in additional systems not present in\\nthe supercluster catalog. The catalog and both lists are also\\navailable electronically at web pages of T artu Observatory\\n(www.aai.ee). There we also demonstrate 3-D computer\\nmodels and animations of the distribution of superclusters\\nand X-ray clusters.\\n2. DA T A\\n2.1. Abell clusters\\nF or the present study we shall use the latest version\\n(March 1999) of the compilation of measured redshifts of\\nAbell clusters described by Andernach & T ago (1998). This\\ncompilation contains all known Abell clusters with mea-\\nsured redshifts, based on redshifts of individual cluster\\ngalaxies, and redshift estimates of the cluster according to\\nthe formula derived by Peacock & W est (1992), for both\\nAbell catalogs (Abell 1958 and ACO). W e omitted from the\\ncompilation all supplementary , or S-clusters, but included\\nclusters of richness class 0 from the main catalog. F rom\\nthis general sample we selected all clusters with measured\\nredshifts not exceedingzlim = 0 . 13; beyond this limit the\\nfraction of clusters with measured redshifts becomes small\\n(selection eﬀects in the Abell cluster sample up to redshift\\nzlim = 0 . 15 shall be studied in Paper III). If no measured\\nredshift was available we applied the same criterion for est i-\\nmated redshifts. Our sample contains 1662 clusters, 1071 of\\nwhich have measured redshifts. W e consider that a cluster\\nhas a measured redshift if at least one of its member galaxy\\nhas a measured redshift. In cases where the cluster has less\\nthan three galaxies with measured redshifts, and the mea-\\nsured and estimated redshifts diﬀer more than a factor of\\ntwo (|log(zmeas/z est)|> 0. 3), the estimated redshift was\\nused. In the case of superimposed clusters or component\\nclusters (A,B,C etc) with comparable number of measured\\nredshifts, we used only the cluster which better matches the\\nestimated redshift.\\nDistances to clusters have been calculated using the fol-\\nlowing formula (Mattig 1958):\\nr = c\\nH0q2\\n0\\nq0z + ( q0 − 1)(√1 + 2 q0z − 1)\\n1 + z ; (1)\\nwhere c is the velocity of light; H0 – the Hubble pa-\\nrameter; and q0 – the deceleration parameter. W e use\\nH0 = 100 h−1 km s −1 Mpc−1, and q0 = 0 . 5.\\n2.2. Superclusters of Abell clusters\\nOn the basis of the Abell cluster sample we constructed\\na list of superclusters of Abell clusters using a friends-of -\\nfriends (F oF) algorithm described in detail by EETDA and\\nE97c. Clusters are assigned to superclusters using a certain\\nneighborhood radius so that all clusters in the system have\\nat least one neighbor at a distance not exceeding this radius. 3\\nFig. 1.— Left panel: The multiplicity functions for Abell clusters. The solid line shows the fraction of isolated clusters as fun ction of the\\nneighborhood radius R; the short-dashed line shows the fraction of clusters in med ium-rich systems with a number of members from 2 to 31.\\nThe dashed line shows the fraction of clusters in very rich sy stems with at least 32 member clusters. Right panel: Supercl uster multiplicities\\nfor a neighborhood radius R = 24 h−1 Mpc. Isolated clusters are included for comparison.\\nThe neighborhood radius to assign clusters to supercluster s\\nshould be chosen in accordance with the spatial density of\\nthe cluster sample. Also, we deﬁne the multiplicity of a\\nsupercluster (supercluster richness), NCL , as the number of\\nits member clusters. Superclusters are divided into richne ss\\nclasses as in E97c: poor superclusters (number of members\\nNCL = 2 , 3), rich superclusters (4 ≤ NCL ≤ 7), and very\\nrich superclusters ( NCL ≥ 8).\\nIn Figure 1 (left panel) we show the fraction of clusters in\\nsystems of diﬀerent multiplicity for a wide range of neigh-\\nborhood radii for the Abell cluster sample. At small radii all\\nclusters are isolated. With increasing neighborhood radiu s\\nsome clusters form superclusters of intermediate richness .\\nIn Figure 1 we plot the fraction of clusters in superclus-\\nters of richness 2≤ NCL ≤ 31. At larger radii extremely\\nlarge superclusters with multiplicity NCL ≥ 32 start to\\nform. By further increasing the neighborhood radius su-\\nperclusters begin to merge into huge conglomerates; ﬁnally\\nall clusters percolate and form a single system penetrating\\nthe whole space under study . In order to obtain superclus-\\nters as the largest still relatively isolated systems we must\\nchoose a neighborhood radius smaller than the percolation\\nradius. The appropriate neighborhood radius is the radius\\nwhich corresponds to the maximum of the fraction of clus-\\nters in systems of intermediate richness. Beyond this radius\\nvery large systems start to form, as seen from Figure 1 (see\\nalso EETDA and E97c). F or Abell clusters the appropriate\\nneighborhood radius to select systems is 24h−1 Mpc. W e\\nshall apply the same radius to the samples of X-ray clusters\\nin order to determine which non-Abell X-ray clusters are\\nthe members of superclusters of Abell clusters, as well as to\\ndetect additional superclusters of non-Abell X-ray clusters.\\nF or the present study we update the supercluster cata-\\nlog and determine systems up to redshifts z = 0 . 13. This\\nlarger redshift limit was used in order to include several\\ndistant rich superclusters whose members have measured\\nredshifts and which also contain X-ray clusters, e.g. the\\nDraco-Ursa Majoris supercluster with 14 member clusters.\\nThe new Abell supercluster catalog contains 285 superclus-\\nters with at least 2 member clusters, 31 of them are very\\nrich superclusters with at least 8 members. The catalog\\nof superclusters of Abell clusters is given in the Appendix\\n(T able A1). In Figure 1 (right panel) we plot supercluster\\nmultiplicities for this catalog. In the present study this su-\\npercluster catalog was used as a reference to look for X-ray\\nclusters in superclusters.\\n2.3. X-ray selected cluster samples\\nThe ROSA T observations were made with the Position\\nSensitive Proportional Counter during the ROSA T All-sky\\nSurvey (RASS) in 1990 and 1991 (T r¨ umper 1993). After\\nthat the so-called Guest Observers (GO) four-year observ-\\ning program was completed.\\nOn the basis of RASS data several catalogs of X-ray se-\\nlected clusters of galaxies were prepared. In the present\\npaper we shall use the following samples of X-ray clusters:\\ni) clusters from the all-sky ROSA T Bright Survey of high\\nGalactic latitude RASS sources. A detailed description of\\nthe data is given in V oges et al. 1999, and the catalog of X-\\nray clusters, AGNs, galaxies, small groups of galaxies and\\nother objects in Schwope et al. 2000. W e shall refer to this\\nsample as RBS.\\nii) ROSA T PSPC observations of the richest ( R ≥ 2)\\nACO clusters (David, F orman and Jones 1999, hereafter\\nDFJ);\\niii) a ﬂux-limited sample of bright clusters from the\\nSouthern sky (de Grandi et al. 1999, see also Guzzo et\\nal. 1999);\\niv) the ROSA T brightest cluster sample (Ebeling et\\nal. 1998, BCS) from the Northern sky .\\nRedshifts are available for all the clusters.\\nThe ROSA T Bright Survey is the only available all- 4\\nsky survey of X-ray clusters. Objects in this survey\\nhave been selected at Galactic latitudes,|b|> 30◦, with\\nPSPC count rate larger than 0.2 s −1 and ﬂux limit 2 . 4 ×\\n10−12 erg cm −2 s−1 in the hard energy band (0 . 5−2. 0 keV).\\nF or our analysis we selected clusters with measured red-\\nshifts up toz = 0 . 13 – the redshift limit of the catalog of\\nsuperclusters of Abell clusters (see above). Altogether, t his\\nsample comprises 203 clusters, including 40 non-Abell clus -\\nters. W e shall refer to this cluster sample as the “RBSC”\\nsample; for cluster numbers we use RBS numbers as given\\nin Schwope et al. (2000).\\nF urther, we use the list of the richest ( R ≥ 2) Abell clus-\\nters detected with ROSA T PSPC observations (DFJ). This\\ncatalog contains data on the clusters of galaxies observed\\nduring the GO phase of the ROSA T mission. The main ad-\\nvantage of these observations is longer exposure time (typi-\\ncally 10 000 seconds) than in the RASS (400 seconds). How-\\never, the sky coverage of this compilation is far less than\\nthat of RBSC catalog since the latter clusters were found\\nin targeted and serendipitous observations. F or the method\\nto calculate X-ray ﬂuxes we refer to DFJ. Up to distances\\nz= 0 . 13 this sample contains 52 clusters. W e shall denote\\nthis sample as DFJ.\\nThe Brightest Cluster Sample (BCS, Ebeling et al. 1998)\\ncovers the Northern sky ( δ > 0◦) at Galactic latitudes\\n|b|> 20◦ in the broad energy band (0 . 1−2. 4 keV). The lower\\nﬂux limit for sample was 4.4 10 −12ergs cm −2s−1. Ebeling\\net al. developed the VTP (V oronoi T essellation and Per-\\ncolation) algorithm to determine X-ray ﬂuxes of extended\\nsources of arbitrary shapes. Up toz = 0 . 13 this sample\\ncontains 141 clusters, including 46 non-Abell clusters. W e\\nshall denote this sample as BCS.\\nThe ﬂux-limited sample of bright clusters of galaxies from\\nthe Southern sky by de Grandi et al. (1999) is selected\\nat galactic latitudes|b|> 20◦, the declination δ < 2. 5◦,\\nand the ﬂux limit in the hard band (0 . 5 − 2. 0 keV) was\\n3 − 4 × 10−12 erg cm −2 s−1. In their study the so-called\\nSteepness Ratio T echnique was used to determine X-ray\\nﬂuxes. Up toz = 0 . 13 this sample contains 101 clusters, 34\\nof which are non-Abell clusters.\\nW e shall discuss the completeness and selection eﬀects of\\nAbell and X-ray clusters in Paper II. In general, at distance s\\nlarger than approximately 250 h−1 Mpc the samples of X-\\nray clusters are rather diluted due to the ﬁxed ﬂux limit;\\non larger distances X-ray clusters have been used in the\\npresent paper for lists of supercluster members only (and\\nnot for correlation analysis in Paper II).\\n3. X-RA Y CLUSTERS IN SUPERCLUSTERS\\nIn this Section we compile a list of X-ray clusters that\\nbelong to the superclusters derived from Abell clusters as\\nlisted in T able A1. In addition, we searched for systems\\nconsisting of non-Abell X-ray clusters and determine their\\nlocation with respect to the supercluster-void network. W e\\nalso calculate the fraction of X-ray clusters in superclusters\\nof various richness and investigate the possible correlati on\\nbetween cluster X-ray luminosities and supercluster rich-\\nnesses.\\n3.1. A list of X-ray clusters in superclusters\\nIn T able B1 we present a list of X-ray clusters in su-\\nperclusters of Abell clusters presented in T able A1. Abell\\nclusters from X-ray catalogs were included by comparison of\\nthe catalogs of X-ray clusters with the supercluster catalo g.\\nIn order to include non-Abell X-ray clusters we searched\\nfor superclusters that contain X-ray clusters in two ways.\\nFirst, we added non-Abell X-ray clusters to our Abell clus-\\nter catalog and applied the F oF algorithm to this combined\\ncatalog. Second, we applied the F oF algorithm to each cat-\\nalog of X-ray clusters separately . In both cases we used\\nthe same neighborhood radius,R = 24 h−1 Mpc as in the\\ncase of Abell clusters. The second procedure was used to\\ncheck whether X-ray clusters that are supercluster members\\nform systems by themselves also. Additionally , for some\\nsuperclusters this second procedure detects outlying Abell\\nclusters as members of superclusters that are not listed in\\nT able A1 (mainly due to small diﬀerences in redshift mea-\\nsurements). In the case of X-ray clusters identiﬁed as Abell\\nclusters this double procedure gives us additional evidence\\nabout the reliability of the superclusters found by optical\\nsurveys.\\nNon-Abell clusters that were found to be members of su-\\nperclusters of Abell clusters (T able A1) were considered as\\nmembers of these systems. However, their membership has\\nto be checked carefully . The superclusters of Abell clusters\\nwere deﬁned as the largest still relatively isolated system s.\\nIn some cases non-Abell clusters (poor clusters of galaxies )\\nreally belong to the superclusters, but in other cases non-\\nAbell clusters actually form a bridge of poor clusters that\\nconnect superclusters of Abell clusters. Therefore, the ac-\\ntual location of each non-Abell cluster that was connected\\nto some supercluster according to the F oF algorithm was\\nchecked separately . W e shall mention below the cases when\\nclusters formed ﬁlaments connecting superclusters, rather\\nthan forming new members of a single supercluster.\\nW e note that in most cases when a supercluster con-\\ntains more than one X-ray cluster, these X-ray clusters\\nthemselves form a supercluster at the neighborhood radius\\nR= 24 h−1 Mpc. Therefore T able B1 lists superclusters\\nof X-ray clusters as well. Only in a few cases of very elon-\\ngated superclusters it happened that some X-ray members\\nof the system remained as separate systems so that the su-\\npercluster was split into smaller systems. The supercluster\\nnumber in the column 1 of T able B1 correspond to super-\\ncluster numbers from the catalog in T able A1.\\nThe use of combined (X-ray and optical) data to deter-\\nmine X-ray clusters in superclusters was very fruitful. In\\nour catalog of superclusters containing X-ray clusters (T a-\\nble B1) there are 99 superclusters. Of these superclusters\\n53 contain only one member as an X-ray cluster. These X-\\nray clusters would be isolated if we would use data on X-ray\\nclusters only; actually they are members of superclusters.\\nSuch an approach could be useful in the analysis of systems\\nof X-ray selected AGNs, as mentioned also in T esch and\\nEngels (2000).\\nIn T able B2 we list additional superclusters that contain\\nnon-Abell clusters. In most cases these systems are pairs\\nof Abell and non-Abell X-ray clusters. Most Abell clusters\\nin these superclusters were isolated if only Abell clusters\\nwere used in supercluster search. W e shall denote these\\nsuperclusters asSCLX + supercluster number from T able\\nB2.\\n3.2. Comments on individual superclusters\\nThe Hercules supercluster (SCL 160) at a distance of\\nabout 100 h−1 Mpc contains the largest number of X-ray 5\\nFig. 2.— The distribution of X-ray clusters (ﬁlled symbols, supercl uster members) and Abell clusters (open circles) in superga lactic coor-\\ndinates. In order to avoid overcrowding of the ﬁgure we plot o nly clusters from very rich superclusters) in supergalacti c coordinates. In each\\npanel we plot Abell clusters and X-ray clusters from one samp le. X-ray samples are plotted as follows. Upper left panel: RBS sample. Here\\nwe plot also members of additional systems (squares, T able B 2), and isolated non-Abell clusters (triangles); upper rig ht panel: DF J sample;\\nlower left panel: BCS sample, and lower right panel: sample by de Grandi et al. (199 9). The extent of all panels in supergalactic X coordinate\\nis 600 h−1 Mpc\\n−300 −200 −100 0 100 200 300\\nY [h\\n−1\\n Mpc] \\n−300\\n−200\\n−100\\n0\\n100\\n200\\n300\\nZ [h\\n−1\\n Mpc]\\n−300 −200 −100 0 100 200 300\\nY [h\\n−1\\n Mpc] \\n−300\\n−200\\n−100\\n0\\n100\\n200\\n300\\nZ [h\\n−1\\n Mpc]\\n−300 −200 −100 0 100 200 300\\nY [h\\n−1\\n Mpc] \\n−300\\n−200\\n−100\\n0\\n100\\n200\\n300\\nZ [h\\n−1\\n Mpc]\\n−300 −200 −100 0 100 200 300\\nY [h\\n−1\\n Mpc] \\n−300\\n−200\\n−100\\n0\\n100\\n200\\n300\\nZ [h\\n−1\\n Mpc]\\nclusters – 14, including 7 non-Abell clusters. All of them\\nare probably true supercluster members.\\nThe Shapley supercluster (SCL 124) at a distance of\\nabout 130 h−1 Mpc contains 9 X-ray clusters, only one of\\nthem is a non-Abell cluster. In this supercluster X-ray emis -\\nsion has been detected also from ﬁlaments of galaxies con-\\nnecting individual clusters (Bardelli et al. 1998 and refer-\\nences therein, Kull and B¨ ohringer 1999, Ettori et al. 1997) .\\nThe Horologium-Reticulum supercluster (SCL 48) , one of\\nthe richest superclusters in the Southern sky , is also very\\nrich in X-ray clusters, containing 11 X-ray clusters; only one\\nof them is a non-Abell cluster. W e note that the number of\\noptically very rich X-ray clusters from the compilation by\\nDFJ is the largest in the last two superclusters, in the Shap-\\nley and in the Horologium-Reticulum superclusters, both\\ncontaining six X-ray clusters.\\nThe supercluster SCL 170 is very interesting. Accord-\\ning to the data used in our study this supercluster contains\\nonly one X-ray cluster – A2312. Actually this supercluster\\nis one of the richest in X-ray clusters – it is the North Eclip-\\ntic Pole (NEP) supercluster (Mullis 1999, Mullis et al. 2000)\\nthat contains approximately 15 X-ray clusters. In the NEP\\nsurvey the X-ray ﬂux limit was lower than in the catalogs\\nused in our study and thus contains fainter X-ray clusters\\nthan those catalogs. This example shows that our list of\\nX-ray clusters in superclusters compiled on the basis of the\\nX-ray brightest cluster catalogs is preliminary , containing\\nthe X-ray brightest supercluster members only . 6\\nTable 1\\nF raction of X-ray clusters in superclusters of diﬀerent ric hness\\nSupercluster richness NA F N X−ray\\nNA FA NnA FnA\\nscl members 1256 182 68\\npoor (2≤ Ncl ≤ 3) 513 41% 47 26% 18 26%\\nrich (4 ≤ Ncl ≤ 7) 370 29% 59 32% 20 30%\\nvery rich ( Ncl > 8) 373 30% 76 42% 30 44%\\nThe Pisces supercluster contains 10 X-ray clusters, 4 of\\nwhich are non-Abell clusters. However, our analysis shows\\nthat actually these poor clusters belong to a ﬁlament that\\nconnects the Pisces supercluster and superclusters 211 and\\n215.\\nPoor X-ray clusters connect the Coma and the Leo super-\\nclusters (SCL 117 and 93), the Sculptor supercluster (SCL\\n9) and SCL 220 (see also Paper II), SCL 126 and 136, and\\nSCL 212 and 297. These cases conﬁrm that poor X-ray\\nclusters trace the supercluster-void network determined by\\nAbell clusters. X-ray clusters either belong to superclust ers\\nthemselves or they form ﬁlaments between them.\\nAdditional superclusters of X-ray clusters from T able B2,\\nbeing located in ﬁlaments between superclusters, also trac e\\nthe supercluster-void network. Several of these systems\\n(SCLX 7, 9, and 12) border the Southern and Northern\\nLocal Supervoids (EETDA). SCLX 9 contains one of the\\nX-ray brightest Abell clusters, A496, see above, and in ad-\\ndition to poor clusters this system harbors two X-ray de-\\ntected AGNs, RBS 550 and RBS 556. X-ray detected AGNs\\nfrom the RBS catalog connectSCLX 4 and 7 from T a-\\nble B2. This joint system contains 11 AGNs and 7 X-ray\\nselected clusters, including 3 Abell clusters and one QSO\\n(QSO 0351+026).\\nIn EETDA we showed that isolated Abell clusters are lo-\\ncated close to the superclusters and do not ﬁll in the voids\\nbetween superclusters. Our present analysis shows addi-\\ntionally that most of the isolated poor X-ray clusters that\\ndo not have neighbors atR ≥ 24 h−1 Mpc are located in ﬁl-\\naments between superclusters or on the borders of Southern\\nand Northern Local voids.\\nIn Figure 2 we plot the distribution of X-ray clusters\\nand Abell clusters that belong to very rich superclusters.\\nW e see that the structures delineated by optical and X-ray\\nclusters coincide and we can see a pattern of superclusters\\nand voids. The supercluster-void network is more clearly\\nseen in three-dimensional animations from our web page,\\nwww.aai.ee.\\nIn this Figure we plot also clusters from additional su-\\nperclusters (T able B2), as well as the location of isolated\\nnon-Abell clusters. Many of them are located near the zone\\nof avoidance where cluster catalogs tend to be incomplete\\nand superclusters cannot be determined.\\n3.3. Fraction of X-ray clusters in superclusters\\nAfter compiling the list of X-ray clusters in superclusters\\nwe calculate the fractions of these clusters in supercluste rs\\nof various richness (T able 1). Superclusters are divided in to\\nrichness classes as in E97c: poor superclusters (number of\\nmembersNCL = 2 , 3), rich superclusters (4 ≤ NCL ≤ 7),\\nand very rich superclusters ( NCL ≥ 8). Additionally , we\\ngive the fraction of isolated X-ray clusters.\\nT able 1 shows that the fraction of X-ray clusters in su-\\nperclusters increases with increasing supercluster richn ess.\\nThe Kolmogorov-Smirnov test conﬁrms that the zero hy-\\npothesis (the distributions of optical and X-ray clusters in\\nsuperclusters of various richness are statistically ident ical)\\nis rejected at the 99% conﬁdence level. In total, about one\\nthird of all superclusters and 23 of 29 very rich superclus-\\nters contain X-ray clusters. About 25% of Abell clusters\\nare isolated at the neighborhood radiusR = 24 h−1 Mpc.\\nIn contrast, only about 15% of X-ray clusters are isolated\\nat this radius.\\nW e note that various surveys used in the present study\\nshow a similar tendency – the increase of the fraction of X-\\nray clusters with supercluster richness. However, the exact\\npercentages of X-ray clusters in systems of various rich-\\nness are somewhat diﬀerent due to the diﬀerences between\\nsamples. F or example, due to the sky coverage limits the\\nfraction of isolated clusters is relatively high in the BCS\\nsample (25% of poor clusters in this sample are isolated,\\nsee also Paper II). Also, due to the incompleteness of X-ray\\ncluster catalogs at large distances these fractions should\\nactually be taken as lower limits: at distances larger than\\nR= 275 h−1 Mpc there are only ﬁve supercluster with more\\nthan one X-ray member cluster, and over 20 superclusters\\ncontaining one X-ray cluster only . However, test calcula-\\ntions with smaller, statistically more complete subsample\\nfrom RBSC catalog in which clusters were selected up to\\nthe distanceR = 250 h−1 Mpc (Paper II) conﬁrm that the\\nfraction of X-ray clusters in rich superclusters is higher t han\\nin poor superclusters.\\n3.4. X-ray luminosities of clusters in superclusters of\\ndiﬀerent richness\\nIn Figure 3 we plot X-ray luminosities for clusters in su-\\nperclusters of diﬀerent richness in units of 10 43 erg s −1.\\nX-ray luminosities are calculated diﬀerently in the variou s\\nX-ray cluster catalogs. In some catalogs the broad en-\\nergy band (0.1 -2.4 keV) is used (e.g. the BCS sample),\\nwhile others are based on the hard energy band (0.5 - 2.0 7\\nFig. 3.— X-ray luminosities for clusters in superclusters of diﬀere nt richness and for isolated clusters (in units of 10 43erg s −1); clusters of\\nthe highest X-ray luminosities are indicated below in paren thesis. X-ray samples are plotted as follows: upper left pan el: RBS sample (A2142,\\nA2029, A401); upper right panel: DFJ sample (A2142, A2029, A 478); lower left panel: BCS sample (A2142, A2029, A478); low er right panel:\\nsample by de Grandi et al. . (A3266, A3186, A3827).\\nkeV). Also, diﬀerent methods are used to determine the\\ntotal X-ray ﬂux of extended sources. As a result, the X-\\nray luminosities for various cluster samples are not directly\\ncomparable, particularly in the case of clusters with com-\\nplicated morphology . However, our aim is to see whether\\ncluster X-ray luminosities are correlated with host super-\\ncluster richness, and for that purpose we may simply plot\\nX-ray luminosities for each sample separately .\\nFigure 3 shows that some clusters of very high X-ray\\nluminosity are located in superclusters of low multiplicit y .\\nSince Figure 3 does not show any other clear correlation be-\\ntween cluster X-ray luminosities and their host supercluster\\nrichness we think that it is preliminary to draw quantitativ e\\nconclusions from this ﬁnding. Instead, we describe shortly\\nthe locations and properties of the brightest X-ray clusters.\\nThe cluster with the highest X-ray luminosity in the\\nNorthern sky is A2142. This cluster is isolated and lo-\\ncated in the low-density ﬁlament of clusters connecting the\\nCorona Borealis and the Bootes A superclusters (SCL 158\\nand 150). Evidence was found for an ongoing merging of\\ntwo subclusters in this cooling ﬂow cluster (Markevitch et\\nal. 2000 and references therein, and White, Jones and F or-\\nman 1997).\\nThe second brightest X-ray cluster in the Northern sky ,\\nA2029, borders the Bootes void and is located in a su-\\npercluster with four members, SCL 154, in the ﬁlament\\nbetween the Hercules and the Corona Borealis superclus-\\nters (SCL 160 and 158). Markevitch et al. (1998, hereafter\\nMFSV) describe this cluster as one of the most regular, well\\nrelaxed X-ray cluster with a very strong cooling ﬂow.\\nThe third brightest X-ray cluster in the RBSC catalog is\\nA401 which forms a cluster pair with A399 (SCL 45). Both 8\\nof these clusters contain a cD galaxy . MFSV suggest that\\nthese clusters may be in the early stages of a collision.\\nAnother isolated cluster of high X-ray luminosity , A478,\\nshows evidence for a strong cooling ﬂow (MFSV and White,\\nJones and F orman 1997). In clusters A478 and A2142\\nthe Sunyaev-Zeldovich eﬀect has been measured (Myers et\\nal. 1998).\\nOne of the clusters of the highest X-ray luminosity in the\\nDFJ sample is A426, a cooling ﬂow cluster (White, Jones\\nand F orman 1997) in the Perseus supercluster (SCL 40).\\nThe brightest X-ray cluster in the sample by de Grandi\\net al. (1999), A3266, is located in the outer region of the\\nHorologium-Reticulum supercluster (SCL 48), i.e. also in a\\nrelatively low-density environment. MFSV and Henriksen\\net al. (2000) show the possibility of a merger event in this\\ncluster.\\nThe second brightest X-ray cluster in the sample by de\\nGrandi et al. (1999), A3186, is one of the most distant clus-\\nters in our sample lying at a distance of about 350h−1 Mpc\\nin an area of a low-density ﬁlament that surrounds distant\\nvoids in the Southern sky . This cD cluster shows evidence\\nof a substructure and a small cooling ﬂow (Nesci and Norci\\n1997). A3186 is of richness classR = 1, while all other\\nclusters of the highest X-ray luminosity mentioned here are\\nof richness classR = 2.\\nThe third brightest cluster in de Grandi’s sam-\\nple is A3827, an outlying member of the poor su-\\npercluster SCL 200. X-ray emission of this clus-\\nter is probably dominated by its central galaxy that\\nshows signs of merging of other galaxies in the clus-\\nter (Astronomy Picture of the Day , August 31, 1998,\\nhttp://antwrp.gsfc.nasa.gov/apod/astropix.html).\\n4. DISCUSSION AND CONCLUSIONS\\nW e have studied the distribution of X-ray clusters with\\nrespect to the supercluster-void network determined by\\nAbell clusters, compiled a list of X-ray clusters in super-\\nclusters and showed that both X-ray and optical clusters\\ndelineate large-scale structure in a similar way . X-ray clus-\\nters that do not belong to superclusters determined by Abell\\nclusters border the Southern and Northern Local supervoid\\nor are located in ﬁlaments between superclusters. X-ray\\nclusters are more strongly clustered than optically selected\\nclusters: the fraction of X-ray clusters is higher in rich an d\\nvery rich superclusters, and the fraction of isolated X-ray\\nclusters is lower than these fractions for optically selected\\nclusters. These results indicate that the structure of the\\nUniverse is traced in a similar way by both optical and X-\\nray clusters up to redshifts ofz = 0 . 13. A similar conclusion\\nhas been obtained by Borgani & Guzzo (2001) based on the\\ncomparison of the REFLEX cluster surveys with the Las\\nCampanas galaxy redshift survey (Shectman et al. 1996).\\nThe rather regular placement of superclusters is notice-\\nable in the case of both X-ray clusters and Abell clusters, es -\\npecially in the Northern sky . W e shall discuss the presence\\nof the regularity in the distribution of X-ray clusters in more\\ndetail in Paper II. In particular, we shall present evidence\\nfor a presence of a characteristic scale of 120h−1 Mpc in\\nthe distribution of X-ray clusters.\\nEETDA demonstrated that the fraction of X-ray clus-\\nters in superclusters increases with supercluster richnes s\\n(T able 4 in EETDA). This result was based on the early\\ncatalogs of X-ray clusters containing altogether 59 X-ray\\nclusters in superclusters. Our present study conﬁrms and\\neven strengthens this early result. The data in T able 1 show\\nthat the fraction of X-ray clusters in the Abell cluster-based\\nsuperclusters increases with supercluster richness. In se v-\\neral superclusters most members are X-ray sources. The\\npresence of X-ray emitting gas in a large fraction of clus-\\nters shows that potential wells in clusters and superclusters\\nof galaxies are rather deep.\\nW e did not detect a correlation between the X-ray lu-\\nminosity of clusters and their host supercluster richness,\\nalthough clusters with the highest X-ray luminosities are\\nlocated in relatively poor superclusters.\\nLoken et al. (1999) showed that massive cooling ﬂow clus-\\nters are located in high density regions. W e ﬁnd that from\\n26 clusters analyzed in their study 24 belong to superclus-\\nters, and 12 of them to very rich superclusters. Six clusters\\nare members of the Hercules supercluster.\\nEngels et al. (1999) found indications that X-ray selected\\nAGNs may be a part of the supercluster-void network de-\\nscribed previously by Einasto and co-workers (see references\\nin the Introduction). Our results conﬁrm this. A number\\nof AGNs from the RBS catalog are located in superclusters\\nof Abell clusters. Several structures seen in the distribution\\nof X-ray selected AGN are also seen in our sample (in the\\ndirection of the Pisces, the Ursa Majoris and the Coma su-\\nperclusters), although, in general, Engels et al. study more\\ndistant objects beyond the borders of our sample.\\nBoughn (1999) demonstrated the presence of X-ray emis-\\nsion from the Local supercluster as a possible evidence of\\nhot diﬀuse gas in superclusters. Scharf et al. (2000) found\\nan evidence for X-ray emission from a distant large scale ﬁl-\\nament of galaxies. In the Shapley supercluster X-ray emis-\\nsion has been detected in the ﬁlaments between supercluster\\nmember clusters (Bardelli et al. 1999, Kull and B¨ ohringer\\n1999). This indicates that the whole central part of the\\nsupercluster is a physical entity forming a deep potential\\nwell.\\nThese ﬁndings give additional evidence that superclus-\\nters are not random associations of clusters but form real\\nphysical systems – large-scale high-density regions of the\\nmatter distribution forming extended potential wells in the\\ndistribution of matter. Both optical and X-ray clusters are\\nparts of the same supercluster-void network that we see in\\nthe distribution of Abell clusters of galaxies. Our results\\nsuggest that optically and X-ray selected cluster samples\\ncan be used to ﬁnd large-scale high-density regions in the\\nUniverse. Samples detected optically and in X-rays are dif-\\nferent in many details, but are common in one important\\naspect – both indicate the skeleton of the supercluster-void\\nnetwork in a rather similar way .\\nMain results of our study of the clustering properties of\\nX-ray clusters are:\\n1) W e present an updated catalog of superclusters of\\nAbell clusters and a list of X-ray clusters in superclusters .\\n2) Optical and X-ray clusters trace the supercluster-void\\nnetwork in a similar way .\\n3) The fraction of X-ray clusters in superclusters increase s\\nwith the supercluster richness suggesting that superclust ers\\nare real physical systems.\\n4) Cluster X-ray luminosity is not correlated with their\\nhost supercluster richness, although the most luminous X-\\nray clusters are located in relatively low density environ-\\nments. 9\\nAPPENDIX A: A CA T ALOG OF SUPERCLUSTERS OF ABELL\\nCLUSTERS\\nHere we present a new supercluster catalog based on the\\nAbell cluster sample (A1) used in this paper.\\nThe catalog of superclusters of Abell clusters is based on\\na cluster sample which contains all superclusters of richne ss\\nclass NCL ≥ 2. T able A1 contains the following entries: N o\\nis the identiﬁcation number. The supercluster should be re-\\nferred to as “SCL nnn” with nnn being the running number\\nN o. As mentioned in the text, an index ”c” in the ﬁrst col-\\numn indicates a supercluster candidate, i.e. a supercluste r\\nthat is not present in the test catalog determined by cluster s\\nof measured redshifts only .\\nNCL is the number of member clusters in the superclus-\\nter; α C and δ C are coordinates of the center of the super-\\ncluster (equinox 1950.0), derived from coordinates of indi -\\nvidual clusters; DC is the distance of the center from us; it\\nfollows the list of Abell clusters which are members of the\\nsupercluster. An index ”e” after the Abell cluster number in\\nthe column 6 shows that this cluster has only an estimated\\ndistance. In the last column we list a commonly used name\\nof the supercluster, which in most cases is based on constel-\\nlation names. T o avoid confusion, we use the same numbers\\nas in our previous version of the catalog (E97d); and add\\nnew numbers (221 and above) for superclusters described\\nin this catalog for the ﬁrst time. Superclusters are sorted\\nbyα C .\\nAPPENDIX B: X-RA Y CLUSTERS IN SUPERCLUSTERS\\nIn T able B1 we present data on X-ray clusters in super-\\nclusters, while T able B2 lists additional systems of X-ray\\nclusters. Columns for both tables are as follows:\\n(1) identiﬁcation number of the supercluster in the cata-\\nlog; subscript C means supercluster candidate;\\n(2) Abell numbers of all clusters in the supercluster, ac-\\ncording to T able A1;\\n(3) , (4) and (5) – center coordinates for the supercluster\\n(α , δ and distance to the supercluster center);\\n(6): Catalog numbers of X-ray clusters in the superclus-\\nter. W e use Abell - ACO catalog numbers for clusters iden-\\ntiﬁed in this catalog. Cluster numbers without subscript\\nare from RBSC catalog; indexG means clusters from de\\nGrandi et al. (1999) catalog only , index D means clusters\\nfrom the DFJ catalog only , index B – clusters from the BCS\\ncatalog only .\\nDouble subscripts refer to non-Abell clusters. Index RR\\nmeans clusters number from RBS catalog; index BB – clus-\\nter number from BCS catalog; index GG – cluster number\\nfrom the catalog by de Grandi et al. (1999).\\nIn T able B2 clusters without subscripts refer to Abell\\nclusters that are not listed in the X-ray cluster catalogs\\nused in the present study .\\n(7): identiﬁcation of supercluster.\\nW e thank G¨ unther Hasinger for providing us with a draft\\nversion of the RBS catalog and discussion of preliminary re-\\nsults of the study . W e thank Enn Saar and Alexei Starobin-\\nsky for stimulating discussion. This work was supported by\\nEstonian Science F oundation grant 2625. JE thanks Astro-\\nphysical Institute Potsdam for hospitality where part of this\\nstudy was performed. HA thanks CONACyT for ﬁnancial\\nsupport under grant 27602-E.\\nREFERENCES\\nAbadi, M.G., Lambas, D.G., & Muriel, H., 1998, ApJ, 507, 526\\nAbell, G. 1958, ApJS, 3, 211\\nAbell, G., Corwin, H., & Olowin, R. 1989, ApJS, 70, 1 (ACO)\\nAndernach, H., & T ago, E. 1998, In: ”Large Scale Structure: Tracks\\nand T races”, W orld Scientiﬁc, Singapore, p. 147\\nBahcall, N.A., 1988, ARA&A, 26, 631\\nBardelli, S., Zucca, E., Zamorani, G., V ettolani, G., & Scaramella,\\nR., 1998, MNRAS, 296, 599\\nBirkinshaw, M., 1998, Physics Reports, 310, 97 [astro-ph/9 808050]\\nBond, J. R., Kofman, L., & Pogosyan, D., 1996, Nature, 380, 60 3\\nBorgani, S., & Guzzo, L., 2001, Nature, 4 January , [astro-\\nph/0012439]\\nBoughn, S.P ., 1999, ApJ, 526, 14\\nCollins, C.A., Guzzo, L., B¨ ohringer, H., Schuecker, P ., Chincarini,\\nG., Cruddace, R., De Grandi, S., MacGillivray , H.T., Neuman n,\\nD.M., Schindler, S., Shaver, P ., & V oges, W. 2000, MNRAS, 319 ,\\n939\\nDalton, G.B., Maddox, S.J., Sutherland, W.J., & Efstathiou , G. 1997,\\nMNRAS, 289, 263\\nDavid, L.P ., F orman, W., & Jones, C. 1999, ApJ, 519, 533\\nde Bernardis, P ., Ade, P . A. R., Bock, J. J., et al. 2000, Nature, 404,\\n955\\nDe Grandi, S., B¨ ohringer, H., Guzzo, L., Molendi, S., Chinc arini, G.,\\nCollins, C., Cruddace, R., Neumann, D., Schindler, S., Schu ecker,\\nP ., & V oges, W. 1999, ApJ, 514, 148\\nde V aucouleurs, G., 1953, MNRAS, 113, 134\\nDiaferio, A., Sunyaev, R.A., & Nusser, A., 2000, ApJL, 533, L71\\nEbeling, H., Edge, A., B¨ ohringer, H., Allen, S.W., Crawfor d, C.S.,\\nF abian, A.C, V oges, W., & Huchra, J.P . 1998, MNRAS, 301, 881\\nEinasto, J., Einasto, M., Gottl¨ ober, S., M¨ uller, V., Saar , V.,\\nStarobinsky , A.A., T ago, E., T ucker, D., Andernach, H., & F r isch,\\nP . 1997a, Nature, 385, 139 (E97a)\\nEinasto, J., Einasto, M., F risch, P ., Gottl¨ ober, S., M¨ ull er, V., Saar,\\nV., Starobinsky , A.A., T ago, E., T ucker, D. & Andernach, H. 1 997b,\\nMNRAS, 289, 801 (E97b)\\nEinasto, J., Einasto, M., T ago, E., Starobinsky , A.A., Atri o-\\nBarandela, F., M¨ uller, V., Knebe, A., & Cen, R., 1999a, ApJ, 519,\\n469 (E99)\\nEinasto M., Einasto J., T ago, E., Andernach, H., Dalton, G ., &\\nM¨ uller, V., 2001, (submitted to AJ, Paper III)\\nEinasto M., Einasto J., T ago, E., Dalton, G. & Andernach, H., 1994,\\nMNRAS, 269, 301 (EETDA)\\nEinasto J., J˜ oeveer M. & Saar E., 1980, MNRAS, 193, 503\\nEinasto, M., T ago, E., Jaaniste, J., Einasto, J., & Andernach, H.,\\n1997c, A&AS, 123, 119 (E97c)\\nEngels, D., T esch, F., Ledoux, C., W ei, J., Ugryumov, A., V al ls-\\nGabaud, D., Hu, J., & V oges, W., 1999, Proceedings of the\\nConference ”Highlights in X-ray Astronomy”, Eds. B. Aschenbach,\\nM. F reyberg, MPE-Report No. 272, pp. 218, [astro-ph/981118 2]\\nEttori, S., F abian, A.C., & White, D.A., 1997, MNRAS, 289, 78 7\\nF risch, P ., Einasto, J., Einasto, M., F reudling, F., F ricke , K.J.,\\nGramann, M., Saar, V., & T oomet, O., 1995, AA, 296, 611\\nGuzzo, L., B¨ ohringer, H., Schuecker, P ., et al 1999, Messen ger 95, 27.\\n[astro-ph/9903396]\\nHanany , S., Ade, P ., Balbi, A., et al. , 2000, ApJ (in press), [ astro-\\nph/0005123]\\nHenriksen, M., Donnelly , R., & Davis, D.S., 2000, ApJ, 529, 6 92\\nKashlinsky , A., & Atrio-Barandela, F., 2000, ApJ, 536, L67, [astro-\\nph/0005197]\\nKatz, N., W einberg, D.H., Miralda-Escude, J., & Hernquist, L., 1996,\\nApJ, 457, L57\\nKull, A., & B¨ ohringer, H., 1999, AA 341, 23\\nLee,S., & Park, C. 1999, [astro-ph/9909008]\\nLoken, C., Melott, A.L., & Miller, C.J., 1999, ApJ Letters, 520, L5\\nMarkevitch, M., F orman, W., Sarazin, C., & Vikhlinin, A., 19 98,\\nApJ, 503, 77 (MFSV)\\nMarkevitch, M., Ponman, T. J., Nulsen, P . E. J., et al. 2000, A pJ,\\n541, 542\\nMattig, W., 1958, Astron. Nachr., 284, 109\\nMiller, C., & Batuski, D., 2000, ApJ, submitted [astro-ph/0002295]\\nMoscardini, L., Matarrese, S., De Grandi, S., & Lucchin, F., 2000,\\nMNRAS, 314, 647, [astro-ph/9904282]\\nMoscardini, L., Matarrese, S., Lucchin, F., & Rosati, P ., 20 00,\\nMNRAS, 316, 283\\nMullis, C., 1999, in ”Large-Scale Structure in the X-ray Uni verse”,\\nSantorini, GREECE (September 1999), [astro-ph/9912258]\\nMullis, C., et al. 2000, ApJ, submitted 10\\nMyers, S.T., Baker, J.E., Readhead, A.C.S., Leitch, E.M., & Herbig,\\nT., 1997, ApJ, 485, 1\\nNesci R., & Norci L. 1997, Astrophys. Lett. and Comm., 36, 201\\nOort, J.H., 1983, Ann. Rev. Astron. Astrophys. 21, 373\\nPeacock, J.A. & W est, M.J., 1992, MNRAS, 259, 494\\nRefregier, A., Spergel, D.N., & Herbig, T., 2000, ApJ, 531, 31\\nRomer, A.K., Collins, C., B¨ ohringer, H., Cruddace, R., Ebe ling, H.,\\nMacGillivray , H.T., & V oges, W. 1994, Nature, 372, 75\\nScharf, C., Donahue, M., V oit, G.M., Rosati, P ., & Postman, M .,\\n2000, ApJ, 528, L73\\nSchwope, A.D., Hasinger, G., Lehmann, I., Schwarz, R., Brun ner, H.,\\nNeizvestny , S., Ugryumov, A., Balega, Y u., T r¨ umper, J., & V oges,\\nW. 2000, Astron. Nachr., 321, 1, [astro-ph/0003039] (RBS)\\nShectman, S.A., Landy , S.D., Oemler, A., T ucker, D.L., Lin, H.,\\nKirshner, R.P . & Schechter, P .L. 1996, ApJ, 470, 172\\nT ago, E., Einasto, J., Einasto, M., M¨ uller, V., & Andernach , H. 2001,\\n(submitted to AJ, Paper II)\\nT esch, F., & Engels, D., 2000, MNRAS, 313, 377\\nT r¨ umper, J., 1993, Science, 260, 1769\\nV ogeley , M. 1998, The Evolving Universe, ed. D. Hamilton,\\n(Dordrecht: Kluwer), p. 395 [astro-ph/9805160]\\nV oges, W. Aschenbach, B., Boller, T., et al. 1999, A&A, 349,3 89\\nWhite, D.A., Jones, C., & F orman, W., 1997, MNRAS, 292, 419\\nZeldovich, Y a.B., Einasto, J. & Shandarin, S.F. 1982, Nature, 300,\\n407 11\\nTable A1\\nThe list of superclusters\\n(1) (2) (3) (4) (5) (6) (7)\\nN o N CL α C δ C DC Abell-ACO No.\\nh−1M pc\\n2 2 0.9 16.3 316 2703 2705\\n1c 2 0.9 32.7 297 7 2687e\\n3 9 1.3 5.2 267 3 16 17 2694 2698 2700 2706 2691e 2696e Pegasus-P isces\\n4 5 1.6 -19.2 271 13 2682e 2710 2719 2756e Aquarius\\n221c 2 2.1 -8.4 346 12 2709e\\n5 5 3.0 -35.8 322 2715 2721 2730 2767 2772\\n222 2 3.4 -41.6 249 2736 2758\\n223c 4 4.6 -79.4 340 2723e 2727e 2837e 4057e\\n6c 5 4.3 -63.1 292 2732e 2796e 2821e 4051e 4067\\n224 2 5.2 -65.3 339 2760e 2770e\\n9 25 5.7 -31.1 289 42 88 118 122 2726e 2751 2755 2759e 2778 2780 S culptor\\n2798 2801 2804 2811 2814 2829 2844 2878e 3984 3998\\n4010 4021 4029 4068 4074e\\n7c 3 6.4 -53.5 303 2779e 2787e 2762\\n225 3 6.7 -21.9 241 50 51e 2765\\n10 19 7.6 -21.3 171 14 27 74 80 85 86 93 114 117 133 Pisces-Cetus\\n151 2660 2686 2716 2734 2800 2816 2824 4053\\n11 2 9.3 30.0 204 71 77\\n8 3 10.3 -38.1 189 2771 2799 2865\\n13 2 10.6 21.3 289 84 98\\n12c 4 11.2 -87.7 293 2757e 3037e 3299e 3650e\\n14 5 11.7 -1.6 331 94e 95 101 105e 112e\\n17 3 12.3 -7.2 283 89 108 121\\n226c 3 12.3 -11.3 334 91 106e 128e\\n15c 3 12.5 -16.6 269 99e 107e 123\\n16 4 12.7 -64.2 204 2810e 2819 2859 2864\\n18 6 13.5 -48.0 81 2731 2806 2836 2870 2877 2896 Phoenix\\n19 3 13.6 0.2 192 102 116 134\\n20c 3 14.4 -30.9 284 2847 2850 2851e\\n21 3 15.6 -48.9 183 2841 2854 2889\\n22 6 16.8 -37.5 325 2823e 2856 2871 2883e 2892e 2909e\\n24 7 16.9 7.7 127 76 119 147 160 168 193 195 Pisces\\n227c 2 18.2 14.8 348 152e 175\\n23 2 18.4 -38.5 216 2860 2911\\n25 2 18.9 37.0 214 161 174\\n228 5 19.2 3.9 344 153 162 172 192 203\\n26c 7 19.6 -18.5 314 2866e 199e 197e 187e 185e 166 183\\n27 2 20.4 -9.7 284 186 190\\n28 2 22.3 -27.6 238 2915 2924\\n29 3 23.0 16.9 306 200 201 247e\\n30 8 23.0 17.5 188 150 154 158 171 225 257 292 311 Pisces-Aries\\n31 4 24.2 -6.6 313 216 217 229 243\\n229 3 25.2 -10.4 351 226 228 259\\n32 2 25.5 6.5 213 245 246\\n33c 2 27.6 -78.6 305 2953e 2957e\\n34 6 27.7 -1.9 261 256 266 267e 268e 271e 277\\n230c 2 28.2 -21.0 343 2942 2966e\\n36 2 28.7 33.5 249 272 278\\n231 6 29.9 -5.0 349 265 274 281 287e 308e 336e\\n232c 9 30.5 -28.3 334 2944e 2961e 2967e 2968e 2971e 2972e 2979e 2981e 2983\\n37 5 34.0 -40.1 296 2960 2984 3006 3013 3033\\n39c 2 34.4 -7.5 307 326e 351e\\n233 6 36.2 -50.6 333 2987e 2988 3002e 3030e 3038 3065\\n40 3 37.8 40.7 53 262 347 426 Perseus\\n234 2 38.1 9.4 334 363 364\\n42c 2 41.5 -27.3 275 3052e 3054e\\n41c 2 41.4 -46.0 265 3059e 3047\\n235c 2 41.9 -47.9 348 3055e 3060e\\n43 3 42.1 -25.6 314 389 3044e 3070\\n236c 3 42.5 4.8 340 382e 392e 393e\\n44 2 43.1 36.8 137 376 407\\n45 2 44.2 13.7 208 399 401\\n46 2 46.5 -11.4 231 415 420\\n48 35 49.9 -46.7 194 3004 3009 3045 3074 3077 3078 3089 3093 3098 3100 Horologium-Reticulum\\n3104 3106 3108 3109 3110 3111 3112 3116 3120 3122\\n3123 3125 3128 3133 3135 3142 3145 3154e 3158 3161e\\n3164 3195 3202 3225 3266\\n50 8 50.6 -70.2 319 3080e 3143e 3155e 3117e 3119e 3121e 3136 31 86\\n237c 3 50.9 -44.4 348 3107 3130e 3132e\\n49 7 51.8 -25.9 184 419 428 3094 3095 3151 3188 3223\\n52 2 54.8 -25.8 292 458 3141\\n53 16 56.4 -31.8 293 3146 3148e 3118e 3152e 3153e 3159e 3166e 3169e 3171e 3173e Fornax-Er anus\\n3182e 3183e 3197e 3192 3194 3205e\\n238c 5 57.3 -18.5 333 459e 473e 3137e 3175e 3196e\\n55c 2 58.5 -78.5 318 3206e 3220e\\n56c 3 61.5 -63.4 270 3191e 3241e 3242e\\n239c 3 62.3 -46.1 335 3204e 3247e 3234 12\\nTable A1\\n...continued\\n(1) (2) (3) (4) (5) (6) (7)\\nN o N CL α C δ C DC Abell-ACO No.\\nh−1M pc\\n58 2 68.6 75.0 225 449 527\\n59 12 70.4 -33.6 295 3253e 3265e 3268e 3273e 3275e 3285e 3289e 3269 3295 3297 Caelum\\n3307e 3325e\\n240c 2 71.0 -17.9 326 512e 3288e\\n60 3 71.6 -20.4 203 500 514 524\\n61 2 73.0 4.1 235 509 526\\n62 5 74.2 8.5 297 515e 523 525e 529 532\\n63c 2 76.1 -50.6 315 3303e 3331e\\n64 2 80.0 -28.6 173 3323 3354\\n65 5 83.1 -41.3 224 3332 3336 3351e 3360 3379e\\n241c 3 83.5 -46.7 345 3349e 3359e 3363e\\n66 4 86.3 -21.4 264 550 3358 3365 3368\\n67 6 88.0 -28.2 114 548 3341 3367 3374 3381 3390\\n68 4 90.0 -51.4 154 3338 3380 3391 3395\\n69c 2 96.6 -50.9 272 3385e 3403e\\n70c 2 98.3 -53.1 207 3397e 3404e\\n71 5 100.9 69.4 308 554e 557e 561e 562 565\\n72 2 103.4 69.8 218 559 564\\n73 2 105.8 -49.1 117 3407 3408\\n74c 3 113.2 42.1 321 580e 585e 591\\n242 2 119.3 68.7 325 588 618\\n243c 2 120.4 80.7 337 575e 625e\\n244 2 120.5 62.5 334 604 608\\n75 2 121.3 34.9 226 612 628\\n245 2 127.3 15.2 249 658 689\\n246c 5 127.6 45.2 341 626e 655 681e 685e 691e\\n247c 2 129.1 51.1 345 678e 682e\\n248c 2 130.3 30.5 329 683e 705e\\n76 4 131.2 28.4 241 690 692 699 722\\n77 2 132.3 31.6 191 695 726\\n249 2 134.5 38.8 264 724 727\\n250c 3 136.7 48.5 339 716e 755e 756\\n79 3 137.6 4.3 326 745 769 774e\\n78 3 139.7 -9.3 151 754 780 838\\n81c 2 145.7 -25.2 167 3420 3429e\\n251 2 146.3 5.6 247 858 878\\n252c 4 146.8 1.8 335 869 867e 884e 892e\\n82 4 151.3 -0.1 262 912 919 933 954\\n83c 3 151.9 5.6 283 921e 941 949e\\n84c 2 152.1 -33.7 212 3432e 3443e\\n85c 2 152.8 19.3 310 942e 952e\\n86c 2 153.6 18.4 249 938e 991\\n87 3 153.9 65.1 324 871e 975 1014\\n88 5 155.5 -8.1 158 970 978 979 993 1069 Sextans\\n253c 2 155.9 9.2 336 989e 1022e\\n254 3 157.8 34.3 340 961 1033 1099e\\n255c 3 158.2 -8.8 338 1041e 1075e 1023\\n89c 2 158.4 38.6 301 1021e 1067\\n256c 6 158.9 42.4 340 967e 1028e 1050 1048e 1056e 1135e\\n257 14 161.5 75.6 337 718e 786 809 818 848 948 1029 1123 1150 1297 Draco-Ursa Majoris\\n1301 1381 1484 1536\\n258 3 162.3 9.3 244 1105 1113 1119\\n259c 3 162.3 17.7 351 1108e 1109e 1114e\\n91 9 162.7 2.9 209 1024 1032 1066 1078 1080 1149 1171 1205 1238 L eo-Sextans\\n90 3 163.5 17.4 248 1085e 1126 1168\\n94 2 165.3 25.5 133 1100 1213\\n93 10 165.8 22.8 95 999 1016 1139 1142 1177 1185 1228 1257 1267 1314 Leo\\n95 5 167.4 39.2 212 1155 1173 1187 1190 1203\\n260c 3 168.0 48.8 339 1143 1202e 1231e\\n96c 3 168.5 -32.7 299 3466e 3476e 3482e\\n97 2 170.1 47.2 310 1222 1227\\n261c 5 170.7 34.4 348 1197e 1226e 1245e 1266e 1305e\\n262 2 171.8 18.5 349 1264 1278\\n98 3 172.1 -4.6 162 1216 1308 1334\\n263c 2 173.0 -8.1 347 1295e 1323e\\n100 9 173.2 -3.0 279 1189e 1200 1214 1296e 1364 1386 1389e 1399 1404 Leo A\\n99 2 173.4 53.1 221 1218 1400\\n105 4 173.7 -11.5 277 1285 1309 1332 1375e\\n264c 4 173.6 43.4 350 1250e 1298e 1340e 1363e\\n102 2 174.0 -12.4 210 1317 1344\\n103 2 174.0 33.5 170 1275 1365\\n101 2 174.4 74.1 230 1186 1412\\n104c 2 174.8 -27.0 275 1347 3488e\\n106 2 175.2 6.2 270 1346 1362\\n107 8 175.9 9.9 310 1341 1342 1345 1354 1356 1372 1379e 1435e Leo-Virgo\\n108c 2 176.4 16.4 302 1338e 1408 13\\nTable A1\\n...continued\\n(1) (2) (3) (4) (5) (6) (7)\\nN o N CL α C δ C DC Abell-ACO No.\\nh−1M pc\\n109 8 177.1 55.0 170 1270 1291 1318 1377 1383 1436 1452 1507 Urs a Majoris\\n265 2 177.3 -1.7 350 1373 1419e\\n266 5 179.5 -33.0 191 3490 3492e 3497 3500e 3509\\n111 15 180.3 9.3 230 1262 1307 1337 1358 1385 1390 1424 1459 1474 1516 Virgo-Coma\\n1526 1527 1541 1552 1564\\n110 2 180.9 31.7 211 1423 1480\\n267c 4 181.0 16.3 341 1414e 1442e 1481e 1503e\\n268c 3 181.1 -7.8 341 1434e 1448 1502e\\n112c 3 181.3 -33.2 281 3494e 3504e 3508e\\n113c 2 181.6 -27.7 226 3501e 3507e\\n114 16 182.0 64.3 303 1289 1302 1322 1366 1402 1406 1421 1432 1446 1477 Draco\\n1518 1559 1566 1621 1646 1674\\n116 3 182.8 71.8 295 1382 1467 1597\\n115c 6 182.8 -28.5 313 3495e 3498e 3506e 3510e 3514e 3516e\\n269c 4 182.8 30.2 348 1427e 1486e 1543e\\n270c 2 183.8 6.7 340 1491e 1523e\\n117 2 185.3 24.1 64 1367 1656 Coma\\n118c 4 186.2 -15.2 269 1520e 1521 1535e 1585e\\n119c 6 188.9 -15.5 338 1555e 1558e 1572e 1573e 1584 1603e\\n271 2 189.3 17.2 2076 1569 1589\\n120 2 192.9 72.7 207 1500 1741\\n121c 4 193.3 -19.4 298 1605e 3529e 3534e 3539e\\n122 2 194.4 18.7 179 1638 1668\\n123c 2 195.5 51.3 304 1666 1673e\\n127 5 195.5 -30.8 212 1648 3524 3531 3549 3557\\n272c 2 195.7 -23.9 352 1664 1671e\\n126 7 195.8 -2.5 236 1620 1650 1651 1658 1663 1692 1750\\n124 3 195.7 -18.5 141 1631 1644 1709\\n125 2 195.9 9.0 250 1662 1684\\n129c 2 197.6 -32.5 256 3545 3546e\\n128 6 197.9 -33.0 39 1060 3526 3537 3565 3574 3581 Hydra-Centaurus\\n273c 2 198.1 38.0 330 1680e 1723e\\n130c 2 198.6 -22.6 294 1699e 3550e\\n131 4 198.8 60.0 331 1640 1701 1738 1764\\n274 2 219.8 38.1 202 1691 1715\\n124 28 200.5 -32.1 133 1631 1644 1709 1736 3528 3530 3532 3542 3548e 3552 Shapley\\n3553 3554 3555 3556 3558 3559 3560 3561e 3562 3563\\n3564 3566 3570 3571 3572 3575 3577 3578\\n133 2 204.5 57.0 199 1767 1783\\n132c 2 204.6 -13.0 317 1768e 1772e\\n134c 2 204.9 -36.5 246 3567e 3569e\\n136 4 206.2 3.6 219 1773 1780 1784 1809\\n275c 2 206.2 -11.9 249 1778e 1796\\n276c 2 207.5 25.5 342 1797e 1818e\\n137c 3 208.4 -26.6 231 1794e 1802e 1846e\\n277c 2 208.9 9.6 347 1808e 1844e\\n138 12 209.8 25.3 193 1775 1781 1795 1800 1825 1827 1828 1831 1861 1873 Bootes\\n1898 1927\\n139c 2 211.0 -24.6 316 1857e 3580e\\n141 4 212.2 -14.5 200 1836 1837 1876 3597e\\n140c 2 212.2 -21.7 273 3583e 3584e\\n278c 2 212.7 6.3 341 1870e 1881e\\n142 2 213.8 41.6 248 1885 1901\\n143 2 215.5 16.8 153 1899 1913\\n279c 2 215.6 26.7 340 1903e 1912e\\n144c 2 216.2 -22.1 254 3596 3599e\\n145c 2 218.0 -21.3 307 1924 1935e\\n146c 2 218.2 -30.4 176 3603 3605e\\n280c 2 218.2 47.4 350 1932e 1948e\\n147 4 220.6 54.9 286 1925 1962 1999 2000\\n148c 2 221.8 -31.2 230 3608e 3613e\\n149c 2 222.8 -24.8 297 1977e 1981e\\n281 3 223.2 27.6 341 1984 1990 2005\\n150 11 223.3 20.9 316 1960 1972 1976 1980 1986 1988 1997e 2001 2006 2017 Bootes A\\n2036\\n151c 3 223.8 -27.7 258 1996e 3612e 3614e\\n152 2 224.0 29.2 161 1982 2022\\n153 2 227.1 -0.5 252 2026 2030\\n154 4 228.0 4.8 221 2028 2029 2033 2066\\n156c 2 228.7 -11.3 303 2031e 2057e\\n155 2 228.8 -0.2 318 2050 2053\\n157 8 229.7 31.1 310 2034 2046e 2049 2056 2062 2069 2083 2110\\n158 8 230.8 29.7 206 2019 2061 2065 2067 2079 2089 2092 2124 Corona Borealis\\n282c 3 232.9 62.1 340 2074e 2090e 2137e\\n283c 2 233.6 3.7 318 2082e 2113e\\n284c 2 234.9 10.6 335 2101e 2119e\\n159c 2 235.6 -2.9 285 2103e 2128 14\\nTable A1\\n...continued\\n(1) (2) (3) (4) (5) (6) (7)\\nN o N CL α C δ C DC Abell-ACO No.\\nh−1M pc\\n160 12 236.2 18.4 105 2040 2052 2055 2063 2107 2147 2148 2151 21 52 2162 Hercules\\n2197 2199\\n161c 2 241.8 15.2 281 2153e 2159\\n162 5 242.9 52.8 180 2149 2168 2169 2184 2194\\n285c 3 246.0 27.4 348 2165 2186e 2217e\\n286c 2 246.4 71.3 346 2171 2236e\\n163c 3 247.4 -83.5 226 3624e 3626e 3629\\n164 5 247.6 27.8 273 2175 2178 2200 2223 2228\\n165c 2 249.6 -75.9 263 3628e 3630e\\n166c 2 251.3 53.9 309 2220 2242e\\n167 2 255.9 33.7 234 2245 2249\\n168 5 261.1 77.7 169 2248 2256 2271 2296 2309\\n169c 3 267.6 53.2 326 2284e 2286e 2292\\n287c 2 269.3 42.4 327 2285e 2297e\\n288c 5 272.8 73.8 339 2290e 2300e 2305e 2306 2310e\\n170 7 276.9 69.6 246 2295 2301 2304 2308 2311 2312 2315\\n171c 2 290.9 -87.8 322 3625e 3763e\\n172 3 303.2 -56.1 169 3651 3667 3685\\n173c 2 307.5 -25.6 244 2325e 3686\\n289c 2 307.7 -38.7 325 3676e 3699e\\n174 10 308.2 -35.0 255 3677 3681e 3682 3690e 3691 3693 3694 3695 3696 3705 Microscopium\\n176c 2 309.5 -32.7 299 3700e 3704e\\n177 2 309.5 -62.1 212 3687 3703\\n178c 4 310.5 -80.7 253 3644e 3684e 3728e 3741e\\n175 4 311.3 -39.3 68 3656 3698 3742 3747\\n180 3 314.4 -30.6 113 3706 3733 3744\\n179 2 314.5 -20.5 312 2330 2333\\n181c 5 318.5 -26.6 271 2338e 3734e 3752e 3753e 3758\\n183 2 319.2 -45.2 270 3754 3757\\n182 6 320.3 -43.3 202 3749 3751 3755 3756 3772 3809\\n290c 2 321.4 -5.4 328 2343e 2350e\\n185 c 4 321.7 -12.1 328 2340e 2345 2348e 2351e\\n186 2 322.7 -13.7 254 2346 2354\\n184 6 322.8 -22.3 323 2339 2347 2357 2371e 3770e 3778\\n187 4 324.1 0.4 327 2353 2355 2356 2379e\\n291c 2 325.6 -7.5 346 2367e 2374e\\n188 8 327.2 -12.9 168 2361 2362 2366 2372 2382 2399 2401 2415 Aquarius-Cetus\\n189 4 327.9 -19.7 230 2370e 2378 2394 2412 Aquarius-Capricor nus\\n190 5 328.4 -30.6 252 3795 3812 3813 3832 3837\\n292 2 328.7 -18.4 263 2384 2405\\n191 3 329.7 7.8 289 2398 2407 2414\\n192 8 329.6 -55.4 211 3771 3785 3806 3822 3825 3849 3867e 3886\\n193 8 330.5 -9.8 236 2376 2377 2400 2402 2410 2420 2428 2448 Aquarius B\\n194 3 331.2 -55.3 111 3816 3826 3869\\n195c 2 332.7 -10.3 284 2421e 2426\\n196 2 335.1 -1.9 256 2436 2440\\n293 3 335.4 14.3 335 2433 2437 2449\\n294c 2 336.5 -5.6 351 2442e 2446e\\n197 11 337.9 -49.7 274 3836 3850e 3862 3864 3877 3883 3908 39103911 3915e Grus\\n3922\\n199 3 339.2 -34.2 174 3880 3895 3912\\n200 3 341.4 -62.9 268 3898e 3907 3921\\n202 5 343.4 17.5 232 2458 2479 2516 2524 2564\\n295 2 343.9 -60.1 318 3906 3966e\\n296c 7 345.1 -14.0 336 2485e 2496 2504e 2519e 2544e 2549e 2563e\\n206 4 345.9 -44.5 345 3952e 3969 3970 3972\\n205 19 346.1 -20.2 237 2456 2459 2462 2480 2492 2500 2502e 2523e 2528 2538 Aquarius\\n2539e 2541 2556 2566 2586 2596e 2599 2600e 2605e\\n297c 2 346.7 -73.5 290 3932e 3986e\\n207 4 346.9 -13.9 300 2529 2533 2543 2559\\n209 7 348.1 -21.1 308 2546 2548 2554 3964e 2579 2583 3985\\n298 2 350.4 28.0 334 2584 2598e\\n210 6 350.4 -10.6 226 2511 2525 2569 2597 2638 2670\\n299 6 351.0 -24.9 341 2565 2577 2585e 2609e 4009e\\n211 4 351.5 15.3 121 2572 2589 2593 2657\\n212c 3 353.3 -69.1 285 3982 4007e 4066e\\n213 6 353.6 21.9 184 2618 2622 2625 2626 2630 2637\\n214 3 354.1 22.4 276 2619 2640 2649\\n300c 4 354.6 24.6 340 2627 2647e 2650e 2651e\\n215 2 355.5 27.4 88 2634 2666\\n216 2 357.1 -27.9 85 4038 4049\\n220 3 357.1 -36.4 147 2717 4008 4059\\n217 2 357.9 6.4 166 2665 2676\\n218 2 358.4 11.9 205 2675 2678 15\\nTable B1\\nThe list of X-ray clusters in superclusters\\n(1) (2) (3) (4) (5) (6) (7)\\nN o N CL α C δ C DC Cluster No.\\nh−1M pc\\n1c 2 0.9 32.7 297 7B\\n3 9 1.3 5.2 267 2700 Pegasus-Pisces\\n4 5 1.6 -19.2 271 13 Aquarius\\n5 5 3.0 -35.8 322 2721\\n9 25 5.7 -31.1 289 42G 2811 2829 11GG Sculptor\\n2016RR 1959RR\\n10 19 7.6 -21.3 171 85 2734 133 151 Pisces-Cetus\\n181RR\\n11 2 9.3 30.0 204 77\\n13 2 10.6 21.3 289 84\\n18 6 13.5 -48.0 81 2877G 28GG Phoenix\\n22 6 16.8 -37.5 325 2871\\n24 7 16.9 7.7 127 76B 119 147G 160B Pisces\\n168G 193 23GG 1BB\\n13BB 194BB\\n228 5 19.2 3.9 344 192D\\n30 8 23.0 17.5 188 292 Pisces-Aries\\n36 2 28.7 33.5 249 272B\\n40 3 37.8 40.7 53 262B 189B 14BB 18BB Perseus\\n19BB 426D\\n43 3 42.1 -25.6 314 389\\n44 2 43.1 36.8 137 376B 407B\\n45 2 44.2 13.7 208 399 401\\n48 35 49.9 -46.7 194 3104 3112 3122 3128 Horologium-Reticulum\\n3158 3266 459RR 3093D\\n3112D 3128D 3135D\\n49 7 51.8 -25.9 184 3223D\\n59 12 70.4 -33.6 295 3297D Caelum\\n50 8 50.6 -70.2 319 3186\\n52 2 54.8 -25.8 292 458\\n60 3 71.6 -20.4 203 500\\n61 2 73.0 4.1 235 42BB\\n62 5 74.2 8.5 297 523B\\n65 5 83.1 -41.3 224 3360D\\n66 4 86.3 -21.4 264 550 3358 3365 3368\\n67 6 88.0 -28.2 114 548G 3341 3390G 3367G Lepus\\n3301G 51GG 53GG 57GG\\n68 4 90.02 -51.4 154 3391G 3395G 61GG\\n243c 2 120.4 80.7 337 625D\\n244 2 120.5 62.5 334 51BB\\n246c 5 127.6 45.2 341 655B\\n78 3 139.7 -9.3 151 754D\\n257 14 161.5 75.6 337 1318D Draco-Ursa Majoris\\n88 5 155.5 -8.1 158 970 1069 Sextans\\n254 3 157.8 34.3 340 961 1033\\n91 9 162.7 2.9 209 1205 Leo-Sextans (Vela)\\n90 3 163.5 17.4 248 1126\\n93 10 165.8 22.8 95 1177B 1185B 1314 95BB Leo\\n96BB 97BB 99BB 100BB\\n1066RR\\n95 5 167.3 39.2 212 1173 1190\\n97 2 170.1 47.2 310 1227\\n101 2 174.4 74.1 230 1186D\\n102 2 174.0 -12.4 210 1042RR\\n105 4 173.7 -11.5 277 1285\\n109 8 177.1 55.0 170 1291 78BB Ursa Majoris\\n111 15 180.3 9.3 230 1307 1072RR 1092RR 98BB Virgo-Coma\\n99BB\\n110 2 180.9 31.7 211 1423\\n114 16 182.0 64.3 303 1302 1366 1446D 1566D Draco\\n117 2 185.3 24.10 64 1367 1656 1064RR Coma\\n271 2 189.3 17.2 2076 1589\\n126 7 195.8 -2.5 236 1650 1651 1663 1750\\n1773 1809\\n122 2 194.4 18.7 179 1668B\\n124 28 200.5 -32.1 133 1644 3528 3532 3558 Shapley\\n3562 1175RR 3559D 3566D\\n3571D\\n128 6 197.9 -33.0 39 1238RR 907RR 454RR 101BB Hydra-Centauru s\\n133 2 204.5 57.0 199 1767\\n136 4 206.2 3.6 219 1773 1809 110BB\\n138 12 209.8 25.3 193 1775 1795 1800 1831 Bootes\\n1927B 130BB\\n141 4 212.2 -14.5 200 1837\\n142 2 213.8 41.6 248 1885\\n154 4 228.0 4.8 221 2029 2033\\n155 2 228.8 -0.2 318 2050\\n157 8 229.7 31.1 310 2069 2110 2034B\\n158 8 230.8 29.7 206 2061 2065 Corona Borealis\\n160 12 236.2 18.4 105 2052 2055 2063 2107 Hercules\\n2147 2151 2199B 1488RR\\n1554RR 1654RR 1632RR 166BB\\n172BB 179BB\\n161c 2 241.8 15.2 281 1552RR\\n162 5 242.9 52.8 180 2149\\n164 5 247.6 27.8 273 2175\\n166c 2 251.3 53.9 309 174BB\\n167 2 255.9 33.7 234 2249B\\n168 5 261.1 77.7 169 2256\\n170 7 276.9 69.6 246 2312B NEP\\n172 3 303.2 -56.1 169 3651G 3667 3809 1719RR\\n75GG\\n174 10 308.2 -35.0 255 3693G 3694 3695 1691RR Microscopium\\n175 4 311.3 -39.3 68 1676RR\\n180 3 314.4 -30.6 113 3744\\n182 6 320.3 -43.3 202 3809 91GG\\n187 4 324.1 0.4 327 2355D 2356D\\n188 8 327.2 -12.9 168 2382G 2415 98GG Aquarius-Cetus\\n192 8 329.6 -55.4 211 3806 3822 3825 1775RR\\n87GG 112GG\\n193 8 330.5 -9.8 236 2377G 2402G 2420 2428 AquariusB\\n2440G 108GG\\n194 3 331.2 -55.3 111 114GG\\n195 2 332.7 -10.3 284 2426\\n196 2 335.1 -1.9 256 2440 108RR\\n197 11 337.9 -49.7 274 3836 3911 Grus\\n199 3 339.2 -34.2 174 3880 1831RR 1921D\\n200 3 341.4 -62.9 268 3921 3827G 107GG\\n296c 7 345.1 -14.0 336 2496\\n205 19 346.1 -20.2 237 2556 2566 Aquarius\\n210 6 350.4 -10.6 226 2597G 2670 2042RR\\n211 4 351.5 15.3 121 2572 2589B 2593B 2657\\n1929RR 1977RR 1BB 194BB\\n212c 3 353.3 -69.1 285 1985RR\\n213 6 353.6 21.9 184 2622B 2626\\n300c 4 354.6 24.6 340 2627\\n215 2 355.5 27.4 88 2634 16\\nTable B2\\nThe list of additional superclusters of non-Abell X-ray clu sters\\n(1) (2) (3) (4) (5) (6)\\nN o N CL α C δ C DC Cluster No.\\nh−1M pc\\n1 2 5.2 28.9 264 21 4BB\\n2 2 11.6 25.0 232 104RR 104\\n3 2 27.2 -4.6 117 295 23GG\\n4 4 32.1 2.7 59 194 400 199RR 26GG\\n5 2 32.4 31.3 103 260 25BB\\n6 2 34.8 -27.6 169 2992 317RR\\n7 3 51.4 14.2 93 397 456RR 461RR\\n8 2 55.6 -54.4 127 3144 485RR\\n9 3 68.8 -12.1 104 496 44GG 540RR\\n10 2 94.7 -64.8 73 3389 62GG\\n11 2 114.0 52.8 186 595 44BB\\n12 5 114.9 54.3 81 569 576 634 47BB\\n48BB\\n13 2 215.1 48.5 201 1904 1380RR\\n14 2 222.9 22.3 271 2021 130BB\\n15 2 305.9 -20.2 160 2324 76GG\\n16 2 315.5 -52.2 136 3716 1719RR\\n17 2 327.4 -44.6 173 3809 91GG\\n18 2 350.9 -40.5 161 4008 122GG\\n19 2 356.1 -3.8 221 2656 2042RR']], 'uris': None, 'data': None, 'metadatas': [[{'filename': '1004.0694v1.pdf', 'keyword': 'clustering', 'topic': 'Machine Learning'}, {'filename': '0012536v1.pdf', 'keyword': 'clustering', 'topic': 'Machine Learning'}]], 'distances': [[1.4157040119171143, 1.468011498451233]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "# Querying the collection\n",
    "query = \"BCG Luminosity Determination\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=2  # Number of results\n",
    ")\n",
    "print(\"Search Results:\", results)\n",
    "\n",
    "# Data is automatically saved; no need for an explicit persist call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'data', 'metadatas', 'distances', 'included'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['2304.05133v2', '1702.06794v1']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['Lecture Notes:\\nNeural Network Architectures\\nEvelyn Herberg1\\n1 Interdisciplinary Center for Scientiﬁc Computing, Ruprecht-Karls-University of Heidelberg,\\n69120 Heidelberg, Germany\\nApril 2023\\nAcknowledgement\\nThese lecture notes were written to serve as theoretical background for programming sessions\\ngiven by Florian Wolf during the SPP1962 Young Researchers’ workshop on Deep Learning in\\nMarch 2023.\\nI do not claim originality, but merely collected material from various sources and wrote it down\\nin a cohesive way. Any mistakes that I may have introduced are of course at my fault.\\nEspecially, parts of these lecture notes are based on a student seminar at the University of\\nHeidelberg that Roland Herzog and I organized together. I thank the involved students: Deniz\\nAydin, Laurin Ernst, Yanxin Jia, Xinyu Liang, Hannah Rickmann, Viktor Stein von Kamienski,\\nXiao Wang and Zixiang Zhou for their contributions.\\nThe main literature for the seminar, and consequently also for these lecture notes, was [15] and\\n[26]. Additional sources are mentioned throughout the document.\\nFurthermore, I thank Harbir Antil and the CMAI work group at George Mason University for\\ntheir guidance in understanding Machine Learning from an optimal control point of view. The\\nnotation in this document is highly inﬂuenced by the CMAI work group, cf. [2, 1, 3].\\nPlease send comments and remarks toevelyn.herberg@iwr.uni-heidelberg.de.\\narXiv:2304.05133v2  [cs.LG]  18 Apr 2023 CONTENTS 2\\nContents\\n1 Introduction 3\\n1.1 Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.2 Unsupervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.3 Optimization Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.4 Overﬁtting and Underﬁtting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n1.5 Hyperparameters and Data Set Splitting . . . . . . . . . . . . . . . . . . . . . . . 12\\n1.6 Modeling logical functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2 Feedforward Neural Network 14\\n2.1 Depth and Width . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.2 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.3 Batch Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.4 Classiﬁcation Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.5 Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n3 Convolutional Neural Network 26\\n3.1 Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n3.2 Convolutional Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n3.3 Detector Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n3.4 Pooling Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n3.5 Local Response Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n4 ResNet 36\\n4.1 Diﬀerent ResNet Versions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n4.2 ResNet18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n4.3 Transfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n5 Recurrent Neural Network 41\\n5.1 Variants of RNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n5.2 Long term dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\\n5.2.1 Gated Recurrent Unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n5.2.2 Long Short Term Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\\n5.3 Language processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 1 INTRODUCTION 3\\n1. Introduction\\nMachine Learning (ML) denotes the ﬁeld of study in which algorithms infer from given data how\\nto perform a speciﬁc task, without being explicitly programmed for the task (Arthur Samuel,\\n1959). Here, we consider a popular subset of ML algorithms:Neural Networks. The inspi-\\nration for a Neural Network (NN) originates from the human brain, where biological neurons\\n(nerve cells) respond to the activation of other neurons they are connected to. At a very simple\\nlevel, neurons in the brain take electrical inputs that are then channeled to outputs. The sensi-\\ntivity of this relation also depends on the strength of the connection, i.e. a neuron may be more\\nresponsive to one neuron, then to another.\\nFigure 1. Brain Neuron Struc-\\nture: electrical inputs are received\\nthrough dendrites and transmitted via\\nthe axon to other cells. There are\\napproximately 86 billion neurons in\\nthe human brain. Image modiﬁed\\nfrom: https://www.smartsheet.com/\\nneural-network-applications.\\nFor a single neuron/node with inputu ∈Rn, a mathematical model, named theperceptron\\n[27], can be described as\\ny= σ\\n( n∑\\ni=1\\nWiui + b\\n)\\n= σ(W⊤u+ b), (1)\\nwhere y is theactivationof the neuron/node,Wi are theweightsand b is thebias.\\ny[0]\\n1\\ny[0]\\n2\\ny[0]\\n3\\ny[1]\\n1\\ninput\\nlayer\\noutput\\nlayer Figure 2. Schematic representation of the perceptron\\nwith a three dimensional input. For generality we denote\\nthe input byy[0] = u and the output byy[1] = y. The\\nweightsWi are applied on the arrows and the bias is added\\nin the nodey[1]\\n1 .\\nThe functionσ : R →R is calledactivation function. Originally, in [27], it was proposed to\\nchoose the Heaviside function as activation function to model whether a neuron ﬁres or not, i.e.\\nσ(y) =\\n{\\n1 if y≥0,\\n0 if y <0.\\nHowever, over time several other activation functions have been suggested and are being used.\\nTypically, they are monotone increasing to remain in the spirit of the original idea, but contin-\\nuous. 1 INTRODUCTION 4\\nPopular activation functions are, cf. [26, p.90]\\nσ(y) = 1\\n1 + exp(−y) sigmoid (logistic),\\nσ(y) = tanh(y) = exp(y) −exp(−y)\\nexp(y) + exp(−y) hyperbolic tangent,\\nσ(y) = max{y,0} rectiﬁed linear unit (ReLU),\\nσ(y) = max{αy,y} leaky ReLU.\\n−4−2 0 2 4\\n−1\\n0\\n1\\n(a) Heaviside\\n−4−2 0 2 4\\n−1\\n0\\n1\\n(b) sigmoid\\n−4−2 0 2 4\\n−1\\n0\\n1\\n(c) tanh\\n−4−2 0 2 4\\n−1\\n0\\n1\\n(d) ReLU\\n−4−2 0 2 4\\n−1\\n0\\n1\\n(e) leaky ReLU\\nFigure 3. Popular activation functions. Leaky ReLU is displayed forα= 0.1.\\nRemark 1.1 The nonlinearity of activation functions is an integral part of the Neural Networks\\nsuccess. Since concatenations of linear functions result again in a linear function, see e.g. [26,\\np.90], the complexity that can be achieved by using linear activation functions is limited.\\nWhile the sigmoid function approximates the Heaviside function continuously, and is diﬀeren-\\ntiable, it contains an exponential operation, which is computationally expensive. Similar prob-\\nlems arise with the hyperbolic tangent function. However, the fact thattanh is closer to the\\nidentity function often helps speed up convergence, since it resembles a linear model, as long as\\nthe values are close to zero. Another challenge that needs to be overcome is vanishing derivatives,\\nwhich is visibly present for Heaviside, sigmoid and hyperbolic tangent. In contrast, ReLU is not\\nbounded on positive values, while also being comparatively cheap to compute, because linear\\ncomputations tend to be very well optimized in modern computing. Altogether, these advan-\\ntages have resulted in ReLU (and variants thereof) becoming the most widely used activation\\nfunction currently. As a remedy for the vanishing gradient on negative values, leaky ReLU was\\nintroduced. When taking derivatives of ReLU one needs to account for the non-diﬀerentiability\\nat 0, but in numerical practice this is easily overcome.\\nWith the help of Neural Networks we want to solve a task, cf. [15, Section 5.1]. Let the\\nperformance of the algorithm for the given task be measured by theloss function L, which\\nneeds to be adequately modeled. ByFwe denote the Neural Network. The variables that will\\nbe learned are the weightsW and biasesb of the Neural Network. Hence, we can formulate the\\nfollowing optimization problem, cf. [1, 2, 3]\\nmin\\nW,b\\nL(y,u,W,b ) s.t. y= F(u,W,b). (P)\\nOne possible choice for F has already been given in (1), the perceptron. In the subsequent\\nsections we introduce and analyze various other Neural Network architectures. They all have in\\ncommon that they contain weights and biases, so that the above problem formulation remains\\nsensible.\\nBefore we move on to diﬀerent network architectures, we discuss the modeling of the loss function.\\nLearning tasks can be divided into two subgroups: Supervised and Unsupervised learning. 1 INTRODUCTION 5\\n1.1. Supervised Learning\\nIn supervised learning we have given datauwith known supervisionS(u) (also called labels), so\\nthat the task is to match the outputyof the Neural Network to the supervision. These problems\\nare further categorized depending on the known supervision, e.g. forS(u) ∈N it is called a\\nclassiﬁcation and forS(u) ∈R a regression. Furthermore, the supervision S(u) can also take\\nmore complex forms like a black and white picture of256 ×256 pixels represented by[0,1]256, a\\nhigher dimensional quantity, a sentence, etc. These cases are called structured output learning.\\nLet us consider one very simple example, cf. [15, Section 5.1.4].\\nExample 1.2 Linear Regression\\nWe have a given set of inputsu(i) ∈Rd with known supervisionsS(u(i)) ∈R for i= 1,...,N .\\nIn this example we only consider weightsW ∈Rd and no bias. Additionally, letσ = id. The\\nperceptron network simpliﬁes to\\ny(i) = W⊤u(i),\\nand the learning task is to ﬁndW, such thaty(i) ≈S(u(i)). This can be modeled by themean\\nsquared error (MSE)function\\nL({y(i)}i,{u(i)}i,W) := 1\\n2N\\nN∑\\ni=1\\n∥y(i) −S(u(i))∥2.\\nBy convention we will use ∥·∥ = ∥·∥ 2 throughout the lecture. The chosen loss function is\\nquadratic, convex and non-negative. We deﬁne\\nU :=\\n\\uf8eb\\n\\uf8ec\\uf8ed\\n(u(1))⊤\\n...\\n(u(N))⊤\\n\\uf8f6\\n\\uf8f7\\uf8f8∈RN×d, S :=\\n\\uf8eb\\n\\uf8ec\\uf8ed\\nS(u(1))\\n...\\nS(u(N))\\n\\uf8f6\\n\\uf8f7\\uf8f8∈RN,\\nso that we can writeL(W) = 1\\n2 ∥UW −S∥2\\n2. Minimizing this function will deliver the same\\noptimal weightW as minimizing the MSE function deﬁned above. We can now derive the gradient\\n∇WL(W) = U⊤UW −U⊤S\\nand immediately ﬁnd the stationary pointW = (U⊤U)−1U⊤S.\\n1.2. Unsupervised Learning\\nIn unsupervised learning, only the input datau is given and we have no knowledge of super-\\nvisions or labels. The algorithm is supposed to learn e.g. a structure or relation in the data.\\nSome examples are k-clustering and principal component analysis (PCA). Modeling the loss\\nfunction speciﬁes the task and has a direct inﬂuence on the learning process. For illustration of\\nthis concept, we introduce the k-means algorithm, see eg. [26, Chapter 10], which is used for\\nclustering. 1 INTRODUCTION 6\\nExample 1.3 We have a set of given data points\\n{\\nu(i)\\n}N\\ni=1\\n∈Rd,\\nand a desired number of clustersk ∈N with k ≤N and typically k ≪N. Every data point is\\nsupposed to be assigned to a cluster. Iteratively every data point is assigned to the cluster with\\nthe nearest centroid, and we redeﬁne cluster centroids as the mean of the vectors in the cluster.\\nThe procedure is speciﬁed in Algorithm 1 and illustrated for an example in Figure 4, which can\\nbe found e.g. in [26, Chapter 10]. The loss function (also called distortion function in this setup)\\ncan be deﬁned as\\nL(c,µ) :=\\nN∑\\ni=1\\n∥u(i) −µc(i) ∥2,\\nwhich is also a model of the quantity that we try to minimize in Algorithm 1. We have a non-\\nconvex set of points inRd, so the algorithm may converge to a local minimum. To prevent this,\\nwe run the algorithm many times, compare the resulting clusterings using the loss function, and\\nchoose the one with the minimal value attained in the loss function.\\nFigure 4. Visualization of k-means algorithm fork= 2 clusters. Data pointsu(i) are indicated\\nas dots, while cluster centroidsµj are shown as crosses. Top, from left to right: Dataset, random\\ninitial cluster centroidsµ1 (red) and µ2 (blue), every data point is assigned to either the red\\nor blue cluster. Bottom, from left to right: cluster centroids are redeﬁned, every data point is\\nreassigned, cluster centroids are redeﬁned again. Image source: [26, Chapter 10].\\nWe will see various other loss functionsL throughout the remainder of this lecture, all of them\\nspeciﬁcally tailored to the task at hand.\\nIn the case of Linear Regression, we have a closed form derivative, so we are able to ﬁnd the\\nsolution by direct calculus, while for k-means clustering the optimization was done by a tailored\\niteration. For general problems we will need a suitable optimization algorithm. We move on to\\nintroduce a few options. 1 INTRODUCTION 7\\nAlgorithm 1k-means clustering\\nRequire: Initial cluster centroidsµ1,...,µ k\\nwhile not convergeddo\\nfor i= 1 : N do\\nc(i) := arg minj∥u(i) −µj∥2\\nend for\\nfor j = 1 : k do\\nµj ←\\n∑N\\ni=1 1{c(i)=j}u(i)\\n∑N\\ni=1 1{c(i)=j}\\nend for\\nend while\\n1.3. Optimization Algorithms\\nHere, for simplicity we deﬁneθ, which collects all variables, i.e. weightsW and biasband write\\nthe loss function as\\nL(θ) = 1\\nN\\nN∑\\ni=1\\nL(i)(θ),\\nwhich we want to minimize. Here,L(i) indicates the loss function evaluated for data pointi,\\nfor example with a MSE lossL(i)(θ) = 1\\n2 ∥y(i) −S(u(i))∥2.\\nFirst, let us recall the standardgradient descentalgorithm, see e.g. [6, Section 9.3], which is\\nalso known as steepest descent or batch gradient descent.\\nAlgorithm 2Gradient Descent\\nRequire: Initial pointθ0, step sizeτ >0, counterk= 0.\\nwhile Stopping criterion not fulﬁlleddo\\nθk+1 = θk −τ ·∇L(θk),\\nk←k+ 1.\\nend while\\nPossible stopping criterion are e.g. setting a maximum number of iterationsk, reaching a certain\\nexactness ∥L(θ)∥<ϵ with a small numberϵ> 0, or a decay in change∥θk+1 −θk∥<ϵ. Deter-\\nmining a suitable step size is integral to the success of the gradient descent method, especially\\nsince this algorithm uses the same step sizeτ for all components ofθ, which can be a large vector\\nin applications. If may happen that in some components the computed descent direction is only\\nproviding descent in a small neighborhood, therefore requiring a small step sizeτ. It is also\\npossible to employ a line search algorithm. However, this is not common in Machine Learning\\ncurrently. Instead, typically a small step size is chosen, so that it will (hopefully) be not too large\\nfor any component ofθ, and then it may be adaptively increased. Furthermore, let us remark\\nthat the step size is often calledlearning ratein a Machine Learning context.\\nAdditionally, a grand challenge in Machine Learning tasks is that we have huge data sets, and\\nthe gradient descent algorithm has to iterate over all data points in every iteration, sinceL(θ) 1 INTRODUCTION 8\\ncontains all data points, which causes a tremendous computational cost. This motivates the use\\nof thestochastic gradient descentalgorithm, cf. [26, Algorithm 1], which only takes one data\\npoint into account per iteration.\\nAlgorithm 3Stochastic Gradient Descent (SGD)\\nRequire: Initial pointθ0, step sizeτ >0, counterk= 0, maximum number of iterationsK.\\nwhile k≤K do\\nSample j ∈{1,...,N }uniformly.\\nθk+1 = θk −τ ·∇L(j)(θk),\\nk←k+ 1.\\nend while\\nSince the stochastic gradient descent method only calculates the gradient for one data point, it\\nproduces an irregular convergence behavior. Indeed, it does not necessarily converge at all, but\\nfor a large number of iterationsK it often produces a good approximation. In fact, actually\\nconverging in training the Neural Network is often not necessary/desired anyhow, since we want\\nto have a solution that generalizes well to unseen data, rather than ﬁt the given data points\\nperfectly. Actually, the latter may lead to overﬁtting, cf. Section 1.4. Therefore, SGD is\\na computationally cheap, reasonable alternative to gradient descent. As a compromise, which\\ngenerates a less irregular convergence behavior, there also existsmini batch gradient descent,\\ncf. [26, Algorithm 2], where every iteration takes into account a subset (mini batch) of the data\\npoints.\\nAlgorithm 4Mini Batch Gradient Descent\\nRequire: Initial point θ0, step sizeτ >0, counter k = 0, maximum number of iterationsK,\\nbatch sizeb∈N.\\nwhile k≤K do\\nSample b examples j1,...,j b uniformly from{1,...,N }\\nθk+1 = θk −τ ·1\\nb\\n∑b\\ni=1 ∇L(ji)(θk),\\nk←k+ 1.\\nend while\\nFinally, we introduce a sophisticated algorithm for stochastic optimization calledAdam, [23],\\nsee Algorithm 5. It is also a gradient-based method, and as an extension of the previous methods\\nit employs adaptive estimates of so-called moments. Good default settings in Adam for the tested\\nmachine learning problems areτ = 0.001, β1 = 0.9,β2 = 0.999 and ϵ= 10−8, cf. [23]. Typically,\\nthe stochasticity of ˜L(θ) will come from using mini batches of the data set, as in Mini Batch\\nGradient Descent, Algorithm 4. 1 INTRODUCTION 9\\nAlgorithm 5Adam. All operations on vectors are element-wise.(gk)2 indicates the element-\\nwise squaregk ⊙gk, and(β1)k,(β2)k denote thek-th power ofβ1 and β2, respectively.\\nRequire: Initial pointθ0, step sizeτ >0, counterk= 0, exponential decay rates for the moment\\nestimates β1,β2 ∈[0,1), ϵ> 0, stochastic approximation ˜L(θ) of the loss function.\\nm0\\n1 ←0 (Initialize ﬁrst moment vector)\\nm0\\n2 ←0 (Initialize second moment vector)\\nwhile θk not convergeddo\\ngk+1 = ∇θ ˜L(θk)\\nmk+1\\n1 = β1 ·mk\\n1 + (1 −β1) ·gk+1\\nmk+1\\n2 = β2 ·mk\\n2 + (1 −β2) ·(gk+1)2\\nmk+1\\n1 ← mk+1\\n1\\n(1−(β1)k)\\nmk+1\\n2 ← mk+1\\n2\\n(1−(β2)k)\\nθk+1 = θk −τ · mk+1\\n1(√\\nmk+1\\n2 +ϵ\\n)\\nk←k+ 1\\nend while\\nRemark 1.4 In any case we need to be cautious when in-\\nterpreting results, since independent of the chosen algorithm,\\nwe are dealing with a non-convex loss function, so that we\\ncan only expect convergence to stationary points.\\nFigure 5. Simple example of a non-convex loss function\\nwith a local and a global minimum.\\nθ\\nLoss\\nIn the following section we discuss how ﬁtting the given data points and generalizing well to\\nunseen data can be contradictory goals.\\n1.4. Overﬁtting and Underﬁtting\\nAs an example we discuss supervised learning with polynomials of degreer, cf. [12, Section\\n1.3.3].\\nExample 1.5 Deﬁne\\np(u,W) :=\\nr∑\\nj=0\\nWjuj = W⊤u,\\nwith u = (u0,...,u r)⊤ ∈Rr+1 the potencies of data pointu, and W := (W0,...,W r)⊤ ∈Rr+1.\\nThe polynomialpis linear inW, but not inu. As in Linear Regression (Example 1.2), we do not\\nconsider biasb here. Our goal is to compute weightsW, given data pointsu(i) with supervisions 1 INTRODUCTION 10\\nS(u(i)), so that p makes good predictions on data it hasn’t seen before. We again employ the\\nMSE loss function\\nL(W) = 1\\n2N\\nN∑\\ni=1\\n∥p(u(i),W) −S(u(i))∥2\\nAs before, we write the loss in matrix-vector notation\\nL(W) = 1\\n2N∥UW −S∥2\\nwhere\\nU :=\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed\\nu(1)\\n0 u(1)\\n1 ... u (1)\\nr\\n... ... ...\\nu(m)\\n0 u(m)\\n1 ... u (m)\\nr\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8, S:=\\n\\uf8eb\\n\\uf8ec\\uf8ed\\nS(u(1))\\n...\\nS(u(m)).\\n\\uf8f6\\n\\uf8f7\\uf8f8\\nThe minimizerW can be directly calculated, cf. Example 1.2.\\nFigure 6. Plots of polynomials of various degrees r (red graph) ﬁtted to the noisy data points\\n(green dots) based on the ground truth (green graph). The model should extend well to the test\\nset data (blue dots). We observe underﬁtting in the top row forr = 0 (left) andr = 1 (right).\\nIn the bottom left withr = 3 reasonable results are achieved, whiler = 9 in the bottom right\\nleads to overﬁtting. Image modiﬁed from: [12, Fig. 1]. 1 INTRODUCTION 11\\nTo measure the performance of the polynomial curve ﬁtting we compute the error on data points\\nthat were not used to determine the best polynomial ﬁt, because we aim for a model that will\\ngeneralize well. To this end, ﬁnding a suitable degree for the polynomial that we are ﬁtting over\\nthe data points is crucial. If the degree is too low, we will encounterunderﬁtting, see Figure\\n6 top row. This means that the complexity of the polynomial is too low and the model does\\nnot even ﬁt the data points. A remedy is to increase the degree of the polynomial, see Figure\\n6 bottom left. However, increasing the degree too much may lead tooverﬁtting, see Figure 6\\nbottom right. The data points are ﬁt perfectly, but the curve will not generalize well.\\nWe can characterize overﬁtting and underﬁtting by using some statistics, cf. [26, Section 8.1].\\nA point estimatorg : UN →Θ (where Udenotes the data space, andΘ denotes the parameter\\nspace) is a function which makes an estimation of the underlying parameters of the model. For\\nexample, the estimate forθ = W from Example 1.2: ˆθ = (U⊤U)−1U⊤S (which we will denote\\nwith a hat in this subsection to emphasize that it is an estimation) is an example of a point\\nestimator. We assume that the data fromUN is i.i.d, so thatˆθ is a random variable. We can\\ndeﬁne the variance and the bias\\nVar(ˆθ) := E(ˆθ2) −E(ˆθ)2, Bias(ˆθ) := E(ˆθ) −θ,\\nwith E denoting the expected value. A good estimator has both, low variance and low bias.\\nWe can characterize overﬁtting with low bias and high variance, and underﬁtting with high bias\\nand low variance. The bias-variance trade-oﬀ is illustrated in Figure 7. Hence, we can make a\\ndecision based on mean squared error of the estimates\\nMSE(ˆθ) := E[(ˆθ−θ)2] = Var(ˆθ) + Bias(ˆθ)2.\\nFigure 7. Bias-variance\\ntrade-oﬀ. Image source:\\n[26, Fig. 8.8].\\nIn general, it can be hard to guess a suitable degree for the polynomial beforehand. We could\\ncompute a ﬁtting curve for diﬀerent choices ofrand then compare the error on previously unseen\\ndata points of the validation data set, cf. Section 1.5, to determine which one generalizes best.\\nThis will require solving the problem multiple times which is unfavorable, especially for large\\ndata sets. Also, the polynomial degree can only be set discretely. Another, continuous way is to\\nintroduce a penalization term in the loss function\\nLλ(θ) := L(θ) + λ∥θ∥2.\\nThis technique is also calledweight decay, cf. [15, Section 5.2.2]. We can also use other norms,\\ne.g. ∥·∥1. Here, we can choose a large degreerand forλbig enough, we will still avoid overﬁtting,\\nbecause many components ofθ will be (close to) zero. Nonetheless, we need to be cautious with\\nthe choice ofλ. If it is too big, we will face again the problem of underﬁtting. 1 INTRODUCTION 12\\nWe see that choosing values for the degreer and the penalization parameterλposes challenges,\\nand will discuss this further in the next section.\\n1.5. Hyperparameters and Data Set Splitting\\nWe call all quantities that need to be chosen before solving the optimization problemhyper-\\nparameters, cf. [15, Section 5.3]. Let us point out that hyperparameters are not learnt by the\\noptimization algorithm itself, but nevertheless have an impact on the algorithms performance.\\nExamples of hyperparameters include the polynomial degreer, the scalarλ, all parameters in\\nthe optimization algorithms (Section 1.3) like the step sizeτ, and also the architecture of the\\nNeural Network, and many more.\\nThe impact of having a good set of hyperparameters can be tremendous, however ﬁnding such a\\nset is not trivial. First of all, we split our given data into three sets.training data, validation\\ndata andtest data (a 4:1:1 ratio is common). We have seen training and test data before. The\\ndata points that we are using as input to solve the optimization problem are called training\\ndata, and the unseen data points, which we use to evaluate whether the model generalizes well,\\nare called test data. Since we don’t want to mix diﬀerent causes of error, we also introduce the\\nvalidation data set. This will be used to compare diﬀerent choices of hyperparameter conﬁg-\\nurations, i.e. we train the model on the training data for diﬀerent hyperparameters, compute\\nthe error on the validation data set, choose the hyperparameter setup with the lowest error and\\nﬁnally evaluate the model on the test set. The reasoning behind this is that if we would use\\nthe test data set to determine the hyperparameter values, the test error may be not meaningful,\\nbecause the hyperparameters have been optimized for this speciﬁc test set. Since we are using\\nthe validation set, we will have the test set with previously unseen data available to determine\\nthe generalization error without giving our network an advantage.\\nStill, imagine you need to choose 5 hyperparameters and have 4 possible values that you want\\nto try for each hyperparameter. This amounts to45 = 1024 combinations you have to run on\\nthe training data and evaluate on the validation set. In real applications the number of hyper-\\nparameters and possible values can be much larger, so that it is nearly infeasible to try every\\ncombination, but rather common to change one hyperparameter at a time. Luckily, some hyper-\\nparameters also have known good default values, like the hyperparameters for Adam Optimizer,\\nAlgorithm 5. Apart from that it is a tedious, manual work to try out, monitor and choose\\nsuitable hyperparameters.\\nFinally, we discuss the limitations of shallow Neural Networks, i.e. networks with only one layer.\\n1.6. Modeling logical functions\\nLet us consider a shallow Neural Network with input layery[0] ∈N2 and output layery[1] ∈N.\\ny[0]\\n1\\ny[0]\\n2\\ny[1]\\n1\\ninput\\nlayer\\noutput\\nlayer\\nFigure 8.A simple perceptron (shallow Neural Network)\\nwith a two dimensional input.\\nWe model true by the value 1 and false by the value 0, which results in the following truth table\\nfor the logical \"OR\" function. 1 INTRODUCTION 13\\ninput y[0]\\n1 input y[0]\\n2 y[0]\\n1 OR y[0]\\n2 (output y[1]\\n1 )\\n0 0 0\\n1 0 1\\n0 1 1\\n1 1 1\\nTable 1.Truth table for the logical \"OR\" function.\\nWith Heaviside activation function, we have\\ny[1]\\n1 =\\n{\\n1, if W1y[0]\\n1 + W2y[0]\\n2 + b≥0,\\n0, else .\\nThe goal is now to chooseW1,W2,b so that we match the output from the truth table for given\\ninput. Obviously,W1 = W2 = 1 and b= −1 is a possible choice that fulﬁlls the task. Similarly,\\none can ﬁnd values forW1,W2 and b to model the logical \"AND\" function.\\nNext, let us consider the logical \"XOR\" function with the following truth table.\\ninput y[0]\\n1 input y[0]\\n2 y[0]\\n1 XOR y[0]\\n2 (output y[1]\\n1 )\\n0 0 0\\n1 0 1\\n0 1 1\\n1 1 0\\nTable 2.Truth table for the logical \"XOR\" function.\\nIn fact, the logical \"XOR\" function can not be represented by the given shallow Neural Network,\\nsince the data is not linearly separable, see e.g. [15, Section 6.1]. This motivates the introduction\\nof additional layers in between the input and output layer, i.e. we choose a more complex function\\nFin the learning problem (P).\\nFigure 9.This illustration shows that the logical \"OR\" function is linearly separable, while the\\nlogical \"XOR\" function is not. Image modiﬁed from:https://dev.to/jbahire/demystifying-\\nthe-xor-problem-1blk . 2 FEEDFORWARD NEURAL NETWORK 14\\n2. Feedforward Neural Network\\nIntroducing hidden layers, i.e. layers between the input and output layer, leads toFeedfor-\\nward Neural Networks (FNNs), also called multilayer perceptrons (MLPs), cf. [15,\\nSection 6]. Essentially, they are multiple perceptrons organized in layers,ℓ = 0,...,L , where\\nevery perceptron takes the output from the previous perceptron as input. The number of layers\\nL is called thedepth of the network, while the number of neurons per layernℓ is thewidth of\\nthe network. The input layer is denoted withy[0] = u∈Rn0 and not counted in the depth of the\\nnetwork. A FNN is calleddeep if it has at least two hidden layers. We now indicate the weights\\nfrom layerℓto ℓ+ 1 by W[ℓ] ∈Rnℓ+1×nℓ and the bias vector byb[ℓ] ∈Rnℓ+1 for ℓ= 0,...,L −1.\\nTo simplify notation, we extend the activation functionσ : R →R to vector valued inputs, by\\napplying it component-wise, so thatσ : Rn →Rn, with (y1,...,y n)⊤ ↦→(σ(y1),...,σ (yn))⊤.\\nThe FNN layers can be represented in the same way as perceptrons\\ny[ℓ] = fℓ(y[ℓ−1]) = σ[ℓ](W[ℓ−1]y[ℓ−1] + b[ℓ−1]) for ℓ= 1,...,L, (2)\\nwhere the activation functionσ[ℓ] may diﬀer from layer to layer. We cally[ℓ] the feature vector\\nof layerℓ. Compactly, we can write a FNN as a composition of its layer functions, cf. [1, 2]\\ny[L] = F(u) = fL ◦fL−2 ◦... ◦f1(u).\\nThis formulation reinforces the choice of nonlinear activation functionσ, cf. Remark 1.1. Oth-\\nerwise, the outputy[L] is linearly dependent on the inputuand hidden layers can be eliminated.\\nHence, with linear activation function, solving rather simple tasks like modeling the logical\\n\"XOR\" function will not be possible. However, sometimes it can be favorable to have one linear\\nlayer.\\nRemark 2.1 In practice, it is not uncommon that the last layer of a FNN is indeed linear,\\ni.e. y[L] = W[L−1]y[L−1]. As long as the previous layers are nonlinear this does not hinder the\\nexpressiveness of the FNN, and is typically used to attain a desired output dimension. In essence,\\nW[L−1] can be seen as a reformatting, in this case.\\nHowever, in the remainder of this section we will consider the FNN architecture as introduced\\nin (2). Let us now try again to represent the \"XOR\" logical function, this time by a FNN with\\none hidden layer.\\ny[0]\\n1\\ny[0]\\n2\\ny[1]\\n1\\ny[1]\\n2\\ny[2]\\n1\\ninput\\nlayer\\nhidden layer output\\nlayer\\nFigure 10. A feedforward network with a two\\ndimensional input and one hidden layer with 2\\nnodes, i.e. n0 = n1 = 2,n2 = 1 and L= 2.\\nThe variable choices\\nW[0] =\\n(\\n1 1\\n−1 −1\\n)\\n, b [0] =\\n(\\n−1\\n1\\n)\\n, W [1] =\\n(1 1 )\\n, b [1] = −2,\\nsolve the task and lead to the following truth table. 2 FEEDFORWARD NEURAL NETWORK 15\\ninput y[0]\\n1 input y[0]\\n2 y[1]\\n1 y[1]\\n2 y[0]\\n1 XOR y[0]\\n2 (output y[2]\\n1 )\\n0 0 0 1 0\\n1 0 1 1 1\\n0 1 1 1 1\\n1 1 1 0 0\\nTable 3.Truth table for the logical \"XOR\" function modeled by the FNN from Figure 10 and\\ngiven variable choices as above.\\nNext, we formulate an optimization problem similar to (P) for multiple layers with the above\\nintroduced notation, cf. [1, 2, 3]\\nmin\\n{W[ℓ]}ℓ,{b[ℓ]}ℓ\\nL\\n(\\n{y[L](i)}i,{u(i)}i,{W[ℓ]}ℓ,{b[ℓ]}ℓ\\n)\\n(Pℓ)\\ns.t. y[L](i) = F\\n(\\nu(i),{W[ℓ]}ℓ,{b[ℓ]}ℓ\\n)\\n.\\nIf we collect again all variables in one vectorθ this will have the following length:\\nn0 ·n1 + ... + nL−1 ·nL\\ued19 \\ued18\\ued17 \\ued1a\\nweights\\n+ n1 + ... + nL.\\ued19 \\ued18\\ued17 \\ued1a\\nbiases\\ny[0]\\n1\\ny[0]\\n2\\ny[0]\\n3\\ny[1]\\n1\\ny[1]\\n2\\ny[1]\\n3\\ny[1]\\n4\\ny[1]\\n5\\ny[2]\\n1\\ny[2]\\n2\\ny[2]\\n3\\ny[2]\\n4\\ny[2]\\n5\\ny[3]\\n1\\ny[3]\\n2\\ny[3]\\n3\\ny[3]\\n4\\ny[3]\\n5\\ny[4]\\n1\\ninput\\nlayer\\nhidden layers\\noutput\\nlayer\\nFigure 11. A feedforward network with 3 hidden layers, layer widthsn0 = 3,n1 = n2 = n3 =\\n5,n4 = 1 and depthL= 4. Collecting all variables of this network in a vector will giveθ∈R86.\\nThe question that immediately arises is: How to choose the network architecture, i.e. depth and\\nwidth of the FNN? The following discussion is based on [15, Section 6.4]. 2 FEEDFORWARD NEURAL NETWORK 16\\n2.1. Depth and Width\\nThe universal approximation theorem, see e.g. [10], states that any vector-valued, multivariate,\\nmeasurable (in particular, continuous) function f : Rn0 → RnL can be approximated with\\narbitrary small error by a Neural Network with one hidden layer. Hence, a ﬁrst approach may\\nbe to choose a network with depthL = 2 and increase the width until the desired accuracy is\\nreached.\\nHowever, this poses certain problems. First of all the universal approximation theorem does\\nnot imply that a training algorithm will actually reach the desired approximation, but rather\\nthat some set of parameters exists, that satisﬁes the requirement. The training algorithm might\\nfor example only ﬁnd a local minimum or choose the wrong function as a result of overﬁtting.\\nAnother problem may be the sheer size of the layer required to achieve the wanted accuracy. In\\nthe worst case the network will need an exponential number of hidden units, with one hidden\\nunit for each possible combination of inputs. Thus in practice, one is discouraged to use only\\none hidden layer, but instead to use deep networks.\\nVarious families of functions are eﬃciently approximated by deep networks with a smaller width.\\nIf one desires to approximate the same function to the same degree of accuracy, the number\\nof hidden units typically grows exponentially. This stems from the fact, that each new layer\\nallows the network to make exponentially more connections, thus allowing a wider output of\\ntarget functions. Another reason why one might choose deeper networks is due to the intuition,\\nthat our desired function may well be a composition of multiple functions. Each new layer adds\\na nonlinear layer function to our network, thus making it easier for the FNN to approximate\\ncomposite functions. Also, heuristically we observe that deep networks typically outperform\\nshallow networks.\\nNonetheless, one main problem with deep FNNs is, that the gradient used for training is the\\nproduct of the partial derivatives of each layer, as we will see in Section 2.5. If these derivatives\\nhave small values, then the gradient for earlier layers can become very small. Thus training has\\na smaller if not even a negligible eﬀect on the ﬁrst layers when the network is too deep. This is\\nespecially a problem for sigmoid activation function, since its derivative is bounded by1\\n4 .\\nAs discussed in Section 1.5, choosing hyperparameters like the depth and width is a non-trivial\\nundertaking and currently, the method of choice is experimenting to ﬁnd a suitable conﬁguration\\nfor the given task.\\nAdditionally, in the optimization algorithms, cf. Section 1.3, we need a starting pointθ0 for the\\nvariables. Hence, we discuss how to initialize the weights and biases in the FNN.\\n2.2. Initialization\\nRecall that due to the non-convex loss function, we can only expect convergence to stationary\\npoints, cf. Remark 1.4. Consequently, the choice of the initial pointθ0 can have great impact on\\nthe algorithm, since two diﬀerent initial points can lead to two diﬀerent results. An unsuitable\\ninitial point may even prevent convergence altogether. Similar to choosing hyperparameters, for\\nthe choice of initial points there exist several well-tested strategies, cf. [12, Section 4.2.2], but it\\nis still an active ﬁeld of research.\\nThenaiveapproachwouldbetoinitialize θ= 0 orwithsomeotherconstantvalue. Unfortunately,\\nthis strategy has major disadvantages. With this initialization, all weights per layer in the Neural\\nNetwork have the same inﬂuence on the loss function and will therefore have the same gradient.\\nThis leads to all those neurons evolving symmetrically throughout training, so that diﬀerent 2 FEEDFORWARD NEURAL NETWORK 17\\nneurons will not learn diﬀerent things, which signiﬁcantly reduces the expressiveness of the\\nFNN. Let us remark that it is ﬁne to initialize the biasesb[ℓ] with zero, as long as the weights\\nW[ℓ] are not initialized constant. Hence, it is suﬃcient to discuss initialization strategies for the\\nweights.\\nWe know now that the weights should be initialized in a way that they diﬀer from each other to\\nensure symmetry breaking, i.e. preventing the neurons from evolving identically. One way to\\nachieve this israndom initialization. However, immediately the next question arises: How to\\ngenerate those random values?\\nFor example, weights can be drawn from a Gaussian distribution with mean zero and some ﬁxed\\nstandard deviation. Choosing a small standard deviation, e.g. 0.01, may cause a problem known\\nas vanishing gradients for deep networks, since the small neuron values will be multiplied with\\neach other in the computation of gradients due to the chain rule, leading to exponentially de-\\ncaying products. As a result learning can be very slow or even diverge. On the other hand,\\nchoosing a large standard deviation, e.g. 0.2, can result in exploding gradients, which is essen-\\ntially the opposite problem, where the products grow exponentially and learning can become\\nunstable, oscillate or even produce \"NaN\" values for the variables. Furthermore, in combination\\nwith saturating activation functions like sigmoid ortanh exploding variable values can lead to\\nsaturation of the activation function, which then leads to vanishing gradients and again hinder\\nlearning, cf. Figure 3.\\nTo ﬁnd a good intermediate value for the standard deviation,Xavier initializationhas been\\nproposed in [14], where the standard deviation value is chosen depending on the input size of the\\nlayer, i.e.\\n1√nℓ\\nfor W[ℓ],ℓ = 0,...,L −1. Note that since input sizes can vary, the Gaussian distribution that\\nthe initial values are drawn from, will also vary. This choice of weights in combination with\\nb[ℓ] = 0 leads to Var(y[ℓ+1]) = Var(y[ℓ]). However, the Xavier initialization assumes zero centered\\nactivation functions, which is not fulﬁlled for sigmoid and all variants of ReLU. As a remedy, the\\nHe initializationhas been proposed in [18], tailored especially to ReLU activation functions.\\nHere, the standard deviation is also chosen depending on the input size of the layer, namely\\n√\\n2\\nnℓ\\n.\\nAdditionally, there also exists an approach to normalize feature vectors throughout the network.\\n2.3. Batch Normalization\\nAssume that we are employing an optimization algorithm, which passes through the data points\\nin batches of size b and that the nodes in hidden layers follow a normal distribution. Then\\nbatch normalization[22] aims at normalizing the feature vectors in a hidden layer over the\\ngiven batch, to stabilize training, especially for unbounded activation functions, such as ReLU.\\nIt can be seen as insertion of an additional layer in a deep neural network, and this layer type is\\nalready pre-implemented in learning frameworks like tensorﬂow and pytorch:\\n• tf.keras.layers.BatchNormalization,\\n• torch.nn.BatchNorm1d (also 2d and 3d available). 2 FEEDFORWARD NEURAL NETWORK 18\\nSay, we have given the current feature vectorsy[ℓ]\\nj ∈Rnℓ of hidden layerℓ for all elements in\\nthe batch, i.e. j = 1,...,b . The batch normalzation technique ﬁrst determines the mean and\\nvariance\\nµ[ℓ] = 1\\nb\\nb∑\\nj=1\\ny[ℓ]\\nj , (σ2)[ℓ] = 1\\nb\\nb∑\\nj=1\\n\\ued79\\ued79\\ued79y[ℓ]\\nj −µ[ℓ]\\n\\ued79\\ued79\\ued79\\n2\\n.\\nSubsequently, the current feature vectorsy[ℓ]\\nj ,j = 1,...,b are normalized via\\nˆy[ℓ]\\nj =\\ny[ℓ]\\nj −µ[ℓ]\\n√\\n(σ2)[ℓ] + ϵ\\n,\\nwhere ϵ ∈R is a constant that helps with numerical stability. Finally, the output of the batch\\nnormalization layer is computed by\\ny[ℓ+1]\\nj = W[ℓ] ˆy[ℓ]\\nj + b[ℓ] ∀j = 1,...,b.\\nAs usual, the weightW[ℓ] ∈Rnℓ+1×nℓ and biasb[ℓ] ∈Rnℓ+1 are variables of the neural network\\nand will be learned.\\nFigure 12. Illustration of Batch Normalization applied to a hidden layer with 3 nodes\\nand batch size b. We assume that every node can be modeled by a normal distribution.\\nImage Source: https://towardsdatascience.com/batch-normalization-in-3-levels-of-\\nunderstanding-14c2da90a338.\\nRemark 2.2 The BN layer is a trainable layer, since it contains learnable variables.\\nLet us now consider an example task that is commonly solved with FNNs.\\n2.4. Classiﬁcation Tasks\\nAs mentioned in 1.1, classiﬁcation is a supervised learning task with labelsS(u) ∈N. First, we\\nconsider binary classiﬁcation, cf. e.g. [12, Section 2.1], where we classify the data into two 2 FEEDFORWARD NEURAL NETWORK 19\\ncategories, i.e. a spam ﬁlter that determines whether an email is spam or not. We could construct\\nour network so that it indicates the category which it concludes is the most likely. However, it\\ncan be useful to know the probability assigned to the output, so that we know how certain the\\ndecision is. Thus, we aim to have outputs between 0 and 1, so that they sum up to 1. In the\\ncase of binary classiﬁcation, the second output is directly determined by the ﬁrst. Consequently,\\nit suﬃces to have a one dimensional output layery[L] ∈[0,1], which should predict\\nP(y[L] = 1 |u,θ),\\ni.e. the probability of the output being category 1 (e.g. spam) given the inputuand variablesθ.\\nAssume that we have already set up a feedforward network up to the second to last layery[L−1] ∈\\nR. It remains to choose the activation functionσ[L−1] that enters the computation ofy[L] and\\nto model the loss functionL.\\nSince we wanty[L] ∈[0,1], a common approach is to use\\nthe sigmoid activation function.\\nσ(y) = 1\\n1 + exp(−y) = exp(y)\\nexp(y) + 1 ∈(0,1).\\n−4 −2 0 2 4\\n0\\n0.5\\n1\\nLet us remark that the casesy[L] ∈{0,1}are not possible with this choice of activation function,\\nbut we are only computing approximations anyhow.\\nNext, we construct a loss function. To this end, we assume that the training data is a sample\\nof the actual relationship we are trying to train, thus it obeys a probability function, which we\\nwant to recover. The main idea for the loss function is to maximize the likelihood of the input\\nparameters, i.e. if the probabilityP(y[L] = S(u) |u,θ) of generating the known supervisionS(u)\\nis high for the input datau, the loss should be small, and vice versa. To model the probability\\nwe choose the Bernoulli distribution, which models binary classiﬁcation:\\nP\\n(\\ny[L] = S(u) |u,θ\\n)\\n= (y[L])S(u)(1 −y[L])(1−S(u)).\\nTo achieve small loss for large probabilities, we apply the logarithm and then maximize this\\nfunction, so that the optimal network variables¯θ can be determined as follows\\n¯θ= argmax\\nθ\\nN∑\\ni=1\\nlog\\n(\\nP\\n(\\ny[L](i) = S(u(i)) |u(i),θ\\n))\\n= argmax\\nθ\\nN∑\\ni=1\\nlog\\n(\\n(y[L](i))S(u(i))(1 −y[L](i))(1−S(u(i)))\\n)\\n= argmax\\nθ\\nN∑\\ni=1\\nS(u(i)) ·log(y[L](i)) + (1−S(u(i))) ·log(1 −y[L](i))\\n= argmin\\nθ\\nN∑\\ni=1\\n−S(u(i)) ·log(y[L](i)) −(1 −S(u(i))) ·log(1 −y[L](i))\\ued19 \\ued18\\ued17 \\ued1a\\n=:L(y[L](i),S(u(i))),Binary Cross Entropy Loss\\n,\\nwhere y[L](i) is a function of the network variablesθ. 2 FEEDFORWARD NEURAL NETWORK 20\\nIn practice, we minimize the cross-entropy, since it is equivalent to maximizing the likelihood,\\nbut stays within our given frame of minimization problems. Let us assume that our data either\\nhas the labelS(u) = 1 (spam) orS(u) = 0 (not spam), then the binary cross entropy loss is\\nL\\n(\\ny[L],S(u)\\n)\\n=\\n{\\n−log(y[L]), if S(u) = 1,\\n−log(1 −y[L]), if S(u) = 0.\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nS(u) = 1\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.5\\n1.0\\n1.5\\n2.0\\n2.5\\n3.0\\n3.5\\n4.0\\nS(u) = 0\\nFigure 13. Binary Cross Entropy Loss for labelS(u) = 1 (left) and labelS(u) = 0 (right).\\nWe see in Figure 13 that the cross entropy loss forS(u) = 1 goes to zero, fory[L] ↗1, and grows\\nfor y[L] ↘0, as desired. On the other hand, the cross entropy loss forS(u) = 0 goes to zero for\\ny[L] ↘0 and grows fory[L] ↗1.\\nAltogether, we have the following loss function for the binary classiﬁcation task\\nL(θ) = 1\\nN\\nN∑\\ni=1\\nL\\n(\\ny[L](i)(θ),S(u(i))\\n)\\n.\\nMulticlass classiﬁcation is a direct extension of binary classiﬁcation. Here, the goal is to\\nclassify the data into multiple (at least 3) categories. A prominent example is the MNIST data\\nset, where black and white pictures of handwritten digits (of size28 ×28 pixels) are supposed\\nto be classiﬁed as {0,1,2,3,4,5,6,7,8,9}, cf. Figure 14. A possible network architecture is\\nillustrated in Figure 15.\\nFigure 14. Example of the MNIST database. Sample belonging to the digit 7 (left) and 100\\nsamples from all 10 classes (right). Image Source: [4, Fig. 1]. 2 FEEDFORWARD NEURAL NETWORK 21\\ny[0]\\n1\\ny[0]\\n2\\ny[0]\\n3\\ny[0]\\n4\\ny[0]\\nn0\\n...\\ny[1]\\n1\\ny[1]\\n2\\ny[1]\\n3\\ny[1]\\nn1\\n...\\ny[2]\\n1\\ny[2]\\n2\\ny[2]\\nn2\\n...\\ny[3]\\n1\\ny[3]\\nn3\\n...\\ny[4]\\n1\\ny[4]\\nn4\\n...\\ninput\\nlayer\\nhidden\\nlayers\\noutput\\nlayer\\nFigure 15.A feedforward network with 3 hidden layers, i.e. depthL= 4. For the MNIST data\\nset we haven0 = 784 and n4 = 10.\\nFor this task, we need a generalization of the sigmoid activation function, which will takey[L−1] ∈\\nRn and map it toy[L] ∈[0,1]n, so that we have∑n\\ni=1 y[L]\\ni = 1, where nL−1 = nL = n is the\\nnumber of classes. A suitable option is the softmax function, which is given component-wise by\\nsoftmax(y)i = exp(yi)∑n\\nj=1 exp(yj) ∈(0,1), for i= 1,...,n.\\nKeep in mind that e.g. in the MNIST case we have labelsS(u) ∈{0,1,2,3,4,5,6,7,8,9}, and\\nin general we have multiple labelsS(u) ∈N. We have seen before that maximizing the log-\\nlikelihood is a suitable choice for classiﬁcation tasks. Since we want to formulate a minimization\\nproblem, we choose the negative log-likelihood as loss function\\nL(θ) = 1\\nN\\nN∑\\n1=1\\n−log\\n(\\nP\\n(\\ny[L](i) = S(u(i)) |u(i),θ\\n))\\n.\\nIn this section we have seen yet another model for the loss functionL, and from Section 1.3 we\\nknow that in any case we will need the gradient∇L(θ) to update our variablesθ. Let us discuss\\nhow frameworks like pytorch and tensorﬂow obtain this information.\\n2.5. Backpropagation\\nThe derivations are based on [15, Section 6.5] and [26, Section 7.3]. When a network, e.g. a\\nFNN, takes an inputu, passes it through its layers and ﬁnally computes an outputy[L], the\\nnetwork feeds forwardthe information, which this is calledforward propagation. Then a\\nloss is assigned to the output and we aim at employing the gradient of the loss function∇L(θ) to\\nupdatethenetworkvariables θ∈RK. InaFNNwehave K = n0·n1+... +nL−1·nL+n1+... +nL. 2 FEEDFORWARD NEURAL NETWORK 22\\nThe gradient is then given by\\n∇L(θ) =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed\\n∂L(θ)\\n∂θ1\\n...\\n∂L(θ)\\n∂θK\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8.\\nTo develop an intuition about the process, we discuss the following simple example.\\ny[0] y[1] y[2] y[3]input\\nlayer\\nhidden layers\\nL\\noutput layer\\nW[0] W[1] W[2]\\nFigure 16. A feedforward network with 2 hidden layers, one node per layer and depthL= 3.\\nExample 2.3 Consider a very simple FNN with one node per layer and assume that we only con-\\nsider weightsW[ℓ] ∈R and no biases. For the network in Figure 16 we haveθ= (W[0],W[1],W[2])⊤,\\ny[3] = σ[3](W[2]y[2]) = σ[3]\\n(\\nW[2]σ[2](W[1]y[1])\\n)\\n= σ[3]\\n(\\nW[2]σ[2]\\n(\\nW[1]σ[1](W[0]y[0])\\n))\\n,\\nand\\nL(θ) = L(y[3](θ)) = L\\n(\\nσ[3]\\n(\\nW[2]σ[2]\\n(\\nW[1]σ[1](W[0]y[0])\\n)))\\n.\\nComputing the components of the gradient, we employ the chain rule to obtain e.g.\\n∂L\\n∂W[0] = ∂L\\n∂y[3] · ∂y[3]\\n∂W[0]\\n= ∂L\\n∂y[3] ·∂y[3]\\n∂y[2] · ∂y[2]\\n∂W[0]\\n= ∂L\\n∂y[3] ·∂y[3]\\n∂y[2] ·∂y[2]\\n∂y[1] · ∂y[1]\\n∂W[0] ,\\nand in general for depthL we get\\n∂L\\n∂W[ℓ] = ∂L\\n∂y[L] ·\\nℓ+2∏\\nj=L\\n∂y[j]\\n∂y[j−1] ·∂y[ℓ+1]\\n∂W[ℓ] .\\nEssentially, to calculate the eﬀect of a variable on the loss function we iterate backwards through\\nthe network, multiplying the derivatives of each layer. This is calledback propagation, often\\nabbreviated asbackprop.\\nTo obtain a computationally eﬃcient version of back propagation, we exploit the fact that parts\\nof the derivatives can be recycled, broadly speaking. E.g.∂L\\n∂y[L] is a part of all derivatives. So,\\nif we compute the derivative byW[L−1] ﬁrst, we already have this component available and can\\nreuse it in the computation of the derivative byW[L−2], etc.\\nIn order to formalize the eﬀective computation of derivatives in a backpropagation algorithm,\\nwe decompose the forward propagation into two parts, cf. e.g. [26, Section 7.3.2].\\nz[ℓ] = W[ℓ−1]y[ℓ−1] + b[ℓ−1] ∈Rnℓ,\\ny[ℓ] = σℓ](z[ℓ]) ∈Rnℓ. 2 FEEDFORWARD NEURAL NETWORK 23\\nThis was not necessary in Example 2.3, because we only consider weights and no biases. Fur-\\nthermore, we assume that the loss functionL takes the ﬁnal outputy[L] as an input. Especially,\\nno other feature vectorsy[ℓ] for ℓ ̸= L enter the loss function directly. This is the case e.g. for\\nmean squared error, cf. Example 1.2, and cross entropy, cf. Section 2.4.\\nIn general, we now have by chain rule for allℓ= 0,...,L −1\\n∂L\\n∂W[ℓ] = ∂L\\n∂y[L] ·\\nℓ+2∏\\nj=L\\n(∂y[j]\\n∂z[j] · ∂z[j]\\n∂y[j−1]\\n)\\n·∂y[ℓ+1]\\n∂z[ℓ+1] ·∂z[ℓ+1]\\n∂W[ℓ] , (3)\\n∂L\\n∂b[ℓ] = ∂L\\n∂y[L] ·\\nℓ+2∏\\nj=L\\n(∂y[j]\\n∂z[j] · ∂z[j]\\n∂y[j−1]\\n)\\n·∂y[ℓ+1]\\n∂z[ℓ+1] ·∂z[ℓ+1]\\n∂b[ℓ] . (4)\\nHowever, we have to understand these derivatives in detail. First of all, let us introduce the\\nfollowing deﬁnition from [21].\\nDeﬁnition 2.4 Let A,B ∈Rm×n be given matrices, thenA⊙B ∈Rm×n, with entries\\n(A⊙B)i,j := (A)i,j ·(B)i,j, for i= 1,...,m, j = 1,...,n,\\nis called theHadamard productof A and B.\\nFurthermore, we deﬁne the derivative of the component-wise activation functionσ : Rm →Rm\\nas follows\\nσ′: Rm →Rm, z =\\n\\uf8eb\\n\\uf8ec\\uf8ed\\nz1\\n...\\nzm\\n\\uf8f6\\n\\uf8f7\\uf8f8↦→\\n\\uf8eb\\n\\uf8ec\\uf8ed\\nσ′(z1)\\n...\\nσ′(zm)\\n\\uf8f6\\n\\uf8f7\\uf8f8= σ′(z).\\nLet us introduce two special cases of multi-dimensional chain rule, cf. [26, p.98], which will prove\\nhelpful to calculate the derivatives.\\n1. Considera= σ(z) ∈Rm, whereσis a component-wise function, e.g. an activation function,\\nz∈Rm, andf = f(a) ∈R. Then, it holds\\n∂f\\n∂z = ∂f\\n∂a ⊙σ′(z) ∈Rm. (5)\\n2. Consider z = Wy + b ∈Rm and f = f(z) ∈R, with W ∈Rm×n and y ∈Rn. Then, it\\nholds\\n∂f\\n∂y = W⊤·∂f\\n∂z ∈Rn, (6)\\n∂f\\n∂W = ∂f\\n∂z ·y⊤ ∈Rm×n, (7)\\n∂f\\n∂b = ∂f\\n∂z ∈Rm. (8)\\nWe can now start working our way backwards through the network to get all derivatives. Assume\\nthat we know ∂L\\n∂y[L] ∈RnL, which will depend in detail on the choice of loss function. We can\\nemploy (5) withf = L,z = z[L],a = y[L] to compute\\n¯z[L] := ∂L\\n∂z[L] = ∂L\\n∂y[L] ⊙(σ[L])′(z[L]) ∈RnL. 2 FEEDFORWARD NEURAL NETWORK 24\\nHere, we employ a typical notation from automatic diﬀerentiation (AD), i.e. the gradient of the\\nloss with respect to a certain variable is denoted by the name of that variable with an overbar.\\nNow, we know\\n¯y[L−1] := ∂L\\n∂y[L−1] = ¯z[L] · ∂z[L]\\n∂y[L−1] ∈RnL−1 ,\\ni.e. we can reuse the previously derived gradient. Furthermore, from (6) with f = L, y =\\ny[L−1],W = W[L−1],z = z[L] we deduce\\n¯y[L−1] = (W[L−1])⊤¯z[L].\\nSubsequently, we use¯y[L−1] to compute ¯z[L−1], and so forth. In this way we can keep iterating\\nto build up the products in (3) and (4).\\nIn every layer,ℓ = 0,...,L −1, we also want to determine¯W[ℓ] := ∂L\\n∂W[ℓ] and ¯b[ℓ] := ∂L\\n∂b[ℓ] . We\\nshow this exemplary forℓ= L−1. It holds\\n¯W[L−1] = ¯z[L] · ∂z[L]\\n∂W[L−1] ∈RnL×nL−1 ,\\n¯b[L−1] = ¯z[L] · ∂z[L]\\n∂b[L−1] ∈RnL.\\nMaking use of (7) and (8) with the same choices as in the computation of¯y[L−1] and b= b[L−1],\\nwe get\\n¯W[L−1] = ¯z[L](y[L−1])⊤,\\n¯b[L−1] = ¯z[L].\\nWith this technique, we have an iterative way to eﬃciently calculate all gradients needed for the\\nvariable update in the optimization method, cf. Section 1.3.\\nRemark 2.5\\n(i) It is even more elegant and eﬃcient to updateW[ℓ] and b[ℓ] during backpropagation, i.e.\\non the ﬂy. This way we do not need to store the gradient and can overwrite the weight\\nand bias variables. It is only necessary to save the current gradients for the next loop, so\\nwe could rewrite the backpropagation algorithm with temporary gradient values. This only\\nworks if the stepsize / learning rateτ is previously known and ﬁxed for all variables, since\\nfor a line search we would need to know the full gradient and could only update the variables\\nafterwards.\\n(ii) Considering we haveN training data points, the backpropagation algorithm has to take all\\nof them into account. When the loss is a sum of loss functions for each data point, this\\ncan be easily incorporated into the algorithm by looping overi= 1,...,N and introducing\\na sum where necessary.\\nAltogether, we formulate Algorithm 6, which collects the gradients with respect to the weights\\nand biases in one ﬁnal gradient vector∇L(θ).\\nIn frameworks like pytorch and tensorﬂow, backpropagation is already implemented, e.g. in\\npytorch the function \"autograd\" handles the backward pass. Broadly speaking, autograd collects 2 FEEDFORWARD NEURAL NETWORK 25\\nAlgorithm 6Backpropagation.\\nRequire: Training data set{u(i),S(u(i))}N\\ni=1.\\nRequire: Current weightsW[ℓ] and biasesb[ℓ] for ℓ= 0,...,L −1.\\nRequire: Activation functionsσ[ℓ] for ℓ= 1,...,L .\\nRequire: Loss functionL(y[L]) and its gradient∇L(y[L]) = ∂L\\n∂y[L] ∈RnL.\\ny[0](i) = u(i) ∈Rn0 for i= 1,...,N .\\nfor ℓ= 1,...,L do\\nz[ℓ](i) = W[ℓ−1]y[ℓ−1] + b[ℓ−1] ∈Rnℓ for i= 1,...,N ,\\ny[ℓ](i) = σ[ℓ](z[ℓ](i)) ∈Rnℓ for i= 1,...,N .\\nend for\\nCompute lossL = 1\\nN\\n∑N\\n1=1 L(y[L](i)) ∈R.\\n¯y[L](i) = 1\\nN ·∇L(y[L](i)) ∈RnL for i= 1,...,N .\\nfor ℓ= L,L −1,..., 1 do\\n¯z[ℓ](i) = ¯y[ℓ](i) ⊙(σ[ℓ])′(z[ℓ](i)) ∈Rnℓ for i= 1,...,N ,\\n¯y[ℓ−1](i) = (W[ℓ−1])⊤¯z[ℓ](i) ∈Rnℓ−1 for i= 1,...,N ,\\n¯W[ℓ−1] = ∑N\\n1=1 ¯z[ℓ](i)(y[ℓ−1](i))⊤∈Rnℓ×nℓ−1 ,\\n¯b[ℓ−1] = ∑N\\n1=1 ¯z[ℓ](i) ∈Rnℓ.\\nend for\\nthe data and all executed operations in a directed acyclic graph. In this graph the inputs are\\nthe leaves, while the outputs are the roots. Now to automatically compute the gradients, the\\ngraph can be traced from roots to leaves, employing the chain rule. This coincides with the\\ncomputations that we just derived by hand. For more details we refer tohttps://pytorch.org/\\ntutorials/beginner/blitz/autograd_tutorial.html. 3 CONVOLUTIONAL NEURAL NETWORK 26\\n3. Convolutional Neural Network\\nIn this section, based on [9],[15, Section 9], we consider Neural Networks with a diﬀerent archi-\\ntecture: convolutional neural networks(CNNs or ConvNets). They were ﬁrst introduced by\\nKunihiko Fukushima in 1980 under the name \"neocognitron\" [11]. Famous examples of convo-\\nlutional neural networks today are \"LeNet\" [25], see Figure 17 and \"AlexNet\" [24].\\nAs a motivation, consider a classiﬁcation task where the input is an image of sizen0,1 ×n0,2\\npixels. We want to train a Neural Network so that it can decide, e.g. which digit is written in\\nthe image (MNIST data set). We have seen in Figure 15 that the image withn0,1 = n0,2 = 28\\nhas been reshaped (vectorized, ﬂattened) into a vector inRn0,1·n0,2 = R784, so that we can use\\nit as an input for a regular FNN. However, this approach has several disadvantages:\\n1. Vectorization causes the input image to loose all of its spatial structure, which could have\\nbeen helpful during training.\\n2. Let e.g. n0,1 = n0,2 = 1000, thenn0 = 106 and the weight matrixW[0] ∈Rn1×106\\ncontains\\nan enormous number of optimization variables. This can make training very slow or even\\ninfeasible.\\nOn the contrary, convolutional neural networks are designed to exploit the relationships between\\nneighboring pixels. In fact, the input of a CNN is typically a matrix or even a three-dimensional\\ntensor, which is then passed through the layers while maintaining this structure. CNNs take\\nsmall patches, e.g. squares or cubes, from the input images and learn features from them.\\nConsequently, they can subsequently recognize these features in other images, even when they\\nappear in other parts of the image.\\nFigure 17. Architecture of LeNet-5. 3 CONVOLUTIONAL NEURAL NETWORK 27\\nIn Figure 17 we see the architecture of \"LeNet-5\". The inputs are images, where we have 1\\nchannel, because we consider grayscale images. At ﬁrst we have two sequences of convolution\\nlayer (yellow), Section 3.2, detector layer (violet), Section 3.3, and pooling layer (orange), Section\\n3.4. These layers retain the multidimensional structure of the input. Since this network is built\\nforaclassiﬁcationtasks, theoutputshouldbeavectorof10. Consequently, themulti-dimensional\\noutput of a hidden layer is ﬂattened, i.e. vectorized, and the remaining layers are fully connected\\nlayers (bright yellow) as we have seen in FNNs.\\nIn other, larger architectures, like AlexNet, cf. Figure 24, to avoid overﬁtting with large fully\\nconnected layers, a technique calleddropout is applied. The key idea is to randomly drop units\\nwith a given probability and their connections from the neural network during training, for more\\ndetails we refer to [28].\\nRemark 3.1\\n(i) We will view convolution, detector and pooling layers as separate layers. However, it is also\\npossible to deﬁne a convolutional layer to consist of a convolution, detector and pooling\\nstage, cf. Figure 18. This can be a source of confusion when referring to convolutional\\nlayers, which we should be aware of.\\n(ii) Throughout the remainder of this section we omit layer indicesℓ to simplify notation, and\\nwe indicate the data with capital letterY to clarify that they are matrices or tensors.\\nFigure 18. Convolutional layer\\n(gray) consisting of stages (left) com-\\npared to viewing the operations as\\nseparate layers (right). We use the\\nterminology as depicted on the right\\nhand side, and refer to convolutional,\\ndetector and pooling layers as sepa-\\nrate layers.\\nBefore we move on to a detailed introduction of the diﬀerent layer types in CNNs, let us recall\\nthe mathematical concept of a convolution.\\n3.1. Convolution\\nAs explained in [15, Section 9.1], in general, convolution describes how one function inﬂuences\\nthe shape of another function. But it can also be used to apply a weight function to another\\nfunction, which is how convolution is used in convolutional neural networks.\\nDeﬁnition 3.2 Let f,g : Rn →R be two functions. If bothf and g are integrable with respect\\nto Lebesgue measure, we can deﬁne theconvolution as:\\nc(t) = (f ∗g)(t) =\\n∫\\nf(x)g(t−x) dx,\\nfor some t ∈Rn. Here, f is called the input and g is called the kernel. The new function\\nc: Rn →R is called the feature map. 3 CONVOLUTIONAL NEURAL NETWORK 28\\nHowever, for convolutional neural networks we need the discrete version.\\nDeﬁnition 3.3 Let f,g : Zn →R be two discrete functions. Thediscrete convolutionis then\\ndeﬁned as:\\nc(t) = (f ∗g)(t) =\\n∑\\nx∈Zn\\nf(x)g(t−x),\\nfor somet∈Zn.\\nA special case of the discrete convolution is settingf and g to n-dimensional vectors and using\\nthe indices as arguments. We illustrate this approach in the following example.\\nExample 3.4 Let X and Y be two random variable each describing the outcome of rolling a\\ndice. The probability mass functions are deﬁned as:\\nfX(t) = fY(t) =\\n{\\n1\\n6 , if t∈{1,2,3,4,5,6},\\n0, if t∈Z \\\\{1,2,3,4,5,6}.\\nWe aim at calculating the probability that the sum of both dice rolls equals nine. To this end,\\nwe take the vectors of all possible outcomes and arrange them into two rows. Here, we ﬂip the\\nsecond vector and slide it to the right, such that the numbers which add to nine align.\\n123456\\n1 2 3 4 5 6\\nNow, we replace the outcomes with their respective probabilities, multiply the adjacent components\\nand add up the results.\\n1\\n6\\n1\\n6\\n1\\n6\\n1\\n6\\n1\\n6\\n1\\n6\\n1\\n6\\n1\\n6\\n1\\n6\\n1\\n6\\n1\\n6\\n1\\n6\\nThis gives\\nfX+Y(9) = 1\\n36 + 1\\n36 + 1\\n36 + 1\\n36 = 1\\n9,\\ni.e. the probability that the sum of the dice equals nine is1\\n9 .\\nIn fact, all the steps we have just done are equivalent to calculating a discrete convolution:\\nfX+Y(9) =\\n6∑\\nx=1\\nfX(x)fY(9 −x) = (fX ∗fY)(9) 3 CONVOLUTIONAL NEURAL NETWORK 29\\n3.2. Convolutional Layer\\nFor the convolutional layers in CNNs we deﬁne convolutions for matrices, cf. e.g. [15, (9.4)].\\nThis can be extended to tensors straight forward.\\nDeﬁnition 3.5 Let Y ∈Rn1×n2 and K ∈Rm1×m2 be given matrices, such thatm1 ≤n1 and\\nm2 ≤n2. The convolution of Y and K is denoted byY ∗K with entries\\n[Y ∗K]i,j :=\\nm1∑\\nk=1\\nm2∑\\nl=1\\nKk,lYi+m1−k,j+m2−l,\\nfor 1 ≤i≤n1 −m1 + 1 and 1 ≤j ≤n2 −m2 + 1. Here, Y is called the input andK is called\\nthe kernel.\\nIn Machine Learning often the closely related concept of(cross) correlation, cf. e.g. [15,\\n(9.6)], is used, and incorrectly referred to as convolution, where\\n[Y ⊛ K]i,j :=\\nm1∑\\nk=1\\nm2∑\\nl=1\\nKk,lYi−1+k,j−1+l,\\nfor 1 ≤i≤n1 −m1 + 1 and 1 ≤j ≤n2 −m2 + 1. The (cross) correlation has the same eﬀect as\\nconvolution, if you ﬂip both, rows and columns of the kernelK, see the changed indices indicated\\nin red. Since we learn the kernel anyway, it is irrelevant whether the kernel is ﬂipped, thus either\\nconcept can be used.\\nWe illustrate the matrix computations with an example.\\nExample 3.6 For this example we have the data matrix\\nY =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed\\n1 5 −2 0 2\\n3 8 7 1 0\\n−1 0 1 2 3\\n4 2 1 −1 2\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8,\\nand the kernel\\nK =\\n\\uf8eb\\n\\uf8ed\\n1 2 3\\n4 5 6\\n7 8 9\\n\\uf8f6\\n\\uf8f8.\\nThe computation of[Y ∗K]1,1 can be illustrated as follows\\n[Y ∗K]1,1 =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed\\n+1 ·9 +5 ·8 −2 ·7 0 2\\n+3 ·6 +8 ·5 +7 ·4 1 0\\n−1 ·3 +0 ·2 +1 ·1 2 3\\n4 2 1 −1 2\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8= 9 + 40−14 + 18 + 40 + 28−3 + 0 + 1 = 119.\\nThe gray values ofY are not used in the computation. Here, we see thatK is ﬂipped when used\\nin the convolution. This also clariﬁes, how the (cross) correlation can be more intuitive, where\\n[Y ⊛ K]1,1 =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed\\n+1 ·1 +5 ·2 −2 ·3 0 2\\n+3 ·4 +8 ·5 +7 ·6 1 0\\n−1 ·7 +0 ·8 +1 ·9 2 3\\n4 2 1 −1 2\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8= 1 + 10−6 + 12 + 40 + 42−7 + 0 + 9 = 101. 3 CONVOLUTIONAL NEURAL NETWORK 30\\nIn a similar way we can proceed to calculate the remaining values by shifting the kernel over the\\nmatrix\\n[Y ⊛ K]1,2 =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed\\n1 +5 ·1 −2 ·2 +0 ·3 2\\n3 +8 ·4 +7 ·5 +1 ·6 0\\n−1 +0 ·7 +1 ·8 +2 ·9 3\\n4 2 1 −1 2\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8= 5 −4 + 0 + 32 + 35 + 6 + 0 + 8 + 18 = 100.\\nAltogether, we get\\nY ∗K =\\n(119 120 53\\n155 155 102\\n)\\nand Y ⊛ K =\\n(101 100 87\\n95 55 58\\n)\\n.\\nEspecially, Y ∗K ̸= Y ⊛ K.\\nFigure 19.An image of size4×5 is divided in blocks of size3×3 by\\nmoving one pixel at a time either horizontally or vertically, as shown\\nexemplary in red and green. Here, the black square is denoted by\\nthe index(1,1), the red one by(1,2) and the green one by(2,1).\\nThe kernel size, which is typically square, e.g.m×m, is a hyperparameter of the CNN. Fur-\\nthermore, the convolutional layer has additional hyperparameters that need to be chosen. We\\nhave seen that a convolution with a m ×m kernel reduces the dimension from n1 ×n2 to\\nn1 −m+ 1 ×n2 −m+ 1. To retain the image dimension we can use(zero) padding, cf. [9],\\napplied to the inputY with p∈N0. Choosing p= 0 yields Y again, whereasp= 1 results in\\nˆY =\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ed\\n0 0 0 0 0 0 0\\n0 1 5 −2 0 2 0\\n0 3 8 7 1 0 0\\n0 −1 0 1 2 3 0\\n0 4 2 1 −1 2 0\\n0 0 0 0 0 0 0\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\n,\\nfor Y from Example 3.6. Consequently, the padded matrixˆY is of dimension(n1 +2p)×(n2 +2p).\\nTo retain the image dimension, we need to choosep so that\\n(n1 + 2p) −m+ 1 = n1,\\n(n2 + 2p) −m+ 1 = n2,\\ni.e. p= m−1\\n2 , which is possible for any oddm.\\nFurthermore, we can choose thestride s∈N, which indicates how far to move the kernel. For\\nexample, in Figure 19 the stride is chosen ass= 1, while the stride in Figure 20 iss= 2.\\nLet us remark that a strides> 1 reduces the output dimension of the convolution to\\n(n1 −m\\ns + 1\\n)\\n×\\n(n2 −m\\ns + 1\\n)\\n.\\nAltogether, we can describe the convolutional layer. It consists ofM ∈N ﬁlters with identical\\nhyperparameters: kernelsize m×m, paddingpandstride s, buteachofthemhasitsownlearnable 3 CONVOLUTIONAL NEURAL NETWORK 31\\nFigure 20. A visualization of the convolution of a7 ×7 images\\nwith a3 ×3 kernel and strides= 2.\\nkernel K. Consequently, the ﬁlters in this layer haveM ·m2 variables in total. Applying allM\\nﬁlters to an input matrixY ∈Rn1×n2 leads to an output of size\\n(n1 + 2p−m\\ns + 1\\n)\\n×\\n(n2 + 2p−m\\ns + 1\\n)\\n×M,\\nwhere the results for allM ﬁlters are stacked, cf. [9]. Typically, thedepth M is chosen as a\\npower of 2, and growing for deeper layers, while height and width are shrinking, cf. Figure 24.\\nObviously, the output is a tensor with three dimensions, hence the subsequent layers need to\\nprocess 3-tensor-valued data. In fact, for colored images already the original input of the net-\\nwork is a tensor. The (cross) correlation operation (and also the convolution operation) can be\\ngeneralized to this case in the following way.\\nAssume we have an input tensor of sizen1 ×n2 ×n3,then we choose a three dimensional kernel\\nof size\\nm×m×n3,\\ni.e. the depth coincides. No striding or padding is applied in the third dimension. Hence, the\\noutput is of dimension\\n(n1 + 2p−m\\ns + 1\\n)\\n×\\n(n2 + 2p−m\\ns + 1\\n)\\n×1,\\nwhich can be understood as a matrix by discarding the redundant third dimension, cf. Figure\\n21. Doing this forM ﬁlters, again leads to the output being a 3-tensor.\\nFigure 21.Illustration of a convolution on a tensor, speciﬁcally a colored image (with red, green,\\nblue color channels) with a three-dimensional kernel and its result, which is a matrix. Here, no\\npadding p = 0 and a single strides = 1 is employed. Image Source:https://datahacker.rs/\\nconvolution-rgb-image/. 3 CONVOLUTIONAL NEURAL NETWORK 32\\nRemark 3.7\\n(i) Convolutional layers have the advantage that they have less variables than fully connected\\nlayers applied to the ﬂattened image. For example consider a grayscale image of size28×28\\nas input in LeNet, cf. Figure 17. The ﬁrst convolution has 6 kernels with5×5 entries. Due\\nto padding withp= 2 the output is28 ×28 ×6, so the image size is retained. Additionally,\\nbefore applying a detector layer, we add a bias per channel, so 6 bias variables in this case.\\nIn total, we have to learn156 variables. Now, imagine this image is ﬂattened to a784 ×1\\nvector and fed into a FNN with fully connected layer, where we also want to retain the size,\\ni.e. the ﬁrst hidden layer has784 nodes. This results in a much larger number of variables:\\n784 ·784\\ued19 \\ued18\\ued17 \\ued1a\\nweight\\n+ 784\\ued19\\ued18\\ued17\\ued1a\\nbias\\n= 615440.\\n(ii) Directly related, a disadvantage of convolutional layers is that every output only sees a\\nsubset of all input neurons, cf. e.g. [15, Section 9.2]. We denote this set of seen inputs\\nby eﬀective receptive ﬁeldof the neuron. In an FNN with fully connected layers the\\neﬀective receptive ﬁeld of a neuron is the entire input. However, the receptive ﬁeld of a\\nneuron increases with depth of the network, as illustrated in Figure 22.\\ny[0]\\n1\\ny[0]\\n2\\ny[0]\\n3\\ny[0]\\n4\\ny[0]\\n5\\ny[1]\\n1\\ny[1]\\n2\\ny[1]\\n3\\ny[1]\\n4\\ny[1]\\n5\\ny[2]\\n1\\ny[2]\\n2\\ny[2]\\n3\\ny[2]\\n4\\ny[2]\\n5\\nFigure 22. Simpliﬁed CNN architecture with an input\\nlayery[0] and two subsequent convolutional layersy[1] and\\ny[2], each with a one dimensional kernel of sizem = 3 ,\\nstride s = 1 and zero paddingp = 1. The colored nodes\\nare the receptive ﬁeld of the neurony[2]\\n3 .\\n3.3. Detector Layer\\nIn standard CNN architecture after a convolutional layer, a detector layer is applied. This simply\\nmeans performing an activation function. To this end, we extend the activation functionσ: R →\\nR to matrix and tensor valued inputs by applying it component-wise, as we did for vectors before\\nin Section 2, e.g. for a 3-tensorY = {Yi,j,k}i,j,k with i = 1,...,n 1,j = 1,...,n 2,k = 1,...,n 3\\nwe get\\n(σ(Y))i,j,k = σ(Yi,j,k).\\n3.4. Pooling Layer\\nAfter the detector layer, typically a pooling layer (also called downsampling layer) is applied, cf.\\ne.g. [15, Section 9.3]. This layer type is responsible for reducing the ﬁrst two dimensions (height\\nand width) and usually does not interfere with the third dimension (depth) of the dataY, but\\nrather is applied for all channels independently. Consequently, the depth of the output coincides\\nwith the depth of the input and we omit the depth in our discussion. 3 CONVOLUTIONAL NEURAL NETWORK 33\\nAs in convolutional layers, pooling layers have a ﬁlter sizem×m, stride s and padding p.\\nHowever, almost alwaysp= 0 is chosen. The most popular values for for the ﬁlter size and stride\\nare m= s= 2. Again, with an input of sizen1 ×n2 the output dimension is\\n(n1 + 2p−m\\ns + 1\\n)\\n×\\n(n1 + 2p−m\\ns + 1\\n)\\nm=s=2,p=0\\n= n1\\n2 ×n2\\n2 .\\nOne common choice isMax Pooling(or Maximum Pooling), where the largest value is selected,\\ncf. e.g. [9]. Below we see an example of max pooling with a2 ×2 kernel, strides = 2 and no\\npadding.\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed\\n1 3 0 −7\\n−2 4 1 −1\\n0 1 8 −3\\n2 0 4 5\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8\\nmax\\n−→\\n( 4 1\\n2 8\\n)\\nAnother common choice isAverage Pooling, where we take the mean of all values. Below we\\nsee an example of average pooling (abbreviated: \"avg\") with a2 ×2 kernel,\\nK =\\n(0.25 0 .25\\n0.25 0 .25\\n)\\n,\\nstride s= 2 and no padding.\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed\\n1 3 0 −7\\n−2 4 1 −1\\n0 1 8 −3\\n2 0 4 5\\n\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8\\navg\\n−→\\n( 1.50 −1.75\\n0.75 3.50\\n)\\nThe eﬀect of average pooling applied to an image is easily visible: It blurs the image. In the\\nnew image every pixel is an average of a pixel and its neighboringm2 −1 pixels, see Figure 23.\\nDepending on the choice of strides and paddingp, the blurred image may also have less pixels.\\nFigure 23.Original image (left) and blurred image produced by average pooling (right) with a\\n5 ×5 kernel, strides= 1 and zero padding withp= 2. Image Source: Laurin Ernst. 3 CONVOLUTIONAL NEURAL NETWORK 34\\nRemark 3.8\\n(i) Pooling layers do not contain variables to learn.\\n(ii) We have seen that when using CNNs, we make the following assumptions:\\n(a) Pixels far away from each other do not need to interact with each other.\\n(b) Small translations are not relevant.\\nIf these assumptions do not hold, employing a CNN can result in underﬁtting.\\n3.5. Local Response Normalization\\nSimilar to batch normalization, Local Response Normalization (LRN) [24, Section 3.3] stabilizes\\ntrainingwithunboundedactivationfunctionslikeReLU.Thisstrategywasﬁrstintroducedwithin\\nthe\"AlexNet\"architecture[24], cf. Figure24, becausecontrarytopreviousCNNslike\"LeNet-5\",\\nwhich used sigmoid activation, \"AlexNet\" employs ReLU activation.\\nFigure 24. Architecture of AlexNet. ReLU activation is employed in the hidden layers. Image\\nSource: https://datahacker.rs/deep-learning-alexnet-architecture/ .\\nThe inter-channel LRN, as introduced in [24, Section 3.3], see also Figure 25 a), is given by\\nˆYi,j,k = Yi,j,k\\n(\\nκ+ γ\\nmin(M,k+ n\\n2 )∑\\nm=max(1,k−n\\n2 )\\n(Yi,j,m)2\\n)β.\\nHere, Yi,j,k and ˆYi,j,k denote the activity of the neuron before and after normalization, respec-\\ntively. The indices i,j,k indicate the height, width and depth ofY. We have i = 1 ,...,n 1,\\nj = 1,...,n 2 and k = 1,...,M , whereM is the number of ﬁlters in the previous convolutional\\nlayer. The values κ,γ,β,n ∈R are hyperparameters, where κ is used to avoid singularities,\\nand γ and β are called normalization and contrasting constants, respectively. Furthermore,n\\ndictates how many surrounding neurons are taken into consideration, see also Figure 25. In [24]\\nκ= 2,γ = 10−4,β = 0.75 were chosen. 3 CONVOLUTIONAL NEURAL NETWORK 35\\nFigure 25. Illustration of local response normalization. Inter-channel version a) as intro-\\nduced in [24, Section 3.3] and intra-channel version b). Both for n = 2 . For clariﬁca-\\ntion: The red pixel in the top left cube is Y1,1,1, while the red pixel in the top row, sec-\\nond from the left cube is Y1,1,2 and the red pixel in the bottom row, second from the left\\ncube isY1,2,1. Image source:https://towardsdatascience.com/difference-between-local-\\nresponse-normalization-and-batch-normalization-272308c034ac .\\nIn the case of intra-channel LRN, the neighborhood is extended within the same channel. This\\nleads to the following formula\\nˆYi,j,k = Yi,j,k\\n(\\nκ+ γ\\nmin(n1,i+ n\\n2 )∑\\np=max(1,i−n\\n2 )\\nmin(n2,j+ n\\n2 )∑\\nq=max(1,j−n\\n2 )\\n(Yp,q,k)2\\n)β.\\nRemark 3.9 The LRN layer is non-trainable, since it only contains hyperparameters and no\\nvariables. 4 RESNET 36\\n4. ResNet\\nWe have seen in Example 2.3 that for a FNN with depthL we have the derivative\\n∂L\\n∂W[ℓ] = ∂L\\n∂y[L] ·\\nℓ+2∏\\nj=L\\n∂y[j]\\n∂y[j−1] ·∂y[ℓ+1]\\n∂W[ℓ] .\\nIn the case that we consider a very deep network, i.e. largeL, the product in the derivative\\ncan be problematic, [5, 14], especially if we take derivatives with respect to variables from early\\nlayers. Two cases may occur:\\n1. If ∂y[j]\\n∂y[j−1] < 1 for all j, the product, and hence the whole derivative, tends to zero for\\ngrowing L. This problem is referred to asvanishing gradient.\\n2. On the other hand, if ∂y[j]\\n∂y[j−1] > 1 for all j, the product, and hence the whole derivative,\\ntends to inﬁnity for growingL. This problem is referred to asexploding gradient.\\nResidual Networks (ResNets)have been developed in [17, 19] with the intention to solve the\\nvanishing gradient problem. Employing the same notation as in FNNs, simpliﬁed ResNet layers\\ncan be represented in the following way\\ny[ℓ] = y[ℓ−1] + σ[ℓ](W[ℓ−1]y[ℓ−1] + b[ℓ−1]) for ℓ= 1,...,L, (9)\\nwith y[0] = u the input data. Essentially, a ResNet is a FNN with an addedskip connection,\\ni.e. +y[ℓ−1], cf. Figure 26\\ny[ℓ−1] W[ℓ−1]y[ℓ−1] + b[ℓ−1] activation σ + y[ℓ]\\nFigure 26. Illustration of a simpliﬁed ResNet layer.\\nRemark 4.1 The ResNet layers in the current form(9) only work, if all feature vectorsy[ℓ]\\nhave the same dimensionnℓ, so that we can add them up. To allow for diﬀerent layer sizes, we\\nneed to insert projection operatorsPℓ\\nℓ−1 ∈Rnℓ×nℓ−1 , cf. [2, Section 4], i.e.\\ny[ℓ] = Pℓ\\nℓ−1y[ℓ−1] + σ[ℓ](W[ℓ−1]y[ℓ−1] + b[ℓ−1]) for ℓ= 1,...,L,\\nWe now revisit the simple FNN with two hidden layers from Example 2.3, and add skip connec-\\ntions to make it a ResNet, see Figure 27.\\nExample 4.2 Consider a simple ResNet with one node per layer and assume that we only con-\\nsider weightsW[ℓ] ∈R and no biases. For the network in Figure 27 we haveθ= (W[0],W[1],W[2])⊤,\\nand\\nL(θ) = L(y[3](θ)).\\nWe deﬁne forℓ= 1,...,L\\na[ℓ] := σ[ℓ](W[ℓ−1]y[ℓ−1]), 4 RESNET 37\\ny[0] y[1] y[2] y[3]input\\nlayer\\nhidden layers\\nL\\noutput layer\\nW[0] W[1] W[2]\\nFigure 27. A ResNet with 2 hidden layers, one node per layer and depthL= 3.\\nso that in the ResNet setup\\ny[ℓ] = y[ℓ−1] + a[ℓ].\\nComputing the components of the gradient, we employ the chain rule to obtain e.g.\\n∂L\\n∂W[0] = ∂L\\n∂y[3] · ∂y[3]\\n∂W[0]\\n= ∂L\\n∂y[3] · ∂\\n∂W[0] (y[2] + a[3])\\n= ∂L\\n∂y[3] ·\\n( ∂y[2]\\n∂W[0] + ∂a[3]\\n∂y[2] · ∂y[2]\\n∂W[0]\\n)\\n= ∂L\\n∂y[3] ·\\n(\\nI + ∂a[3]\\n∂y[2]\\n)\\n· ∂y[2]\\n∂W[0]\\n= ∂L\\n∂y[3] ·\\n(\\nI + ∂a[3]\\n∂y[2]\\n)\\n·\\n(\\nI + ∂a[2]\\n∂y[1]\\n)\\n· ∂y[1]\\n∂W[0] ,\\nwhere I denotes the identity. In general for depthL, we get\\n∂L\\n∂W[ℓ] = ∂L\\n∂y[L] ·\\nℓ+2∏\\nj=L\\n(\\nI + ∂a[j]\\n∂y[j−1]\\n)\\n·∂y[ℓ+1]\\n∂W[ℓ] . (10)\\nIfwegeneralizethederivative(10)toResNetarchitectures, wherewedonotonlyconsiderweights\\nW[ℓ], see e.g. [2, Theorem 6.1], the structure of the product in the derivative remains the same,\\ni.e. it also contains an identity term.\\nRemember that for FNNs it holdsy[j] = a[j], i.e. the fraction in the product coincides in both\\ncases. However, due to the added identity, even if\\n∂a[j]\\n∂y[j−1] <1\\nholds for allj, we will not encounter vanishing gradients in the ResNet architecture. The ex-\\nploding gradients problem can still occur.\\nWe will see in Section 4.1 that there exist several versions of ResNets. However, from a mathe-\\nmatical point of view the simpliﬁed version (9) is especially interesting, because it can be related\\nto ordinary diﬀerential equations (ODEs), as ﬁrst done in [16]. Inserting a parameterτ[ℓ] ∈R in\\nfront of the activation functionσ and rearranging the terms of the forward propagation delivers\\ny[ℓ] = y[ℓ−1] + τ[ℓ]σ(W[ℓ−1]y[ℓ−1] + b[ℓ−1])\\n⇒ y[ℓ] −y[ℓ−1]\\nτ[ℓ] = σ(W[ℓ−1]y[ℓ−1] + b[ℓ−1]). 4 RESNET 38\\nHere, we consider the same activation functionσ for all layers. Now, the left hand side of the\\nequation can be interpreted as a ﬁnite diﬀerence representation of a time derivative, whereτ[ℓ] is\\nthe time step size andy[ℓ],y[ℓ−1] are the values attained at two neighboring points in time. This\\nrelation between ResNets and ODEs is also studied under the name of Neural ODEs, [7]. It is\\nalso possible to learn the time step sizeτ[ℓ] as an additional variable, [2].\\nLet us now introduce the diﬀerent ResNet versions from the original papers, [17, 19].\\n4.1. Diﬀerent ResNet Versions\\nIn contrast to the simpliﬁed ResNet layer version (9) that we introduced, original ResNet ar-\\nchitectures [17] consist ofresidual blocks, cf. Figure 28. Here, diﬀerent layers are grouped\\ntogether into one residual block and then residual blocks are stacked to form a ResNet.\\ny[ℓ−1] Weights BN ReLU Weights BN + ReLU y[ℓ]\\nFigure 28. Illustration of a residual block as introduced in [17].\\nIn the residual block, cf. Figure 28, we have the following layer types:\\n• Weights: fully connected or convolutional layer,\\n• BN: Batch Normalization layer, cf. Section 2.3,\\n• ReLU: activation functionσ= ReLU.\\nClearly, the residual block and the simpliﬁed ResNet layer both contain a skip connection, which\\nis the integral part of ResNets success, since it helps avoid the vanishing gradient problem.\\nHowever, the residual block is less easy to interpret from a mathematical point of view and can\\nnot directly be related to ODEs.\\nIn frameworks like Tensorﬂow and Pytorch, if you encounter a network architecture called\\n\"ResNet\", it will usually be built by stacking residual blocks of this original form, Figure 28.\\nSubsequently, in a follow up paper, [19], several other options to sort the occurring layers in a\\nresidual block have been introduced. The option which performed best in numerical tests (see\\nalso Figure 30) is illustrated in Figure 29.\\ny[ℓ−1] BN ReLU Weights BN ReLU Weights + y[ℓ]\\nFigure 29. Illustration of a full pre-activation residual block as proposed in [19, Fig.1(b)]. 4 RESNET 39\\nRemark 4.3 The authors of [19] call the residual block in Figure 29 thefull pre-activation\\nresidual block, since both activation functions are exercised before (pre) the skip connection.\\nMeanwhile, in the original residual block there is also a post-activation, i.e. an activation function\\nafter (post) the skip connection. In this sense, the simpliﬁed ResNet layer(9) can be termed a\\npre-activation ResNet layer.\\nA ResNet built with full pre-activation residual blocks can be found e.g. in Tensorﬂow under\\nthe name \"ResNetV2\". In the literature, there also exist other variants of the simpliﬁed ResNet\\nlayer, e.g. with a weight matrix applied outside the activation function.\\nIn Figure 30 we see a comparison of a 1001-layer ResNet built with original residual blocks and\\na 1001-layer ResNet built with full pre-activation (proposed) residual blocks. This result clearly\\ndemonstrates the advantage of full pre-activation for very deep networks, since both training loss\\nand test error can be improved with the proposed residual block.\\nFigure 30.Training loss (dashed line, left y-axis) and test error (solid line, right y-axis) plotted\\nagainst the iteration counter for a 1001-layer ResNet on the CIFAR-10 dataset. Here, the original\\nresidualblock(blue)iscomparedtotheproposedfullpre-activationresidualblock(green). Image\\nSource: [19, Fig.1]\\n4.2. ResNet18\\nAs an example for a ResNet architecture, we look at \"ResNet18\", cf. Figure 31. Here, 18\\nindicates the number of layers with learnable weights, i.e. convolutional and fully connected\\nlayers. Even though the batch normalization layers also contain learnable weights, they are\\ntypically not counted here. This is a ResNet architecture intended for use on image data sets,\\nhence the weights layers are convolutional layers, like in a CNN.\\nIn Block A the input data is pre-processed, while Block B to Block E are built of two residual\\nblocks each. To be certain which type of residual block the network is built of, it is recommended\\nto look into the details of the implementation. However, in most cases the original residual 4 RESNET 40\\nFigure 31.Illustration of \"ResNet18\" architecture, built of residual blocks. Image Source: [13].\\nblock, cf. Figure 28, is employed. Finally, as usual for a classiﬁcation task, the data is ﬂattened\\nand passed through a fully connected (FC) layer before the output is generated. Altogether,\\n\"ResNet18\" has over 10 million trainable parameters, i.e. variables.\\n4.3. Transfer Learning\\nAn advantage of having so many diﬀerent ResNet architectures available and also pre-trained\\nis, that they can be employed forTransfer Learning, see e.g. [29]. The main idea of transfer\\nlearning is to take a model that has been trained on a (potentially large) data set for the same\\ntype of task (e.g. image classiﬁcation), and then adjust the ﬁrst/last layer to ﬁt your data set.\\nDepending on your task you may need to change one or both layers, for example:\\n(i) Your input data has a diﬀerent structure: adapt the ﬁrst layer.\\n(ii) Your data set has a diﬀerent set of labels (supervisions): adapt the last layer.\\nIf your input data and labels both coincide with the original task then you don’t need to employ\\ntransfer learning. You can just use the pre-trained model for your task. When adapting a\\nlayer, this layers needs to be initialized. All remaining layers can be initialized with the pre-\\ntrained weights, which will most likely give a good starting point. Then you train the adapted\\nnetwork on your data, which will typically take a lot loss time than training with a random\\ninitialization. Hence, transfer learning can save a signiﬁcant amount of computing time. Clearly,\\ntransfer learning is also possible with other network architectures, as long as the network has\\nbeen pre-trained.\\nData Pretrained Model Labels\\nMy Data Pretrained Model My Labels\\ntransfer\\nadjust if necessary adjust if necessary\\nFigure 32. Illustration of transfer learning. 5 RECURRENT NEURAL NETWORK 41\\n5. Recurrent Neural Network\\nThe Neural Networks we introduced so far rely on theassumption of independenceamong\\nthe training and test examples. They process one data point at a time, which is no problem\\nfor data sets, in which every data point is generated independently. However, for sequential\\ndata that occurs in machine translation, speech recognition, sentiment classiﬁcation, etc., the\\ndependence is highly relevant to the task.\\nRecurrent Neural Networks(RNNs), cf. e.g. [15, Section 10] and [12, Section 8.1], are\\nconnectionist models that capture the dynamics of sequences via cycles in the network of nodes.\\nUnlike standard FNNs, recurrent neural networks retain a state that can represent information\\nfrom an arbitrarily long context window.\\nExample 5.1 (machine translation)\\nTranslate a given english input sentenceu, consisting ofTin words u<t>,t = 1,...,T in, e.g.\\nThe sun is shining today\\nu<1> u<2> u<3> u<4> u<5>\\nto a german output sentencey, consisting of Tout words y<t>,t = 1 ,...,T out. Hopefully, the\\noutput will be something like\\nHeute scheint die Sonne\\ny<1> y<2> y<3> y<4>\\nA comparison of FNN and RNN architecture can be seen in Figure 33. For simplicity of notation\\nwe condense all hidden layers of the FNN into a representative computation nodeh.\\nu\\nh\\ny\\nFNN\\nu\\nh\\ny\\nRNN\\nu<1>\\nh<1>\\ny<1>\\nu<2>\\nh<2>\\ny<2>\\nu<3>\\nh<3>\\ny<3>\\nRNN unrolled\\n···\\nFigure 33. Feedforward Neural Network compared to Recurrent Neural Network with inputu,\\noutput y and hidden computation nodesh. The index is understood as time instance.\\nIn RNNs the computation nodesh are often calledRNN cells, cf. [15, Section 10.2]. A RNN\\ncell for a time instancet takes as an inputu<t> and h<t−1>, and computes the outputsh<t>\\nand y<t>, cf. Figure 34. More speciﬁcally for allt= 1,...,T out\\nh<t> = σ\\n(\\nWin ·[h<t−1>; u<t>] + b\\n)\\n, (11)\\ny<t> = Wout ·h<t>. (12) 5 RECURRENT NEURAL NETWORK 42\\nu<t>\\nh<t−1> Weights tanh\\nWeights\\nh<t>\\ny<t>\\nRNN Cell Figure 34. Architecture of\\na RNN cell.\\nThe equations (11) and (12) describe the forward propagation in RNNs. Here,[h<t−1>; u<t>]\\ndenotes the concatenation of the vectors, andh<0> is set to a vector of zeros, so that we do not\\nneed to formulate a special case fort = 1. Depending on the application, a softmax function\\nmay be applied toWouth<t> to get the outputy<t>.\\nIt may happen that input and output have diﬀerent lengths Tin ̸= Tout, see e.g. Example\\n5.1. Depending on the task and the structure of the data, there exist various types of RNN\\narchitectures, cf. [12, Section 8.1] and Figure 35:\\n• one to many, e.g. image description (image to sentence),\\n• many to one, e.g. sentiment analysis (video to word),\\n• many to many, e.g. machine translation (sentence to sentence), like Example 5.1,\\n• many to many, e.g. object tracking (video to object location per frame).\\none to many many to one many to many many to many\\nFigure 35. Illustration of diﬀerent types of RNN architectures.\\nWe note that the weightsWin,Wout and bias b in (11) and (12) do not change over time, but\\ncoincide for all temporal layers of the RNN. Sharing the variables allows the RNN to model\\nvariable length sequences, whereas if we had speciﬁc parameters for each value of the order\\nparameter, we could not generalize to sequence lengths not seen during training. Typically,\\nσ = tanh is chosen in RNNs, and this does also not vary between the layers. To obtain a\\ncomplete optimization problem (P), we still need a loss functionL, since the RNN represents\\nonly the networkF. To this end, each outputy<t> is evaluated with a loss functionL<t> and 5 RECURRENT NEURAL NETWORK 43\\nthe ﬁnal loss is computed by taking the sum over all time instances\\nL(θ) =\\nTout∑\\nt=1\\nL<t>(y<t>(θ)).\\nHere, as usual,θ contains the weightsWin,Wout, and biasb.\\n5.1. Variants of RNNs\\nWe brieﬂy introduce two popular variants of RNNs.\\nIn many applications the output at timet should be a prediction depending on the whole input\\nsequence, not only the \"earlier\" inputsu<i> with i≤t. E.g., in speech recognition, the correct\\ninterpretation of the current sound as a phoneme may depend on the next few phonemes because\\nof co-articulation and potentially may even depend on the next few words because of the linguistic\\ndependencies between nearby words. As a remedy, we can combine a forward-going RNN and\\na backward-going RNN, which is then called aBidirectional RNN, [15, Section 10.3]. This\\narchitecture allows to compute an outputy<t> that depends on both the past and the future\\ninputs, but is most sensitive to the input values around timet. Figure 36 (left) illustrates the\\ntypical bidirectional RNN, withh<t> and g<t> representing the states of the sub-RNNs that\\nmove forward and backward through time, respectively.\\nAnother variant of RNNs is theDeep RNN, [15, Section 10.5]. As seen in FNNs, Section 2,\\nmultiple hidden layers allow the network to have a higher expressiveness. Similarly, a RNN can\\nbe made deep by stacking RNN cells, see Figure 36 (right).\\nFigure 36. Examples of Bidirectional RNNs and Deep RNNs. Here, the inputs are denoted by\\nx instead ofu. Image source:https://stanford.edu/~shervine/teaching/cs-230/. 5 RECURRENT NEURAL NETWORK 44\\n5.2. Long term dependencies\\nIn this section we investigate one of the main challenges, that a RNN can encounter, cf. [15,\\nSection 10.7]. Consider the following illustrative example.\\nExample 5.2\\nPredict the next word in the sequence:\\n1. The cat, which ..., was ...\\n2. The cats, which ..., were ...\\nHere, depending on whether we are talking about one cat or multiple cats the verb has to be\\nadjusted. The \"...\" part in the sentence can be very extensive, so that the dependence becomes\\nlong.\\nThe gradient from the outputy<t> with large t has to propagate back through many layers\\nto aﬀect weights in early layers. Here, the vanishing gradient and exploding gradient problems\\n(cf. Section 4) may occur and hinder training. The exploding gradient problem can be solved\\nrelatively robust bygradient clipping, see e.g. [15, Section 10.11.1]. The idea is quite simple.\\nIf a gradient∂θiL, with respect to some variableθi gets too large, we rescale it. I.e. if∥∂θiL∥≥\\nC ∈R for a hyperparameterC, we set\\n∂θiL ←C· ∂θiL\\n∥∂θiL∥.\\nLet us remark that this is a heuristic approach. In contrast, the vanishing gradient problem is\\nmore diﬃcult to solve.\\nu<1>\\nh<1>\\ny<1>\\nu<2>\\nh<2>\\ny<2>\\nu<3>\\nh<3>\\ny<3>\\nu<4>\\nh<4>\\ny<4>\\nu<5>\\nh<5>\\ny<5>\\nu<6>\\nh<6>\\ny<6>\\nu<7>\\nh<7>\\ny<7>\\nFigure 37. Illustration of vanishing gradient problem for RNNs. The shading of the nodes\\nindicates the sensitivity over time of the network nodes to the inputu<1> (the darker the shade,\\nthe greater the sensitivity). The sensitivity decays over time.\\nAcommonremedyistomodifytheRNNcellsothatitcancapturelongtermdependenciesbetter,\\nand avoids vanishing gradients. Two popular options areGated Recurrent Unit (GRU)from\\n2014 [8], and the cell architecture as suggested already in 1997 inLong Short Term Memory\\n(LSTM) networks [20]. The core idea in both cell architectures is to add gating mechanisms.\\nThese gates have a signiﬁcant inﬂuence on whether, and how severely, the input and previous\\nhidden state inﬂuence the output and new hidden state. Additionally, the gating mechanism\\nhelps to solve the vanishing gradient problem. 5 RECURRENT NEURAL NETWORK 45\\n5.2.1. Gated Recurrent Unit\\nThe gated recurrent unit has a reset (or relevance) gateΓr and an update gateΓu. The compu-\\ntations for one unit are as follows\\nΓr = σ\\n(\\nWr ·[h<t−1>; u<t>] + br\\n)\\n, reset gate\\nΓu = σ\\n(\\nWu ·[h<t−1>; u<t>] + bu\\n)\\n, update gate\\n˜h<t> = tanh\\n(\\nWin ·[Γr ⊙h<t−1>; u<t>] + b\\n)\\n, hidden state candidate\\nh<t> = Γu ⊙˜h<t> + (1 −Γu) ⊙h<t−1>, hidden state\\ny<t> = Wout ·h<t>. output\\nThe computations of the hidden state candidate˜h<t> and the outputy<t> resemble the com-\\nputations in the RNN cell (11) and (12), respectively. However, e.g. ifΓu = 0, then the new\\nhidden state will coincide with the previous hidden state and the candidate will not be taken\\ninto account. Also, the GRU has signiﬁcantly more variables per cell, in comparison with the\\nstandard RNN cell.\\nu<t>\\n·\\nσ σ tanh\\n·1−\\n·h<t−1> h<t>\\ny<t>\\nΓr Γu ˜h<t>\\nGRU\\nFigure 38. Architecture of a gated recurrent unit. Weights are omitted in this illustration. A\\nwhite circle illustrates concatenation, while a circle with a dot represents the Hadamard product\\nand a circle with a plus indicates an addition.\\n5.2.2. Long Short Term Memory\\nThe key to LSTM networks is that in addition to the hidden state, there also exists a cell state\\nc<t>, which is propagated through the network. It can be understood like a conveyor belt, 5 RECURRENT NEURAL NETWORK 46\\nwhich only has minor interactions and runs down the entire chain of LSTM cells, see Figure 39.\\nThis allows information to ﬂow through the network easily. In contrast to GRU, the LSTM cell\\ncontains three gates: the forget gateΓf, input gateΓi and output gateΓo.\\nΓf = σ\\n(\\nWf ·[h<t−1>; u<t>] + bf\\n)\\n, forget gate\\nΓi = σ\\n(\\nWi ·[h<t−1>; u<t>] + bi\\n)\\n, input gate\\nΓo = σ\\n(\\nWo ·[h<t−1>; u<t>] + bo\\n)\\n, output gate\\n˜c<t> = tanh\\n(\\nWc ·[h<t−1>; u<t>] + bc\\n)\\n, cell state candidate\\nc<t> = Γf ⊙˜c<t−1> + Γi ⊙˜c<t>, cell state\\nh<t> = Γo ⊙tanh(c<t>), hidden state\\ny<t> = Wout ·h<t>. output\\nu<t>\\nh<t−1>\\nc<t−1>\\nh<t>\\nc<t>\\ny<t>\\nσ σ tanh σ\\n·\\n· ·\\ntanh\\nΓf Γi ˜c<t> Γo\\nLSTM Cell\\nFigure 39. Architecture of a LSTM cell. Weights are omitted in this illustration. A white\\ncircle illustrates concatenation, while a circle with a dot represents the Hadamard product and\\na circle with a plus indicates an addition.\\nRemark 5.3 Without gates, i.e.Γf = Γi = Γo = 1, the LSTM network has a certain similarity\\nwith the ResNet structure, which was developed later than the LSTM, in 2016 in [17]. This is\\nnot so surprising, since both networks aim at solving the vanishing gradient problem. In fact,\\npropagating the cell state has similar eﬀects on the gradients as introducing skip connections. REFERENCES 47\\n5.3. Language processing\\nAn important application of RNNs is language processing, e.g. machine translation, see Example\\n5.1. In such tasks the words need to be represented, so that the RNN can work with them.\\nFurthermore, we need a way to deal with punctuation marks, and an indicator for the end of a\\nsentence.\\nTo represent the words, we form a dictionary. For the english language we will end up with a\\nvector containing more than 10000 words. Intuitively, we sort the words alphabetically and to\\nsimplify computations we use aone-hot representation. E.g., if \"the\" is the 8367th word in\\nthe english dictionary vector, we represent the ﬁrst input\\nu<1> =\\n(\\n0 ... 0 1 0 ... 0\\n)⊤\\n= e8367,\\nwith the 8367th unit vector. This allows for an easy way to measure correctness in supervised\\nlearning and later on we can use the dictionary to recover the words. Additionally, it is common\\nto create a token for unknown words, which are not in the dictionary. Punctuation marks can\\neither be ignored, or we also create tokens for them. However, the dictionary should at least\\ncontain an \"end of sentence\" token to separate sentences from each other.\\nReferences\\n[1] H. Antil, T. S. Brown, R. Löhner, F. Togashi, and D. Verma.Deep Neural Nets with Fixed\\nBias Conﬁguration. 2022. arXiv:2107.01308 [math.OC].\\n[2] H. Antil, H. Díaz, and E. Herberg. An Optimal Time Variable Learning Framework for\\nDeep Neural Networks. 2022. arXiv:2204.08528 [math.OC].\\n[3] H. Antil, R. Khatri, R. Löhner, and D. Verma. “Fractional deep neural network via con-\\nstrainedoptimization”.In: Machine Learning: Science and Technology2.1(2020),p.015003.\\n[4] A. Baldominos, Y. Saez, and P. Isasi. “A Survey of Handwritten Character Recognition\\nwith MNIST and EMNIST”. In:Applied Sciences 9.15 (2019). issn: 2076-3417. doi: 10.\\n3390/app9153169. url: https://www.mdpi.com/2076-3417/9/15/3169.\\n[5] Y. Bengio, P. Simard, and P. Frasconi. “Learning long-term dependencies with gradient\\ndescent is diﬃcult”. In:IEEE transactions on neural networks5.2 (1994), pp. 157–166.\\n[6] S. Boyd and L. Vandenberghe.Convex optimization. Cambridge university press, 2004.\\n[7] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. “Neural ordinary\\ndiﬀerential equations”. In:Advances in neural information processing systems31 (2018).\\n[8] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y.\\nBengio. Learning phrase representations using RNN encoder-decoder for statistical machine\\ntranslation. 2014. arXiv:1406.1078.\\n[9] CS231n Convolutional Neural Networks for Visual Recognition. https://cs231n.github.\\nio/convolutional-networks/. 2023.\\n[10] G. Cybenko. “Approximation by superpositions of a sigmoidal function”. In:Mathematics\\nof control, signals and systems2.4 (1989).\\n[11] K. Fukushima. “Neocognitron: A Self-organizing Neural Network Model for a Mechanism of\\nPattern Recognition Unaﬀected by Shift in Position”. In:Biological Cybernetics36 (1980).\\n[12] A.Geiger. Deep Learning Lecture Notes.https://drive.google.com/file/d/16TaFr6d3eZXNkShgJJxaf6CN7xz1eOBs/\\nview. 2021. REFERENCES 48\\n[13] S.GhassemiandE.Magli.“ConvolutionalNeuralNetworksforOn-BoardCloudScreening”.\\nIn: Remote Sensing11 (2019).doi: 10.3390/rs11121417.\\n[14] X. Glorot and Y. Bengio. “Understanding the diﬃculty of training deep feedforward neural\\nnetworks”.In:Proceedings of the thirteenth international conference on artiﬁcial intelligence\\nand statistics. JMLR Workshop and Conference Proceedings. 2010.\\n[15] I. Goodfellow, Y. Bengio, and A. Courville.Deep learning. MIT press, 2016.\\n[16] E. Haber and L. Ruthotto. “Stable architectures for deep neural networks”. In:Inverse\\nproblems 34.1 (2017).doi: 10.1088/1361-6420/aa9a90.\\n[17] K. He, X. Zhang, S. Ren, and J. Sun. “Deep residual learning for image recognition”. In:\\nProceedings of the IEEE conference on computer vision and pattern recognition. 2016.\\n[18] K. He, X. Zhang, S. Ren, and J. Sun. “Delving deep into rectiﬁers: Surpassing human-\\nlevel performance on imagenet classiﬁcation”. In:Proceedings of the IEEE international\\nconference on computer vision. 2015.\\n[19] K. He, X. Zhang, S. Ren, and J. Sun. “Identity mappings in deep residual networks”. In:\\nEuropean conference on computer vision. Springer. 2016.\\n[20] S. Hochreiter and J.f Schmidhuber. “Long short-term memory”. In:Neural computation9.8\\n(1997).\\n[21] R. A. Horn. “The hadamard product”. In:Proc. Symp. Appl. Math. Vol. 40. 1990, pp. 87–\\n169.\\n[22] S. Ioﬀe and C. Szegedy. “Batch normalization: Accelerating deep network training by re-\\nducing internal covariate shift”. In:International conference on machine learning. pmlr.\\n2015, pp. 448–456.\\n[23] D. P. Kingma and J. Ba. “Adam: A method for stochastic optimization”. In:arXiv preprint\\narXiv:1412.6980 (2014). arXiv:1412.6980.\\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. “ImageNet Classiﬁcation with Deep Con-\\nvolutional Neural Networks”. In:Communications of the ACM60.6 (2017).\\n[25] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D.\\nJackel. “Gradient-based learning applied to document recognition”. In:Proceedings of the\\nIEEE 86 (1998).\\n[26] A. Ng. CS229 Lecture Notes. https://cs229.stanford.edu/notes2022fall/main_\\nnotes.pdf. 2022.\\n[27] F. Rosenblatt. “The perceptron: a probabilistic model for information storage and organi-\\nzation in the brain.” In:Psychological review65.6 (1958), p. 386.\\n[28] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. “Dropout:\\na simple way to prevent neural networks from overﬁtting”. In:The journal of machine\\nlearning research15.1 (2014).\\n[29] L. Torrey and J. Shavlik. “Transfer learning”. In:Handbook of research on machine learning\\napplications and trends: algorithms, methods, and techniques. IGI global, 2010, pp. 242–\\n264.',\n",
       "   'Tackling Error Propagation through Reinforcement Learning:\\nA Case of Greedy Dependency Parsing\\nMinh Lê\\nCLTL\\nVrije Universiteit Amsterdam\\nAmsterdam, The Netherlands\\nm.n.le@vu.nl\\nAntske Fokkens\\nCLTL\\nVrije Universiteit Amsterdam\\nAmsterdam, The Netherlands\\nantske.fokkens@vu.nl\\nAbstract\\nError propagation is a common problem\\nin NLP. Reinforcement learning explores\\nerroneous states during training and can\\ntherefore be more robust when mistakes\\nare made early in a process. In this paper,\\nwe apply reinforcement learning to greedy\\ndependency parsing which is known to\\nsuffer from error propagation. Reinforce-\\nment learning improves accuracy of both\\nlabeled and unlabeled dependencies of\\nthe Stanford Neural Dependency Parser,\\na high performance greedy parser, while\\nmaintaining its efﬁciency. We investigate\\nthe portion of errors which are the result\\nof error propagation and conﬁrm that rein-\\nforcement learning reduces the occurrence\\nof error propagation.\\n1 Introduction\\nError propagation is a common problem for many\\nNLP tasks (Song et al., 2012; Quirk and Corston-\\nOliver, 2006; Han et al., 2013; Gildea and Palmer,\\n2002; Yang and Cardie, 2013). It can occur when\\nNLP tools applied early on in a pipeline make\\nmistakes that have negative impact on higher-level\\ntasks further down the pipeline. It can also occur\\nwithin the application of a speciﬁc task, when se-\\nquential decisions are taken and errors made early\\nin the process affect decisions made later on.\\nWhen reinforcement learning is applied, a sys-\\ntem actively tries out different sequences of ac-\\ntions. Most of these sequences will contain some\\nerrors. We hypothesize that a system trained in this\\nmanner will be more robust and less susceptible to\\nerror propagation.\\nWe test our hypothesis by applying reinforce-\\nment learning to greedy transition-based parsers\\n(Yamada and Matsumoto, 2003; Nivre, 2004),\\nwhich have been popular because of superior efﬁ-\\nciency and accuracy nearing state-of-the-art. They\\nare also known to suffer from error propagation.\\nBecause they work by carrying out a sequence of\\nactions without reconsideration, an erroneous ac-\\ntion can exert a negative effect on all subsequent\\ndecisions. By rendering correct parses unreach-\\nable or promoting incorrect features, the ﬁrst error\\ninduces the second error and so on. McDonald\\nand Nivre (2007) argue that the observed negative\\ncorrelation between parsing accuracy and sentence\\nlength indicates error propagation is at work.\\nWe compare reinforcement learning to super-\\nvised learning on Chen and Manning (2014)’s\\nparser. This high performance parser is available\\nas open source. It does not make use of alterna-\\ntive strategies for tackling error propagation and\\nthus provides a clean experimental setup to test\\nour hypothesis. Reinforcement learning increased\\nboth unlabeled and labeled accuracy on the Penn\\nTreeBank and German part of SPMRL (Seddah\\net al., 2014). This outcome shows that reinforce-\\nment learning has a positive effect, but does not yet\\nprove that this is indeed the result of reduced er-\\nror propagation. We therefore designed an exper-\\niment which identiﬁed which errors are the result\\nof error propagation. We found that around 50%\\nof avoided errors were cases of error propagation\\nin our best arc-standard system. Considering that\\n27% of the original errors were caused by error\\npropagation, this result conﬁrms our hypothesis.\\nThis paper provides the following contributions:\\n1. We introduce Approximate Policy Gradient\\n(APG), a new algorithm that is suited for de-\\npendency parsing and other structured pre-\\ndiction problems.\\n2. We show that this algorithm improves the ac-\\ncuracy of a high-performance greedy parser.\\narXiv:1702.06794v1  [cs.CL]  22 Feb 2017 3. We design an experiment for analyzing error\\npropagation in parsing.\\n4. We conﬁrm our hypothesis that reinforce-\\nment learning reduces error propagation.\\nTo our knowledge, this paper is the ﬁrst to ex-\\nperimentally show that reinforcement learning can\\nreduce error propagation in NLP.\\nThe rest of this paper is structured as follows.\\nWe discuss related work in Section 2. This is fol-\\nlowed by a description of the parsers used in our\\nexperiments in Section 3. Section 4 outlines our\\nexperimental setup and presents our results. The\\nerror propagation experiment and its outcome are\\ndescribed in Section 5. Finally, we conclude and\\ndiscuss future research in Section 6.\\n2 Related Work\\nIn this section, we address related work on depen-\\ndency parsing, including alternative approaches\\nfor reducing error propagation, and reinforcement\\nlearning.\\n2.1 Dependency Parsing\\nWe use Chen and Manning (2014)’s parser as a\\nbasis for our experiments. Their parser is open-\\nsource and has served as a reference point for\\nmany recent publications (Dyer et al., 2015; Weiss\\net al., 2015; Alberti et al., 2015; Honnibal and\\nJohnson, 2015, among others). They provide an\\nefﬁcient neural network that learns dense vec-\\ntor representations of words, PoS-tags and depen-\\ndency labels. This small set of features makes their\\nparser signiﬁcantly more efﬁcient than other popu-\\nlar parsers, such as the Malt (Nivre et al., 2007) or\\nMST (McDonald et al., 2005) parser while obtain-\\ning higher accuracy. They acknowledge the error\\npropagation problem of greedy parsers, but leave\\naddressing this through (e.g.) beam search for fu-\\nture work.\\nDyer et al. (2015) introduce an approach that\\nuses Long Short-Term Memory (LSTM). Their\\nparser still works incrementally and the number of\\nrequired operations grows linearly with the length\\nof the sentence, but it uses the complete buffer,\\nstack and history of parsing decisions, giving the\\nmodel access to global information. Weiss et al.\\n(2015) introduce several improvements on Chen\\nand Manning (2014)’s parser. Most importantly,\\nthey put a globally-trained perceptron layer in-\\nstead of a softmax output layer. Their model uses\\nsmaller embeddings, rectiﬁed linear instead of cu-\\nbic activation function, and two hidden layers in-\\nstead of one. They furthermore apply an aver-\\naged stochastic gradient descent (ASGD) learn-\\ning scheme. In addition, they apply beam search\\nand increase training data by using unlabeled data\\nthrough the tri-training approach introduced by Li\\net al. (2014), which leads to further improvements.\\nKiperwasser and Goldberg (2016) introduce a\\nnew way to represent features using a bidirectional\\nLSTM and improve the results of a greedy parser.\\nAndor et al. (2016) present a mathematical proof\\nthat globally normalized models are more expres-\\nsive than locally normalized counterparts and pro-\\npose to use global normalization with beam search\\nat both training and testing.\\nOur approach differs from all of the work men-\\ntioned above, in that it manages to improve results\\nof Chen and Manning (2014)without changing the\\narchitecture of the model nor the input represen-\\ntation. The only substantial difference lies in the\\nway the model is trained. In this respect, our re-\\nsearch is most similar to training approaches us-\\ning dynamic oracles (Goldberg and Nivre, 2012).\\nTraditional static oracles can generate only one se-\\nquence of actions per sentence. A dynamic ora-\\ncle gives all trajectories leading to the best pos-\\nsible result from every valid parse conﬁguration.\\nThey can therefore be used to generate more train-\\ning sequences including those containing errors.\\nA drawback of this approach is that dynamic or-\\nacles have to be developed speciﬁcally for indi-\\nvidual transition systems (e.g. arc-standard, arc-\\neager). Therefore, a large number of dynamic or-\\nacles have been developed in recent years (Gold-\\nberg and Nivre, 2012; Goldberg and Nivre, 2013;\\nGoldberg et al., 2014; Gomez-Rodriguez et al.,\\n2014; Björkelund and Nivre, 2015). In contrast,\\nthe reinforcement learning approach proposed in\\nthis paper is more general and can be applied to a\\nvariety of systems.\\nZhang and Chan (2009) present the only study\\nwe are aware of that also uses reinforcement learn-\\ning for dependency parsing. They compare their\\nresults to Nivre et al. (2006b) using the same fea-\\ntures, but they also change the model and apply\\nbeam search. It is thus unclear to what extend their\\nimprovements are due to reinforcement learning.\\nEven though most approaches mentioned above\\nimprove the results reported by Chen and Man-\\nning (2014) and even more impressive results on dependency parsing have been achieved since (no-\\ntably, Andor et al. (2016)), Chen and Manning’s\\nparser provides a better baseline for our purposes.\\nWe aim at investigating the inﬂuence of reinforce-\\nment learning on error propagation and want to\\ntest this in a clean environment, where reinforce-\\nment learning does not interfere with other meth-\\nods that address the same problem.\\n2.2 Reinforcement Learning\\nReinforcement learning has been applied to sev-\\neral NLP tasks with success, e.g. agenda-based\\nparsing (Jiang et al., 2012), semantic parsing (Be-\\nrant and Liang, 2015) and simultaneous machine\\ntranslation (Grissom II et al., 2014). To our knowl-\\nedge, however, none of these studies investigated\\nthe inﬂuence of reinforcement learning on error\\npropagation.\\nLearning to Search (L2S) is probably the most\\nprominent line of research that applies reinforce-\\nment learning (more precisely, imitation learn-\\ning) to NLP. Various algorithms, e.g. SEARN\\n(Daumé III et al., 2009) and DAgger (Ross et\\nal., 2011), have been developed sharing common\\nhigh-level steps: a roll-in policy is executed to\\ngenerate training states from which a roll-out pol-\\nicy is used to estimate the loss of certain actions.\\nThe concrete instantiation differs from one algo-\\nrithm to another with choices including a referent\\npolicy (static or dynamic oracle), learned policy,\\nor a mixture of the two. Early work in L2S fo-\\ncused on reducing reinforcement learning into bi-\\nnary classiﬁcation (Daumé III et al., 2009), but\\nnewer systems favored regressors for efﬁciency\\n(Chang et al., 2015, Supplementary material, Sec-\\ntion B). Our algorithm APG is simpler than L2S in\\nthat it uses only one policy (pre-trained with stan-\\ndard supervised learning) and applies the existing\\nclassiﬁer directly without reduction (the only re-\\nquirement is that it is probabilistic). Nevertheless,\\nour results demonstrate its effectiveness.\\nAPG belongs to the family of policy gradient al-\\ngorithms (Sutton et al., 1999), i.e. it maximizes the\\nexpected reward directly by following its gradient\\nw.r.t. the parameters. The advantage of using a\\npolicy gradient algorithm in NLP is that gradient-\\nbased optimization is already widely used. REIN-\\nFORCE (Williams, 1992; Ranzato et al., 2016) is\\na widely-used policy gradient algorithm but it is\\nalso well-known for suffering from high variance\\n(Sutton et al., 1999).\\nWe directly compare our approach to REIN-\\nFORCE, whereas we leave a direct comparison\\nto L2S for future work. Our experiments show\\nthat our algorithm results in lower variance and\\nachieves better performance than REINFORCE.\\nRecent work addresses the approximation of re-\\ninforcement learning gradient in the context of\\nmachine translation. Shen et al. (2016)’s algo-\\nrithm is roughly equivalent to the combination\\nof an oracle and random sampling. Their ap-\\nproach differs from ours, because it does not retain\\nmemory across iteration as in our best-performing\\nmodel (see Section 3.4).\\n2.3 Reinforcement and error propagation\\nAs mentioned above, previous work that applied\\nreinforcement learning to NLP has, to our knowl-\\nedge, not shown that it improved results by reduc-\\ning error propagation.\\nWork on identifying the impact of error prop-\\nagation in parsing is rare, Ng and Curran (2015)\\nbeing a notable exception. They provide a detailed\\nerror analysis for parsing and classify which kind\\nof parsing errors are involved with error propa-\\ngation. There are four main differences between\\ntheir approaches and ours. First, Ng and Curran\\ncorrect arcs in the tree and our algorithm corrects\\ndecisions of the parsing algorithm. Second, our\\napproach distinguishes between cases where one\\nerroneous action deterministically leads to multi-\\nple erroneous arcs and cases where an erroneous\\naction leads to conditions that indirectly result in\\nfurther errors (see Section 5.1 for a detailed expla-\\nnation). Third, Ng and Curran’s algorithm corrects\\nall erroneous arcs that are the same type of pars-\\ning error and point out that they cannot examine\\nthe interaction between multiple errors of the same\\ntype in a sentence. Our algorithm corrects errors\\nincrementally and therefore avoids this issue. Fi-\\nnally, the classiﬁcation and analysis presented in\\nNg and Curran (2015) are more extensive and de-\\ntailed than ours. Our algorithm can, however, eas-\\nily be extended to perform similar analysis. Over-\\nall, Ng and Curran’s approach for error analysis\\nand ours are complementary. Combining them and\\napplying them to various systems would form an\\ninteresting direction for future work.\\n3 A Reinforced Greedy Parser\\nThis section describes the systems used in our ex-\\nperiments. We ﬁrst describe the arc-standard al- Step Transition Stack Buffer Arcs\\n0 <ROOT > waves hit ... Big Board ∅\\n1 SHIFT <ROOT >waves hit stocks ... Big Board ∅\\n2 SHIFT <ROOT >waves hit stocks themselves ... Big Board ∅\\n3 LEFT nsubj <ROOT >hit stocks themselves ... Big Board A1 = { hit\\nnsubj\\n−−−→waves}\\n4 SHIFT <ROOT >hit stocks themselves on the Big Board A1\\n5 SHIFT <ROOT >hit stocks themselves on the Big Board A1\\n6 RIGHT dep <ROOT >hit stocks on the Big Board A2 = A1∪\\n{ stock\\ndep\\n−−→themselves}\\n7 RIGHT dobj <ROOT >hit on the Big Board A3 = A2∪{ hit\\ndobj\\n−−−→stock}\\nTable 1: Parsing oracle walk-through\\ngorithm, because familiarity with it helps to un-\\nderstand our error propagation analysis. Next, we\\nbrieﬂy point out the main differences between the\\narc-standard algorithm and the alternative algo-\\nrithms we experimented with (arc-eager and swap-\\nstandard). We then outline the traditional and our\\nnovel machine learning approaches. The features\\nwe used are identical to those described in Chen\\nand Manning (2014). We are not aware of research\\nidentifying the best feature for a neural parser with\\narc-eager or swap-standard so we use the same\\nfeatures for all transition systems.\\n3.1 Transition-Based Dependency Parsing\\nIn an arc-standard system (Nivre, 2004), a parsing\\nconﬁguration consists of a triple ⟨Σ,β,A ⟩, where\\nΣ is a stack, β is a buffer containing the remain-\\ning input tokens and A are the dependency arcs\\nthat are created during parsing process. At initi-\\nation, the stack contains only the root symbol ( Σ\\n= [ROOT]), the buffer contains the tokens of the\\nsentence (β = [w1,...,w n]) and the set of arcs is\\nempty (A= ∅).\\nThe arc-standard system supports three transi-\\ntions. When σ1 is the top element and σ2 the sec-\\nond element on the stack, and β1 the ﬁrst element\\nof the buffer:1\\nLEFTl adds an arc σ1\\nl− →σ2 to Aand removes σ2\\nfrom the stack.\\nRIGHTl adds an arc σ2\\nl− →σ1 to Aand removes\\nσ1 from the stack.\\nSHIFT moves β1 to the stack.\\nWhen the buffer is empty, the stack contains\\nonly the root symbol and Acontains a parse tree,\\nthe conﬁguration is completed. For a sentence of\\n1Naturally, the transitions LEFT l and RIGHTl can only\\ntake place if the stack contains at least two elements and\\nSHIFT can only occur when there is at least one element on\\nthe buffer.\\n<ROOT> waves hit stocks themselves on the Big Board\\nFigure 1: Correct dependencies for a simpliﬁed\\nexample from Penn TreeBank\\nNw tokens, a full parse takes 2 Nw + 1 transitions\\nto complete (including the initiation). Figure 1\\nprovides the gold parse tree for a (simpliﬁed) ex-\\nample from the Penn Treebank. The steps taken\\nto create the dependencies between the sentence’s\\nhead word hit and its subject and direct object are\\nprovided in Table 1.\\nTo demonstrate that reinforcement learning can\\ntrain different systems, we also carried out ex-\\nperiments with arc-eager (Nivre, 2003) and swap-\\nstandard (Nivre, 2009). Arc-eager is designed for\\nincremental parsing and included in the popular\\nMaltParser (Nivre et al., 2006a). Swap-standard is\\na simple and effective solution to unprojective de-\\npendency trees. Because arc-eager does not guar-\\nantee complete parse trees, we used a variation\\nthat employs an action called UNSHIFT to re-\\nsume processing of tokens that would otherwise\\nnot be attached to a head (Nivre and Fernández-\\nGonzález, 2014).\\n3.2 Training with a Static Oracle\\nIn transition-based dependency parsing, it is com-\\nmon to convert a dependency treebank D∋ (x,y)\\ninto a collection of input features s ∈S and cor-\\nresponding gold-standard actions a∈A for train-\\ning, using a static oracle O. In Chen and Man-\\nning (2014), a neural network works as a function\\nmapping input features to probabilities of actions:\\nfNN : S×A → [0,1]. The neural network is\\ntrained to minimize negative log-likelihood loss on the converted treebank:\\nL=\\n∑\\n(x,y)∈D\\n∑\\n(s,a)∈O(x,y)\\n−log fNN (s,a; θ) (1)\\n3.3 Reinforcement Learning\\nFollowing Maes et al. (2009), we view transition-\\nbased dependency parsing as a deterministic\\nMarkov Decision Process. The problem is sum-\\nmarized by a tuple ⟨S,A,T,r⟩where Sis the set\\nof all possible states, Acontains all possible ac-\\ntions, Tis a mapping S×A→S called transition\\nfunction and r: S×A→ R is a reward function.\\nA state corresponds to a conﬁguration and is\\nsummarized into input features. Possible actions\\nare deﬁned for each transition system described in\\nSection 3.1. We keep the training approach simple\\nby using only one reward r(¯y) at the end of each\\nparse.\\nGiven this framework, a stochastic policy\\nguides our parser by mapping each state to a prob-\\nabilistic distribution of actions. During training,\\nwe use function fNN described in Section 3.2 as a\\nstochastic policy. At test time, actions are chosen\\nin a greedy fashion following existing literature.\\nWe aim at ﬁnding the policy that maximizes the\\nexpected reward (or, equivalently, minimizes the\\nexpected loss) on the training dataset:\\nmaximize η=\\n∑\\n(x,y)∈D\\n∑\\na1:m∼f\\nr(¯y)\\nm∏\\ni=1\\nfNN (si,ai; θ)\\n(2)\\nwhere a1:m is a sequence of actions obtained by\\nfollowing policy fNN until termination and s1:m\\nare corresponding states (with sm+1 being the ter-\\nmination state).\\n3.4 Approximate Policy Gradient\\nGradient ascent can be used to maximize the ex-\\npected reward in Equation 2. The gradient of ex-\\npected reward w.r.t. parameters is (note that dz =\\nzd(log z)):\\n∂η\\n∂θ =\\n∑\\n(x,y)∈D\\n∑\\na1:m∼fNN\\nr(¯y)\\nm∏\\ni=1\\nfNN (si,ai)\\nm∑\\ni=1\\n∂\\n∂θ log fNN (si,ai; θ)\\n(3)\\nBecause of the exponential number of possible\\ntrajectories, calculating the gradient exactly is not\\npossible. We propose to replace it by an approxi-\\nmation (hence the name Approximate Policy Gra-\\ndient) by summing over a small subsetUof trajec-\\ntories. Following common practice, we also use a\\nbaseline b(y) that only depends on the correct de-\\npendency tree. The parameter is then updated by\\nfollowing the approximate gradient:\\n∆θ∝\\n∑\\n(x,y)∈D\\n∑\\na1:m∈U\\n(r(¯y) −b(y))\\nm∏\\ni=1\\nfNN (si,ai)\\nm∑\\ni=1\\n∂\\n∂θ log fNN (si,ai; θ)\\n(4)\\nInstead of sampling one trajectory at a time as in\\nREINFORCE, Equation 4 has the advantage that\\nsampling over multiple trajectories could lead to\\nmore stable training and higher performance. To\\nachieve that goal, the choice of U is critical. We\\nempirically evaluate three strategies:\\nRL-O RACLE : only includes the oracle transition\\nsequence.\\nRL-R ANDOM : randomly samples k distinct tra-\\njectories at each iteration. Every action is\\nsampled according tofNN , i.e. preferring tra-\\njectories for which the current policy assigns\\nhigher probability.\\nRL-M EMORY : samples randomly as the previ-\\nous method but retains k trajectories with\\nhighest rewards across iterations in a sepa-\\nrate memory. Trajectories are “forgotten” (re-\\nmoved) randomly with probability ρ before\\neach iteration.2\\nIntuitively, trajectories that are more likely and\\nproduce higher rewards are better training exam-\\nples. It follows from Equation 3 that they also\\nbear bigger weight on the true gradient. This is the\\nrationale behind RL-R ANDOM and RL-O RACLE .\\nFor a suboptimal parser, however, these objec-\\ntives sometimes work against each other. RL-\\nMEMORY was designed to ﬁnd the right balance\\nbetween them. It is furthermore important that the\\nparser is pretrained to ensure good samples. Algo-\\nrithm 1 illustrates the procedure of training a de-\\npendency parser using the proposed algorithms.\\n2We assign a random number (drawn uniformly from\\n[0,1]) to each trajectory in memory and remove those as-\\nsigned a number less than ρ. MemorySeqs ←∅;\\nforeach training batch bdo\\nforeach sentence s∈b do\\nOracleSeq ←Oracle(s);\\nSystemSeqs ←(sample kparsing\\ntransition sequences for s);\\nif RL-Oracle then\\nComputeGradients(OracleSeq);\\nelse ifRL-Random then\\nComputeGradients(SystemSeqs);\\nelse ifRL-Memory then\\nm←MemorySeqs[s];\\nforeach q ∈m do\\nif RandomNumber() <ρthen\\nRemove qfrom m;\\nend\\nend\\nforeach q ∈SystemSeqs do\\nif |m| < kthen\\nInsert qinto m;\\nelse\\np←(sequence with\\nsmallest reward in m);\\nif reward(q) > reward(p)\\nthen\\nReplace pby qin m;\\nend\\nend\\nComputeGradients(m);\\nend\\nPerform one gradient descent step;\\nend\\nAlgorithm 1:Training a dependency parser with\\napproximate policy gradient.\\n4 Reinforcement Learning Experiments\\nWe ﬁrst train a parser using a supervised learning\\nprocedure and then improve its performance using\\nAPG. We empirically tested that training a second\\ntime with supervised learning has little to no effect\\non performance.\\n4.1 Experimental Setup\\nWe use PENN Treebank 3 with standard split\\n(training, development and test set) for our exper-\\niments with arg-standard and arg-eager. Because\\nthe swap-standard parser is mainly suited for non-\\nprojective structures, which are rare in the PENN\\nTreebank, we evaluate this parser on the German\\nArc- Arc- Swap-\\nstandard eager standard\\nUAS LAS UAS LAS UAS LAS\\nSL 91.3 89.4 88.3 85.8 84.3 81.3\\nRE 91.9 90.2 89.7 87.2 87.5 84.4\\nRL-O 91.8 90.2 88.9 86.5 86.8 83.9\\nRL-R 92.2 90.6 89.4 87.0 87.5 84.5\\nRL-M 92.2 90.6 89.8 87.4 87.6 84.6\\nTable 2: Comparing training methods on PENN\\nTreebank (arc-standard and arc-eager) and Ger-\\nman part of SPMRL-2014 (swap-standard).\\nsection of the SPMRL dataset. For PENN Tree-\\nbank, we follow Chen and Manning’s preprocess-\\ning steps. We also use their pretrained model 3 for\\narc-standard and train our own models in similar\\nsettings for other transition systems.\\nFor reinforcement learning, we use AdaGrad for\\noptimization. We do not use dropout because we\\nobserved that it destablized the training process.\\nThe reward r(¯y) is the number of correct labeled\\narcs (i.e. LAS multiplied by number of tokens). 4\\nThe baseline is ﬁxed to half the number of tokens\\n(corresponding to a 0.5 LAS score). As train-\\ning takes a lot of time, we tried only few values\\nof hyperparameters on the development set and\\npicked k = 8 and ρ = 0.01. 1,000 updates were\\nperformed (except for REINFORCE which was\\ntrained for 8,000 updates) with each training batch\\ncontains 512 randomly selected sentences. The\\nStanford dependency scorer 5 was used for evalu-\\nation.\\n4.2 Effectiveness of Reinforcement Learning\\nTable 2 displays the performance of different ap-\\nproaches to training dependency parsers. Al-\\nthough we used Chen and Manning (2014)’s pre-\\ntrained model and Stanford open-source software,\\nthe results of our baseline are slightly worse than\\nwhat is reported in their paper. This could be due\\nto minor differences in settings and does not affect\\nour conclusions.\\nAcross transition systems and two languages,\\nAPG outperforms supervised learning, verifying\\nour hypothesis. Moreover, it is not simply be-\\ncause the learners are exposed to more examples\\nthan their supervised counterparts. RL-O RACLE\\n3We use PTB_Stanford_params.txt.gz down-\\nloaded from http://nlp.stanford.edu/software/\\nnndep.shtml on December 30th, 2015.\\n4Punctuation is not taken into account, following Chen\\nand Manning (2014).\\n5Downloaded from http://nlp.stanford.edu/\\nsoftware/lex-parser.shtml. is trained on exactly the same examples as the\\nstandard supervised learning system (SL), yet it\\nis consistently superior. This can only be ex-\\nplained by the superiority of the reinforcement\\nlearning objective function compared to negative\\nlog-likelihood.\\nThe results support our hypothesis that APG is\\nbetter than REINFORCE (abbreviated as RE in\\nTable 2) as RL-M EMORY always outperforms the\\nclassical algorithm and the other two heuristics do\\nin two out of three cases. The usefulness of train-\\ning examples that contain errors is evident through\\nthe better performance of RL-R ANDOM and RL-\\nMEMORY in comparison to RL-O RACLE .\\nTable 3 shows the importance of samples for\\nRL-R ANDOM . The algorithm hurts performance\\nwhen only one sample is used whereas training\\nwith two or more samples improves the results.\\nThe difference cannot be explained by the total\\nnumber of observed samples because one-sample\\ntraining is still worse after 8,000 iterations com-\\npared to a sample size of 8 after 1,000 itera-\\ntions. The beneﬁt of added samples is twofold: in-\\ncreased performance and decreased variance. Be-\\ncause these beneﬁts saturate quickly, we did not\\ntest sample sizes beyond 32.\\nDev Test Test std.\\nUAS LAS UAS LAS UAS LAS\\nSL 91.5 89.6 91.3 89.4 - -\\nRE 92.1∗ 90.4∗ 91.9∗ 90.2∗ 0.04 0.05\\n1 91.2∗ 89.1∗ 91.0∗ 88.9∗ 0.12 0.15\\n2 91.8∗ 90.0∗ 91.6∗ 89.9∗ 0.09 0.09\\n4 92.2∗ 90.5∗ 92.0∗ 90.4∗ 0.09 0.08\\n8 92.4∗ 90.8∗ 92.2∗ 90.6∗ 0.03 0.05\\n16 92.4 90.8 92.2 90.6 - -\\n32 92.4 90.8 92.3 90.6 - -\\nTable 3: Parsing accuracy of RL-R ANDOM (arc-\\nstandard) with different sample sizes compared to\\nsupervised learning (SL) and REINFORCE (RE).\\n∗: signiﬁcantly different from SL with p< 0.001\\n5 Error Propagation Experiment\\nWe hypothesized that reinforcement learning\\navoids error propagation. In this section, we de-\\nscribe our algorithm and the experiment that iden-\\ntiﬁes error propagation in the arc-standard parsers.\\n5.1 Error Propagation\\nSection 3.1 explained that a transition-based\\nparser goes through the sentence incrementally\\nand must select a transition from [SHIFT, LEFT l,\\n<ROOT> waves hit stocks themselves on the Big Board\\n<ROOT> waves hit stocks themselves on the Big Board\\n(A)\\n(B)\\n<ROOT> waves hit stocks themselves on the Big Board\\n(C)\\nFigure 2: Three dependency graphs: gold (A), arc\\nerrors caused by one decision error (B) and arc er-\\nrors caused by multiple decision errors (C).\\nRIGHTl] at each step. We use the term arc error\\nto refer to an erroneous arc in the resulting tree.\\nThe term decision errorrefers to a transition that\\nleads to a loss in parsing accuracy. Decision er-\\nrors in the parsing process lead to one or more arc\\nerrors in the resulting tree. There are two ways\\nin which a single decision error may lead to mul-\\ntiple arc errors. First, the decision can determin-\\nistically lead to more than one arc error, because\\n(e.g.) an erroneously formed arc also blocks other\\ncorrect arcs. Second, an erroneous parse decision\\nchanges some of the features that the model uses\\nfor future decisions and these changes can lead to\\nfurther (decision) errors down the road.\\nWe illustrate both cases using two incorrect\\nderivations presented in Figure 2. The original\\ngold tree is repeated in (A). The dependency graph\\nin Figure 2 (B) contains three erroneous depen-\\ndency arcs (indicated by dashed arrows). The ﬁrst\\nerror must have occurred when the parser executed\\nRIGHTamod creating the arc Big →Board. After\\nthis error, it is impossible to create the correct re-\\nlations on →Board and Board →the. The wrong\\narcs Big →the and on →Big are thus all the result\\nof a single decision error.\\nFigure 2 (C) represents the dependency graph\\nthat is actually produced by our parser.6 It contains\\ntwo erroneous arcs: hit →themselves and them-\\nselves →on. Table 4 provides a possible sequence\\nof steps that led to this derivation, starting from\\nthe moment stocks was added to the stack (Step\\n4). The ﬁrst error is introduced in Step 5’, where\\nhit combines with stocks before stocks has picked\\nup its dependent themselves. At that point, them-\\nselves can no longer be combined with the right\\nhead. The proposition on, on the other hand, can\\n6The example is a fragment of a more complex sentence\\nconsisting of 33 tokens. The parser does provide the correct\\noutput when is analyzes this sequence in isolation. Step Transition Stack Buffer Arcs\\n4 SHIFT <ROOT >hit stocks themselves on the Big Board A1\\n5’ RIGHT dobj <ROOT >hit themselves on the Big Board A2 = A1∪\\n{hit\\ndobj\\n−−−→stock}\\n6’ SHIFT <ROOT >hit themselves on the Big Board A2\\n7’ SHIFT <ROOT >hit themselves on the Big Board A2\\n...\\n10’ SHIFT <ROOT >hit themselves on the Big Board A2\\n11’ LEFT nn <ROOT >hit themselves on the Board A3 = A2∪\\n{Board\\nnn\\n−→Big}\\n12’ LEFT det <ROOT >hit themselves on Board A4 = A3∪\\n{Board\\ndet\\n−−→the}\\n13’ RIGHT pobj <ROOT >hit themselves on A5 = A4∪\\n{on\\npobj\\n−−−→Board}\\n14’ RIGHT dep <ROOT >hit themselves A6 = A5∪\\n{themselves\\ndep\\n−−→on}\\nTable 4: Possible parsing walk-through with error\\nstill be combined with the correct head. This error\\nis introduced in Step 7’, where the parser moves\\non to the stack rather than creating an arc from hit\\nto themselves.7 There are thus two decision er-\\nrors that lead to the arc errors in Figure 2 (C). The\\nsecond decision error can, however, be caused in-\\ndirectly by the ﬁrst error. If a decision error causes\\nadditional decision errors later in the parsing pro-\\ncess, we talk of error propagation. This cannot be\\nknown just by looking at the derivation.\\n5.2 Examining the impact of decision errors\\nWe examine the impact of individual decision er-\\nrors on the overall parse results in our test set by\\ncombining a dynamic oracle and a recursive func-\\ntion. We use a dynamic oracle based on Goldberg\\net al. (2014) which gives us the overall loss at any\\npoint during the derivation. The loss is equal to\\nthe minimal number of arc errors that will have\\nbeen made once the parse is complete. We can\\nthus deduce how many arc errors are deterministi-\\ncally caused by a given decision error.\\nThe propagation of decision errors cannot be\\ndetermined by simply examining the increase in\\nloss during the parsing process. We use a recur-\\nsive function to identify whether a particular parse\\nsuffered from this. While parsing the sentence, we\\nregister which decisions lead to an increase in loss.\\nWe then recursively reparse the sentence correct-\\ning one additional decision error during each run\\nuntil the parser produces the gold. If each erro-\\nneous decision has to be corrected in order to ar-\\nrive at the gold, we assume the decision errors are\\n7Note that technically, on can still become a dependent\\nof hit, but this can only happen if on becomes the head of\\nthemselves which would also be an error.\\nSL RL-O RL-R RL-M\\nTotal Loss 7069 6227 6042 6144\\nDec. Errors 5177 4410 4345 4476\\nErr. Prop. 1399 1124 992 1035\\nNew errors 411 432 403 400\\nLoss/error 1.37 1.41 1.39 1.37\\nErr. Prop. (%) 27.0 25.5 22.8 23.1\\nTable 5: Overview of average impact of decision\\nerrors\\nindependent of each other. If, on the other hand,\\nthe correction of a speciﬁc decision also ﬁxes other\\ndecisions down the road, the original parse suffers\\nfrom error propagation.\\nThe results are presented in Table 5. Total Loss\\nindicates the number of arc errors in the corpus,\\nDec. Errorsthe number of decision errors andErr.\\nProp. the number of decision errors that were the\\nresult of error propagation. This number was ob-\\ntained by comparing the number of decision er-\\nrors in the original parse to the number of decision\\nerrors that needed to be ﬁxed to obtain the gold\\nparse. If less errors had to be ﬁxed than originally\\npresent, we counted the difference as error prop-\\nagation. Note that ﬁxing errors sometimes leads\\nto new decision errors during the derivation. We\\nalso counted the cases where more decision errors\\nneeded to be ﬁxed than were originally present and\\nreport them in Table 5.8\\n8We ran an alternative analysis where we counted all cases\\nwhere ﬁxing one decision error in the derivation reduced the\\noverall number of decision errors in the parse by more than\\none. Under this alternative analysis, similar reductions in the\\nproportion of error propagation were observed for reinforce-\\nment learning. On average, decision errors deterministically\\nlead to more than one arc error in the resulting\\nparse tree. This remains stable across systems\\n(around 1.4 arc errors per decision error). We\\nfurthermore observe that the proportion of deci-\\nsion errors that are the result of error propagation\\nhas indeed reduced for all reinforcement learn-\\ning models. Among the errors avoided by APG,\\n35.9% were propagated errors for RL-O RACLE ,\\n48.9% for RL-R ANDOM , and 51.9% for RL-\\nMEMORY . These percentages are all higher than\\nthe proportion of propagated errors occurring in\\nthe corpus parsed by SL (27%). This outcome\\nconﬁrms our hypothesis that reinforcement learn-\\ning is indeed more robust for making decisions in\\nimperfect environments and therefore reduces er-\\nror propagation.\\n6 Conclusion\\nThis paper introduced Approximate Policy Gra-\\ndient (APG), an efﬁcient reinforcement learning\\nalgorithm for NLP, and applied it to a high-\\nperformance greedy dependency parser. We hy-\\npothesized that reinforcement learning would be\\nmore robust against error propagation and would\\nhence improve parsing accuracy.\\nTo verify our hypothesis, we ran experiments\\napplying APG to three transition systems and two\\nlanguages. We furthermore introduced an exper-\\niment to investigate which portion of errors were\\nthe result of error propagation and compared the\\noutput of standard supervised machine learning to\\nreinforcement learning. Our results showed that:\\n(a) reinforcement learning indeed improved pars-\\ning accuracy and (b) propagated errors were over-\\nrepresented in the set of avoided errors, conﬁrming\\nour hypothesis.\\nTo our knowledge, this paper is the ﬁrst to show\\nexperimentally that reinforcement learning can re-\\nduce error propagation in an NLP task. This re-\\nsult was obtained by a straight-forward implemen-\\ntation of reinforcement learning. Furthermore, we\\nonly applied reinforcement learning in the training\\nphase, leaving the original efﬁciency of the model\\nintact. Overall, we see the outcome of our exper-\\niments as an important ﬁrst step in exploring the\\npossibilities of reinforcement learning for tackling\\nerror propagation.\\nRecent research on parsing has seen impressive\\nimprovement during the last year achieving UAS\\naround 94% (Andor et al., 2016). This improve-\\nment is partially due to other approaches that, at\\nleast in theory, address error propagation, such as\\nbeam search. Both the reinforcement learning al-\\ngorithm and the error propagation study we devel-\\noped can be applied to other parsing approaches.\\nThere are two (related) main questions to be ad-\\ndressed in future work in the domain of parsing.\\nThe ﬁrst addresses whether our method is comple-\\nmentary to alternative approaches and could also\\nimprove the current state-of-the-art. The second\\nquestion would address the impact of various ap-\\nproaches on error propagation and the kind of er-\\nrors they manage to avoid (following Ng and Cur-\\nran (2015)).\\nAPG is general enough for other structured pre-\\ndiction problems. We therefore plan to investigate\\nwhether we can apply our approach to other NLP\\ntasks such as coreference resolution or semantic\\nrole labeling and investigate if it can also reduce\\nerror propagation for these tasks.\\nThe source code of all experiments is pub-\\nlicly available at https://bitbucket.org/\\ncltl/redep-java.\\nAcknowledgments\\nThe research for this paper was supported by the\\nNetherlands Organisation for Scientiﬁc Research\\n(NWO) via the Spinoza-prize V ossen projects (SPI\\n30-673, 2014-2019) and the VENI project Read-\\ning between the lines (VENI 275-89-029). Ex-\\nperiments were carried out on the Dutch national\\ne-infrastructure with the support of SURF Co-\\noperative. We would like to thank our friends\\nand colleagues Piek V ossen, Roser Morante, Tom-\\nmaso Caselli, Emiel van Miltenburg, and Ngoc Do\\nfor many useful comments and discussions. We\\nwould like to extend our thanks the anonymous\\nreviewers for their feedback which helped improv-\\ning this paper. All remaining errors are our own.\\nReferences\\n[Alberti et al.2015] Chris Alberti, David Weiss, Greg\\nCoppola, and Slav Petrov. 2015. Improved\\nTransition-Based Parsing and Tagging with Neu-\\nral Networks. In EMNLP 2015, pages 1354–1359.\\nACL.\\n[Andor et al.2016] Daniel Andor, Chris Alberti, David\\nWeiss, Aliaksei Severyn, Alessandro Presta, Kuz-\\nman Ganchev, Slav Petrov, and Michael Collins.\\n2016. Globally Normalized Transition-Based Neu-\\nral Networks. arXiv.org, cs.CL. [Berant and Liang2015] Jonathan Berant and Percy\\nLiang. 2015. Imitation Learning of Agenda-based\\nSemantic Parsers. TACL, 3:545–558.\\n[Björkelund and Nivre2015] Anders Björkelund and\\nJoakim Nivre. 2015. Non-Deterministic Oracles for\\nUnrestricted Non-Projective Transition-Based De-\\npendency Parsing. In IWPT 2015, pages 76–86.\\nACL.\\n[Chang et al.2015] Kai-Wei Chang, Akshay Krishna-\\nmurthy, Alekh Agarwal, Hal Daumé III, and John\\nLangford. 2015. Learning to search better than your\\nteacher. In ICML 2015.\\n[Chen and Manning2014] Danqi Chen and Christopher\\nManning. 2014. A Fast and Accurate Dependency\\nParser using Neural Networks. In EMNLP 2014,\\npages 740–750. ACL.\\n[Daumé III et al.2009] Hal Daumé III, John Langford,\\nand Daniel Marcu. 2009. Search-based Structured\\nPrediction. Machine Learning, 75(3):297–325, 6.\\n[Dyer et al.2015] Chris Dyer, Miguel Ballesteros,\\nWang Ling, Austin Matthews, and Noah A Smith.\\n2015. Transition-Based Dependency Parsing with\\nStack Long Short-Term Memory. In ACL 2015,\\npages 334–343.\\n[Gildea and Palmer2002] Daniel Gildea and Martha\\nPalmer. 2002. The Necessity of Parsing for Pred-\\nicate Argument Recognition. In ACL 2002, pages\\n239–246. ACL.\\n[Goldberg and Nivre2012] Yoav Goldberg and Joakim\\nNivre. 2012. A Dynamic Oracle for Arc-Eager De-\\npendency Parsing. In COLING 2012, pages 959–\\n976.\\n[Goldberg and Nivre2013] Yoav Goldberg and Joakim\\nNivre. 2013. Training Deterministic Parsers with\\nNon-Deterministic Oracles. In TACL 2013, vol-\\nume 1, pages 403–414.\\n[Goldberg et al.2014] Yoav Goldberg, Francesco Sarto-\\nrio, and Giorgio Satta. 2014. A tabular method\\nfor dynamic oracles in transition-based parsing. In\\nTACL 2014, volume 2, pages 119–130.\\n[Gomez-Rodriguez et al.2014] Carlos Gomez-\\nRodriguez, Francesco Sartorio, and Giorgio\\nSatta. 2014. A Polynomial-Time Dynamic Oracle\\nfor Non-Projective Dependency Parsing. In EMNLP\\n2014, pages 917–927. ACL.\\n[Grissom II et al.2014] Alvin C. Grissom II, Jordan\\nBoyd-Graber, He He, John Morgan, and Hal\\nDaume III. 2014. Don’t Until the Final Verb Wait:\\nReinforcement Learning for Simultaneous Machine\\nTranslation. In EMNLP 2014, pages 1342–1352.\\n[Han et al.2013] Dan Han, Pascual Martínez-Gómez,\\nYusuke Miyao, Katsuhito Sudoh, and Masaaki\\nNagata. 2013. Effects of parsing errors on\\npre-reordering performance for Chinese-to-Japanese\\nSMT. PACLIC 27, pages 267–276.\\n[Honnibal and Johnson2015] Matthew Honnibal and\\nMark Johnson. 2015. An Improved Non-monotonic\\nTransition System for Dependency Parsing. In\\nEMNLP 2015, pages 1373–1378. ACL.\\n[Jiang et al.2012] Jiarong Jiang, Adam Teichert, Hal\\nDaumé III, and Jason Eisner. 2012. Learned Priori-\\ntization for Trading Off Accuracy and Speed. ICML\\nworkshop on Inferning: Interactions between Infer-\\nence and Learning, (0964681):1–9.\\n[Kiperwasser and Goldberg2016] Eliyahu Kiperwasser\\nand Yoav Goldberg. 2016. Simple and Accu-\\nrate Dependency Parsing Using Bidirectional LSTM\\nFeature Representations. CoRR, abs/1603.0.\\n[Li et al.2014] Zhenghua Li, Min Zhang, and Wenliang\\nChen. 2014. Ambiguity-aware Ensemble Training\\nfor Semi-supervised Dependency Parsing. In ACL\\n2014, pages 457–467.\\n[Maes et al.2009] Francis Maes, Ludovic Denoyer, and\\nPatrick Gallinari. 2009. Structured prediction\\nwith reinforcement learning. Machine Learning,\\n(77):271–301.\\n[McDonald and Nivre2007] Ryan McDonald and\\nJoakim Nivre. 2007. Characterizing the Errors\\nof Data-Driven Dependency Parsing Models. In\\nEMNLP-CoNLL 2007.\\n[McDonald et al.2005] Ryan McDonald, Fernando\\nPereira, Kiril Ribarov, and Jan Haji ˇc. 2005. Non-\\nprojective dependency parsing using spanning tree\\nalgorithms. In HLT-EMNLP 2005, pages 523–530.\\nAssociation for Computational Linguistics.\\n[Ng and Curran2015] Dominick Ng and James R Cur-\\nran. 2015. Identifying Cascading Errors using Con-\\nstraints in Dependency Parsing. In ACL-IJCNLP,\\npages 1148–1158, Beijing. ACL.\\n[Nivre and Fernández-González2014] Joakim Nivre\\nand Daniel Fernández-González. 2014. Arc-eager\\nParsing with the Tree Constraint. Computational\\nLinguistics, 40(2):259–267, 6.\\n[Nivre et al.2006a] Joakim Nivre, Johan Hall, and Jens\\nNilsson. 2006a. MaltParser: A data-driven parser-\\ngenerator for dependency parsing. In LREC 2006,\\nvolume 6, pages 2216–2219.\\n[Nivre et al.2006b] Joakim Nivre, Johan Hall, Jens\\nNilsson, Gül¸ sen Eryi˘git, and Svetoslav Marinov.\\n2006b. Labeled pseudo-projective dependency pars-\\ning with support vector machines. In CoNLL 2006,\\npages 221–225. ACL.\\n[Nivre et al.2007] Joakim Nivre, Johan Hall, Jens Nils-\\nson, Atanas Chanev, Eryiˇgit Gül¸ sen, Sandra Kübler,\\nSvetoslav Marinov, and Erwin Marsi. 2007. Malt-\\nParser: A language-independent system for data-\\ndriven dependency parsing. Natural Language En-\\ngineering, 13(02):95–135. [Nivre2003] Joakim Nivre. 2003. An Efﬁcient Algo-\\nrithm for Projective Dependency Parsing. In IWPT\\n2003, pages 149–160.\\n[Nivre2004] Joakim Nivre. 2004. Incrementality in\\nDeterministic Dependency Parsing. In Proceedings\\nof the Workshop on Incremental Parsing: Bringing\\nEngineering and Cognition Together.\\n[Nivre2009] Joakim Nivre. 2009. Non-projective De-\\npendency Parsing in Expected Linear Time. InACL-\\nIJCNLP 2009, pages 351–359, Stroudsburg, PA,\\nUSA. ACL.\\n[Quirk and Corston-Oliver2006] Chris Quirk and Si-\\nmon Corston-Oliver. 2006. The impact of parse\\nquality on syntactically-informed statistical machine\\ntranslation. In EMNLP 2006, pages 62–69, Sydney,\\nAustralia. ACL.\\n[Ranzato et al.2016] Marc’Aurelio Ranzato, Sumit\\nChopra, Michael Auli, and Wojciech Zaremba.\\n2016. Sequence Level Training with Recurrent\\nNeural Networks. ICLR, pages 1–15.\\n[Ross et al.2011] Stephane Ross, Geoffrey J Gordon,\\nand J Andrew Bagnell. 2011. A Reduction of Im-\\nitation Learning and Structured Prediction to No-\\nRegret Online Learning. AISTATS, 15:627–635.\\n[Seddah et al.2014] Djamé Seddah, Sandra Kübler, and\\nReut Tsarfaty. 2014. Introducing the SPMRL 2014\\nShared Task on Parsing Morphologically-Rich Lan-\\nguages. In Proceedings of the First Joint Workshop\\non Statistical Parsing of Morphologically Rich Lan-\\nguages and Syntactic Analysis of Non-Canonical\\nLanguages, pages 103–109.\\n[Shen et al.2016] Shiqi Shen, Yong Cheng, Zhongjun\\nHe, Wei He, Hua Wu, Maosong Sun, and Yang Liu.\\n2016. Minimum Risk Training for Neural Machine\\nTranslation. In ACL 2016, pages 1683–1692, Berlin,\\nGermany. ACL.\\n[Song et al.2012] Hyun-Je Song, Jeong-Woo Son, Tae-\\nGil Noh, Seong-Bae Park, and Sang-Jo Lee. 2012.\\nA Cost Sensitive Part-of-Speech Tagging: Differen-\\ntiating Serious Errors from Minor Errors. In ACL\\n2012, pages 1025–1034. ACL.\\n[Sutton et al.1999] Richard S. Sutton, David\\nMcallester, Satinder Singh, and Yishay Mansour.\\n1999. Policy Gradient Methods for Reinforcement\\nLearning with Function Approximation. In NIPS\\n1999, pages 1057–1063.\\n[Weiss et al.2015] David Weiss, Chris Alberti, Michael\\nCollins, and Slav Petrov. 2015. Structured Train-\\ning for Neural Network Transition-Based Parsing.\\nIn ACL-IJCNLP 2015, pages 323–333. ACL.\\n[Williams1992] Ronald J. Williams. 1992. Simple\\nstatistical gradient-following algorithms for connec-\\ntionist reinforcement learning. Machine Learning,\\n8(3-4):229–256.\\n[Yamada and Matsumoto2003] Hiroyasu Yamada and\\nYuji Matsumoto. 2003. Statistical Dependency\\nAnalysis with Support Vector Machines. In Pro-\\nceedings of IWPT, pages 195–206.\\n[Yang and Cardie2013] Bishan Yang and Claire Cardie.\\n2013. Joint Inference for Fine-grained Opinion Ex-\\ntraction. In ACL 2013, pages 1640–1649. ACL.\\n[Zhang and Chan2009] Lidan Zhang and Kwok Ping\\nChan. 2009. Dependency Parsing with Energy-\\nbased Reinforcement Learning. In IWPT 2009,\\npages 234–237. ACL.']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'metadatas': [[{'filename': '2304.05133v2.pdf',\n",
       "    'keyword': 'neural networks',\n",
       "    'topic': 'Machine Learning'},\n",
       "   {'filename': '1702.06794v1.pdf',\n",
       "    'keyword': 'reinforcement learning',\n",
       "    'topic': 'Machine Learning'}]],\n",
       " 'distances': [[1.5894837379455566, 1.6929992975097026]],\n",
       " 'included': [<IncludeEnum.distances: 'distances'>,\n",
       "  <IncludeEnum.documents: 'documents'>,\n",
       "  <IncludeEnum.metadatas: 'metadatas'>]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science_assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
